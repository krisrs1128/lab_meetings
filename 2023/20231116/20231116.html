<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Visualization in Deep Learning: Theme and Variations</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
    <script src="libs/robservable-binding-0.2.2/robservable.js"></script>
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title
background-size: cover



&lt;div id="title"&gt;
Visualization in Deep Learning: &lt;br/&gt; Theme and Variations
&lt;/div&gt;

&lt;div id="links"&gt;

Slides: &lt;a href="https://go.wisc.edu/9p83o9"&gt;https://go.wisc.edu/9p83o9&lt;/a&gt;
&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;

.center[
&lt;img src="figures/svcca_training.gif"/&gt;
]

&lt;div id="subtitle"&gt;
Kris Sankaran &lt;br/&gt;
&lt;a href="https://go.wisc.edu/pgb8nl"&gt;go.wisc.edu/pgb8nl&lt;/a&gt; &lt;br/&gt;
16 | November | 2023 &lt;br/&gt;
Machine Learning Lunch Meetings
&lt;/div&gt;

---

### Audience Question

What kinds of visualizations do you use for your research? Why do you make them?

[**https://go.wisc.edu/z03w7z**](https://go.wisc.edu/z03w7z)

---

### Variations

Visualization is often used to:
1. Summarize training dynamics.
2. Reason about errors and predictions.
3. Describe memory and representation learning mechanisms.

These abstractions help with:
1. Training and improving real-world models. (1, 2)
2. Guiding research through improved mental models. (1, 3)
3. Tying models into broader scientific discussion. (3)

---

### Variations

I’ll be drawing examples mainly from two projects:

.pull-left[
2. **Glacier Ecosystem Mapping**: Application of satellite image segmentation to climate change adaptation and disaster preparedness.
3. Remembrances of States Past: A visual analysis of "time warping" in sequence models.
]

.pull-right[
&lt;img src="figures/GL082082E30346N.png" width="100%"/&gt;
]

---

### Variations

I’ll be drawing examples mainly from two projects:

.pull-left[
2. Glacier Ecosystem Mapping: Application of satellite image segmentation to climate change adaptation and disaster preparedness.
3. **Remembrances of States Past**: A visual analysis of "time warping" in sequence models.
]

.pull-right[
&lt;iframe width="100%" height="800" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past@1310?cells=chart9"&gt;&lt;/iframe&gt;
]

---

## Visualizing Training Dynamics

---

### Loss Curves

.pull-left[
These visualizations are easy to make and highlight whether the model is
over/underfitting. This has immediate consequences for model architecture,
optimization hyperparameters, and regularization.
]

.pull-right[
&lt;img src="figures/loss_example.png" width=500/&gt;

Figure from [1].
]

---

### Example: Counting Crossings

In this toy problem, we apply a sequence model (with gated recurrent units) to count the number of times a curve has crossed the grey band.

.center[
&lt;iframe width="800" height="484" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past?cells=chart"&gt;&lt;/iframe&gt;
]

---

### Example: Counting Crossings

The example dataset are a collection of labeled pairs: 

* `\(\mathbf{x}_{i} \in \mathbb{R}^{200}\)`: A random trajectory stored as a long vector.
* `\(y_i\)`: The number of times the trajectory crosses the interval `\(\left[0, 1\right]\)`.

.center[
&lt;iframe width="100%" height="400" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past@1310?cells=chart9"&gt;&lt;/iframe&gt;
]

---

### Loss Curves

It's interesting to visualize the evolution of instance-level errors, especially
examples that remain difficult to predict late in training.

.center[
&lt;iframe width="950" height="500" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past@1311?cells=chart11"&gt;&lt;/iframe&gt;
]

---

### &lt;span style="col: red;"&gt;Dynamic Linking&lt;/span&gt;

This visualization applies dynamic linking [2]. By coordinating interaction across panels, we can show
several views of the same data.

<div id="htmlwidget-61a7543193b3257d07b2" style="width:1200px;height:250px;" class="robservable html-widget "></div>
<script type="application/json" data-for="htmlwidget-61a7543193b3257d07b2">{"x":{"notebook":"@krisrs1128/week-3-4","include":5,"hide":null,"input":null,"input_js":[],"observers":null,"update_height":true,"update_width":true},"evals":[],"jsHooks":[]}</script>

---

### Activations and Gradients

.pull-left[
More than losses, it can be worthwhile to visualize feature maps and gradients
throughout the training process.

These are activations from a UMAP model trained to segment glaciers [3]. The skip connections prevented the deepest layer in
the architecture from ever being learned!
]

.pull-right[
&lt;img src="figures/glacier_representations.png" width=460/&gt;
]

---

### Dimensionality Reduction

.pull-left[
1. Dimensionality reduction can shed more light onto training dynamics for the full model.
2. This figure from [4] shows that unsupervised
pretraining acts like a regularizer. It's especially interesting because it
works on function space, not the parameter space.
]

.pull-right[
&lt;img src="figures/unsupervised-pretraining.png" width=550/&gt;
]

---

## Visualizing Errors and Predictions

---

### Visualizing Errors

We fit satellite image segmentation models to datasets on building footprints [5]. Here are examples that were flagged as being especially poor quality.

.center[
&lt;img src="figures/img_6560_kitsap4.png" width=250/&gt;
&lt;img src="figures/pred_6560_kitsap4.png" width=250/&gt;
&lt;img src="figures/mask_6560_kitsap4.png" width=250/&gt;
]

In both cases, our worst examples are due to label noise. Can you tell what happened?

---

For problems where each sample is associated with a continuous accuracy measure, it is helpful to look at representatives from across the continuum.

.center[
&lt;img src="figures/glacial_lakes_grid.gif" width=700/&gt;
]

Different models can have different failure modes, and this kind of visualization compactly represents that.

---
&lt;span style="color: #D93611"/&gt;Small Multiples&lt;/span&gt;

In visualization, the idea of repeating a view across many parallel instances is called small multiples. This creates information-dense views.

.center[
&lt;img src="figures/small-multiples.jpeg" width=850/&gt;
]

---

&lt;span style="color: #D93611"/&gt;Data-to-Ink&lt;/span&gt;

.pull-left[
We were also careful to remove extraneous plot elements (e.g., unnecessary tick marks and grid lines). This is line with the goal of maximizing data-to-ink .]

.pull-right[
&lt;img src="figures/glacial_lakes_grid.gif"/&gt;
]

---

### Smoothed Error Rates

This is the result of a PCA on the activations at the last layer of the building
footprint segmentation model. For the background, we learned smoothed error
metrics across training examples.

.center[
&lt;iframe src="https://adrijanik.github.io/unet-vis#div_main" width=1000 height=350/&gt;
]

---

### Navigating Predictions

.pull-left[
How might a model's predictions guide decision making?

We worked on a project with ICIMOD to identify regions that lakes that were growing rapidly. These are a risk factor for glacial lake outbursts.
]

.pull-right[
&lt;img src="figures/nyt_glacial_lakes.png" width=450/&gt;
]

---

### Navigating Predictions

From predicted segmentations, we could fit trends to each lake's estimated area.
This volcano plot shows those that need more proactive monitoring.

.center[
&lt;img src="figures/volcano_plot.gif" width=640/&gt;
]

---

## Visualizing Representations

---

### Representations of DL Models

.pull-left[
A central goal of deep learning is to automatically learn higher-level
abstractions from data. What visualizations can help us gauge progress?
]

.pull-right[
&lt;img src="figures/semantic-representation.png" width=230/&gt;

Figure from [6]
]

---

### Goals of Representation Analysis

Deep learning models should sense higher-order abstractions. We can evaluate this by analyzing their learned representations.

1. How do architectural components compare, and how does it relate to performance differences?
2. Why do common training practices like transfer learning and normalization seem to help?
3. How do data-driven representations relate to concepts designed by human experts?

---

### Parking lot or…?

Back to the building footprint labeling task, here are two images we found with similar feature activations.

.pull-left[
&lt;img src="figures/cars.png" width=400/&gt;
]
.pull-right[
&lt;img src="figures/graves.png" width=400/&gt;
]

---

### Visualizing LSTMs

The classic paper [7] looked at feature
activations in character-level sequence models. It discovered representations
related to sequence position and code properties, for example.

&lt;img src="figures/lstm-activations-1.png"/&gt;

---

### Visualizing LSTMs

This paper helped demystify the mechanics of LSTM models. We could see how gating prevented important pieces of memory from being overwritten over long stretches of text.

&lt;img src="figures/lstm-activations-2.png"/&gt;

---

### Counting Model

Let’s look at features from the counting model. The first few layers encode the
general `\(y\)`-value of the trajectory. Later ones focus in on crossings.


&lt;iframe width="100%" height="484" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past@1324?cells=chart7"&gt;&lt;/iframe&gt;

---

### Comparing Representations

To compare high-dimensional representations, we need to
measure multivariate association. Popular choices are [8; 9], though also note [10].

&lt;img src="figures/cka.png"/&gt;

A CKA representation analysis contrasting ViT and ResNet representations across
depths, from [11].

---

### Comparing Representations

To compare high-dimensional representations, we need to
measure multivariate association. Popular choices are [8; 9], though also note [10].

.center[
&lt;img src="figures/cca_angle.png" width=500/&gt;
]

These contrast the column spaces of feature activation matrices.

---

### Learned Representations in Science

Scientific foundation models are gaining prominence, and it’s common to make dimensionality reduction plots from them. Can we make something that encourages more precise discourse?

.center[
&lt;iframe src="https://esmatlas.com/explore" width=950 height=350/&gt;
]

---

### Learned Representations in Science

Scientific foundation models are gaining prominence, and it’s common to make dimensionality reduction plots from them. Can we make something that encourages more precise discourse?

.center[
&lt;img src="figures/scgpt-atlas.png" width=700/&gt;
]

---

### Intriguing Experiment

.pull-left[
[12; 13] presented connections between linguistics and BERT embeddings. For example, they found that embedding and formal parse-tree distances were closely related.
]

.pull-right[
&lt;img src="figures/parse-tree.png" width=500/&gt;
]

---

### Intriguing Experiment

They also saw how the embeddings reflect word sense disambiguation and built an app to interactively query different words.

.center[
&lt;img src="figures/word-sense.png" width=840/&gt;
]

---

### Why do these work?

I like how these visualizations:

(1) Encourage readers to engage with existing mental models. 

(2) Provide domain-relevant context for interacting with learned representations.

This seems like a promising way to balance efficiency and agency in theory building. Hopefully we can avoid the "kaggleization of science" [14].

---

### Multi-omics analog?

How might these ideas play out in multi-omics foundation models?

* Gene regulatory networks &lt;—&gt; parse trees. Both provide simple abstractions for reasoning about complex processes.
* Gene-sense disambiguation. A protein’s purpose can depend on its cellular surroundings, and it’s possible that foundation models have learned to represent this.

.center[
&lt;img src="figures/go-graph.png"/&gt;
]

---

### Conclusion

I hope that you have learned a few visualization ideas that can help your
research and collaborations.

Some final thoughts:

1. Training, evaluation, and representation analysis all provide opportunities for thoughtful visualization.
2. When we build deep learning models, we are part of a human-machine system, and visualization is an important part of the link.

---

### References

[1] T. Ma and A. Ng. "CS229 Lecture notes".
In: _NA_ (2007).
&lt;https://api.semanticscholar.org/CorpusID:628573&gt;.

[2] B. Shneiderman. "The eyes have it: a
task by data type taxonomy for information
visualizations". In: _Proceedings 1996 IEEE
Symposium on Visual Languages_ (1996), pp.
336-343.
&lt;https://api.semanticscholar.org/CorpusID:2281975&gt;.

[3] M. Zheng, X. Miao, and K. Sankaran.
"Interactive Visualization and
Representation Analysis Applied to Glacier
Segmentation". In: _ISPRS Int. J. Geo Inf._
11 (2021), p. 415.

---

### References

[4] D. Erhan, A. C. Courville, Y. Bengio, et
al. "Why Does Unsupervised Pre-training Help
Deep Learning?" In: _International
Conference on Artificial Intelligence and
Statistics_. 2010.
&lt;https://api.semanticscholar.org/CorpusID:15796526&gt;.

[5] A. Janik, K. Sankaran, and A. Ortiz.
"Interpreting Black-Box Semantic
Segmentation Models in Remote Sensing
Applications". In: _MLVis@EuroVis_. 2019.
&lt;https://api.semanticscholar.org/CorpusID:201139207&gt;.

[6] Y. Bengio. "Learning Deep Architectures
for AI". In: _Found. Trends Mach. Learn._ 2
(2007), pp. 1-127.
&lt;https://api.semanticscholar.org/CorpusID:207178999&gt;.

[7] A. Karpathy, J. Johnson, and L. Fei-Fei.
"Visualizing and Understanding Recurrent
Networks". In: _ArXiv_ abs/1506.02078
(2015).
&lt;https://api.semanticscholar.org/CorpusID:988348&gt;.

---

### References

[8] A. Saha, A. Bialkowski, and S. Khalifa.
"Distilling Representational Similarity
using Centered Kernel Alignment (CKA)". In:
_British Machine Vision Conference_. 2022.
&lt;https://api.semanticscholar.org/CorpusID:256902315&gt;.

[9] M. Raghu, J. Gilmer, J. Yosinski, et al.
"SVCCA: Singular Vector Canonical
Correlation Analysis for Deep Learning
Dynamics and Interpretability". In: _Neural
Information Processing Systems_. 2017.
&lt;https://api.semanticscholar.org/CorpusID:23890457&gt;.

[10] J. Josse and S. P. Holmes. "Measuring
multivariate association and beyond." In:
_Statistics surveys_ 10 (2016), pp.  132-167
.
&lt;https://api.semanticscholar.org/CorpusID:207137323&gt;.

---

### References

[11] M. Raghu, T. Unterthiner, S. Kornblith,
et al. "Do Vision Transformers See Like
Convolutional Neural Networks?" In: _Neural
Information Processing Systems_. 2021.
&lt;https://api.semanticscholar.org/CorpusID:237213700&gt;.

[12] A. Coenen, E. Reif, A. Yuan, et al.
"Visualizing and Measuring the Geometry of
BERT". In: _ArXiv_ abs/1906.02715 (2019).
&lt;https://api.semanticscholar.org/CorpusID:174802633&gt;.

[13] J. Hewitt and C. D. Manning. "A
Structural Probe for Finding Syntax in Word
Representations". In: _North American
Chapter of the Association for Computational
Linguistics_. 2019.
&lt;https://api.semanticscholar.org/CorpusID:106402715&gt;.

---

### References

[14] _Fireside Chat with Christopher
Manning_.
&lt;https://www.microsoft.com/en-us/research/video/fireside-chat-with-christopher-manning/&gt;.
Accessed: 2023-11-14.

[15] J. Heer and S. K. Card. "DOITrees
revisited: scalable, space-constrained
visualization of hierarchical data". In:
_Proceedings of the working conference on
Advanced visual interfaces_ (2004).
&lt;https://api.semanticscholar.org/CorpusID:818990&gt;.

---

### References



---

### Navigating Predictions

From predicted segmentations, we could fit trends to each lake's estimated area.
This volcano plot shows those that need more proactive monitoring.

.center[
&lt;img src="figures/sanka8-3215722-large.gif" width=1000/&gt;
]

---

### Navigating Predictions

We also implemented a Shiny App to look up images from lakes with interesting trends.

&lt;iframe width=1000 src="https://krisrs1128.shinyapps.io/glacial_lake_visualization/" height=450/&gt;

---
### &lt;span style="col: #D93611;"&gt;Focus-plus-Context&lt;/span&gt;

This is an instance of the focus-plus-context principle [15]. The idea is to let the reader zoom into patterns of interest without losing relevant context.

.center[
&lt;img src="figures/doitree_dmoz.gif" width=800/&gt;
]

---

### Visualizing GRU Mechanics

`\begin{align*}
{\color{9955bb}h_{t}} &amp;= \left(1 - {\color{ffba00}z_t}\right) \circ {\color{#ff9966}{h_{t - 1}}} + {\color{ffba00}z_{t}} \circ \tilde{h}_{t} \\
{\color{ffba00}{z_t}} &amp;= \sigma\left(W_z {\color{#ab294d}{x_t}} + U_z {\color{#ff9966}{h_{t - 1}}}\right) \\
\tilde{h}_{t} &amp;= \tanh\left({\color{#298eab}W}{\color{#ab294d}{x_t}} + {\color{#29ab87}{U}}\left({\color{#ffa6c9}r_t} \circ {\color{#ff9966}h_{t - 1}}\right)\right)
\end{align*}`

&lt;iframe width="100%" height="409" frameborder="0"
  src="https://observablehq.com/embed/@krisrs1128/remembrances-of-states-past?cells=chart5"&gt;&lt;/iframe&gt;

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
