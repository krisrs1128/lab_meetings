---
title: "Selective Inference in Computational Genomics"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title
background-image: url("figure/selective-cover.png")
background-size: cover

$$\def\absarg#1{\left|#1\right|}$$
$$\def\Earg#1{\mathbf{E}\left[#1\right]}$$
$$\def\reals{\mathbb{R}} % Real number symbol$$
$$\def\integers{\mathbb{Z}} % Integer symbol$$
$$\def\*#1{\mathbf{#1}}$$
$$\def\m#1{\boldsymbol{#1}}$$
$$\def\FDR{\widehat{\operatorname{FDR}}}$$
$$\def\Gsn{\mathcal{N}}$$
$$\def\Unif{\operatorname{Unif}}$$
$$\def\Bern{\operatorname{Bern}}$$

```{r, echo = FALSE, warnings = FALSE, message = FALSE}
library(RefManageR)
library(knitr)
library(tidyverse)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3, eval = TRUE)
opts_knit$set(eval.after = "fig.cap")

BibOptions(cite.style = "numeric")
bib <- ReadBib("references.bib")
```

.pull-left[
<div id="title">
Selective Inference for Computational Genomics
</div>
<div id="subtitle">
Computational Genomics Summer Institute <br/>
Kris Sankaran <br/>
14 | July | 2023 <br/>
https://github.io/krisrs1128/LSLab
</div>
]

---

### Learning Objectives

You will be able to help collaborators solve scientific problems by adapting
modern methods from selective inference.

By the end of this talk, you will be able to:

1. Discuss the role of selective inference abstractions in computational genomics settings.
1. Interpret $p$-value histograms and explain how they motivate the Benjamini-Hochberg procedure.
1. Improve power in large scale inference by focusing attention on the most promising contexts.
1. Diagnose miscalibration in large scale inference and apply data splitting to address it.

---

### Large Scale Inference

Notation
  * Hypotheses of interest: $H_{1}, \dots, H_{M}$
  * Associated $p$-values: $p_{1}, \dots, p_{M}$.

Goal: Reject as many non-null hypotheses as possible while controlling the _False Discovery Rate_,

\begin{align*}
\Earg{\frac{\absarg{\text{False Positives}}}{\absarg{\text{Rejections}}\vee 1}}
\end{align*}

---

### Examples

* Microbiome: Is taxon $m$ associated with development of autism in infants?
* Epigenetics: Is CpG site $m$ differentially methlyated among smokers?
* Cancer: Is elevated immune cell expression of gene $m$ associated with improved survival rates?

---

### $p$-value histogram

* Under the null, the $p$-values follow a uniform distribution. 
* The spike near 0 comes from alternative hypotheses
* Let $\pi_{0}$ denote the true proportion of nulls

.center[
  <img src="figure/histogram-1.png" width=800/>
]

---

### $p$-value histogram

* Under the null, the $p$-values follow a uniform distribution. 
* The spike near 0 are likely from the alternative
* Let $\pi_{0}$ denote the true proportion of nulls

.center[
  <img src="figure/histogram-2.png" width=800/>
]

---

### Benjamini-Hochberg

The Benjamini-Hochberg (BH) procedure `r Citep(bib, "BH")` controls the FDR at
level $q$.

1. Sort the p-values: $p_{(1)} \leq p_{(2}) \leq \dots \leq p_{(m)}$
1. Find the largest $i$ such that $p_{(i)} \leq \frac{i q}{m}$
1. Reject hypotheses associated with $p_{(1)} \leq \dots \leq p_{(i)}$ 

---

### Why?

At any threshold $t$, estimate the FDR using the relative areas from the null and alternative density.

.center[
<img src="figure/annotated_histogram.png" width=400/>
]

---

### Why?

At any threshold $t$, estimate the FDR using relative areas from the null and alternative densities.

.center[
<img src="figure/annotated_histogram-2.png" width=400/>
]

---

### Why?

Let $R\left(t\right)$ be the number of rejected hypotheses at threshold $t$. Then,

.pull-left[
\begin{align*}
\FDR\left(t\right) &= \frac{\pi_{0}mt}{R\left(t\right)}
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-2.png" width=400/>
]

---

### Why?

Maximize the number of rejections while limiting false discoveries.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &R\left(t\right) \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-2.png" width=450/>
]

---

### Why?

Maximize the number of rejections while limiting false discoveries.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-2.png" width=450>
]

---

### Why?

Larger thresholds let more hypotheses through.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } & t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-3.png" width=450>
]

---

### Why?

Larger thresholds let more hypotheses through.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } & t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-4.png" width=450>
]

---

### Why?

Look for the largest $p$-value satisfying this inequality.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
<img src="figure/annotated_histogram-5.png" width=450>
]

---

### Why?

Look for the largest $p$-value satisfying this inequality.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}
]

.pull-right[
If we substitute $t = p_{(i)}$, then

\begin{align*}
&\frac{\pi_{0}m p_{(i)}}{i} \leq q \\
\implies & p_{(i)} \leq \frac{i q}{\pi_{0} m}
\end{align*}
]

---

.middle[
## Power through Context
]

---

### Hypotheses are not identical

* We may know in advance that some hypotheses are more likely to be rejected than others.
  - Prior literature
  - Library size
* Using this information can improve power

.center[
  <img width=650 src="figure/split-histograms.png"/>
]

---

### Selective Inference

Step (1) distinguishes selective inference from ordinary statistical inference.

1. Search for interesting patterns.
1. Test whether they could have just been coincidences.

Looking for promising contexts $\implies$ selective inference.

---

### Example: Testing Pairs

.pull-left[
Suppose we are testing associations between,

* SNPs $x_{1}, \dots, x_{I}$
* Histone Markers $y_{1}, \dots, y_{J}$
]

.pull-right[
$H_{m}: \text{Cor}\left(x_{i}, y_{j}\right) = 0$.

Even for moderate $I, J$, this is many tests!
]

.center[
  <img src="figure/pairwise-cors.png" width=800/>
]

---

### Pairwise Distance

We expect that most true associations are local, but we don't necessarily want to rule out long-range connections.

.center[
<img src="figure/expected-distances.png" width=600 />
]

---

### Grouping by Distance

* Group the distances into bins $1, \dots, G$.
* We use different thresholds $\*t = \left(t_{1}, \dots, t_{G}\right)$ across groups.

.center[
<img src="figure/pairwise-example-groups.png" width=800/>
]

---

### Generalization

* Group the distances into bins $1 , \dots, G$.
* We use different thresholds $\*t = \left(t_{1}, \dots, t_{G}\right)$ across groups.

.pull-left[
\begin{align*}
R\left(\*t\right) &:= \sum_{g} R_{g}\left(t_{g}\right) \\
\FDR\left(\mathbf{t}\right) &:= \frac{\sum_{g = 1}^{G} m_{g}t_{g}}{R\left(\*t\right)}
\end{align*}
]

.pull-right[
\begin{align*}
\text{maximize } &R\left(\*t\right) \\
\text{subject to } &\FDR\left(\*t\right) \leq q
\end{align*}
]

---

### Code Demo

We simulated a SNP-histone marker association dataset $x_{n} \in \{0, 1\}^{I\prime}$ and $y_{n} \in \reals^{J}$.

.pull-left[
\begin{align*}
y_{n} &= B x_{n} + \epsilon_{n} \\
x_{ni} &\sim \Bern\left(\frac{1}{2}\right)
B_{nj} &\sim \Bern\left(0.4 \exp{- \frac{1}{10}\left|i - j\right|\right)
\end{align*}
]

.pull-right[
<img src="figure/beta_heatmap.png"/>
]

---

### Code Demo

We simulated a SNP-histone marker association dataset $x_{n} \in \{0, 1\}^{I\prime}$ and $y_{n} \in \reals^{J}$.

.pull-left[
\begin{align*}
y_{n} &= B x_{n} + \epsilon_{n} \\
x_{ni} &\sim \Bern\left(\frac{1}{2}\right)
B_{nj} &\sim \Bern\left(0.4 \exp{- \frac{1}{10}\left|\frac{i}{I} - \frac{j}{J}\right|\right)
\end{align*}
]

.pull-right[
<img src="figure/context_p_values_histogram.png"/>
]

---

### Code Demo

We can estimate $\FDR\left(\*t\right)$ with a few lines of code:

```{r, eval = FALSE}
m <- map_dbl(p_values, length)
R <- map2_dbl(p_values, thresholds, ~ sum(.x < .y))
sum(m * thresholds) / R
```

* `p_values` is a list of $p$-values for each group $g$
* `thresholds` is a vector storing $\*t$

---

### Code Demo

We can have more generous thresholds for $p$-values in the nearby group. This
invests the most "budget" on the promising contexts.
  
.center[
  <img src="figure/optimal-thresholds-1.png" width=800/>
]

---

### Code Demo

We can have more generous thresholds for $p$-values in the nearby group. This
invests the most "budget" on the promising contexts.
  
.center[
  <img src="figure/optimal-thresholds-2.png" width=800/>
]

---

### Optimization

* In practice, we would optimize $\*t$, rather than scanning across the entire
space.
* The number of rejections can be derived from the CDF of the $p$-value histogram.

\begin{align*}
R_{g}\left(t_{g}\right) &= \hat{F}_{g}\left(t_{g}\right)
\end{align*}

* To ensure the optimization is convex, we can use the Grenander estimate of $\hat{F}_{g}$

.center[
  <img src="figure/grenander_comparison.png" width=800>
]

---

### Optimization

This can be plugged into generic optimization solvers:

\begin{align*}
\text{maximize } & \sum_{g} m_{g}\hat{F}\left(t_{g}\right) \\
\text{subject to } &
\sum_{g = 1}^{G} m_{g}\left(t_{g} - m_{g}\hat{F}\left(t_{g}\right)\right) \leq q
\end{align*}

---

### Subtleties

.pull-left[
* Cross-Weighting: As written, the thresholds $\*t$ are not independent of $p_{m}$. We should split the hypotheses to ensure independence.
* Regularization: Thresholds can be smoothed to reflect group similarity
]

.pull-right[
A more elegant implementation uses a sequence $g = 1, \dots, G$ across many
distance thresholds.
]

---

.middle[
## Calibration through Splitting
]

---

### Miscalibration

Often in computational genomics, model assumptions don't exactly hold and
$p$-values are not uniformly distributed.

.center[
  <img src="figure/miscalibration_histogram.png" width=800>
]

---

### Example

Imagine $x_{ij} \vert \text{disease = t} \sim \text{NB}\left(\mu_{jt}, \varphi\right)$ are genes whose expression potentially varies across disease status.

\begin{align*}
H_{0j}: \mu_{j0} = \mu_{j1} \\
H_{1j}: \mu_{j0} \neq \mu_{j1}
\end{align*}

---

### Example

* We applied Trimmed Maximum of Means and a log normalization to these simulated data, as implemented in the `edgeR` package
* We then apply two-sample $t$-tests. Notice the nonuniformity of the null $p$-values.

.center[
<img src="figure/miscalibration_histogram.png" width=700/>
]

---

### Key Idea

We can avoid this problem if we bypass $p$-values altogether.
  - Significance can be deduced from test statistics directly
  - We don't have to use BH to ensure FDR control
  
.center[
<img src="figure/mirror_inspiration_raw.png" width = 800/>
]

This is a highly adaptable idea. We'll discuss `r Citep(bib, "Dai2020FalseDR")`, but see also `r Citep(bib, c("adapt", "ihw", "clipper"))`.

---

### Testing $\to$ Regression

We work with the original data $\*X \in \reals^{N \times M}$ and $\*y \in \reals^{N}$. For example,
 * $y_{n}$: Disease state for patient $n$ 
 * $x_{nm}$: Expression level for gene $m$.

Consider the regression $\*y = \*X \*\beta + \epsilon$.
  * $H_{0m}: \hat{\beta}_{m}$ is symmetric around 0

---

### Testing $\to$ Regression

Consider the regression $\*y = \*X \*\beta + \epsilon$.
  * $H_{0m}: \hat{\beta}_{m}$ is symmetric around 0

.center[
  <img src="figure/regression-assumptions.png" width=800/>
]

---

### Symmetry and Mirrors

Randomly split rows into $\left(\*X^{(1)}, \*y^{(1)}\right)$ and $\left(\*X^{(2)}, \*y^{(2)}\right)$.

Mirror statistics have the form,
\begin{align*}
T_{m} &= \text{sign}\left(\hat{\beta}^{1}_{m}\hat{\beta}_{m}^{(2)}\right)f\left(\hat{\beta}^{(1)}_{m}, \hat{\beta}^{(2)}_{m}\right)
\end{align*}

where $\hat{\beta}^{(j)}$ are estimated on the separate splits.

---

### Symmetry and Mirrors

Mirror statistics measure the agreement in estimates across splits.

Example: $\text{sign}\left(\hat{\beta}^{1}_{m}\hat{\beta}_{m}^{(2)}\right)\left[\absarg{\hat{\beta}_{m}^{(1)}} + \absarg{\hat{\beta}_{m}^{(2)}}\right]$

.center[
  <img src="figure/regression-mirrors.png" width=750>
]

---

### Symmetry and Mirrors

Mirror statistics measure the agreement in estimates across splits.

Example: $\text{sign}\left(\hat{\beta}^{1}_{m}\hat{\beta}_{m}^{(2)}\right)\left[\absarg{\hat{\beta}_{m}^{(1)}} + \absarg{\hat{\beta}_{m}^{(2)}}\right]$


```{r, eval = FALSE}
ix <- sample(nrow(x), 0.5 * nrow(x))
beta1 <- train_fun(x[ix], y[ix])
beta2 <- train_fun(x[-ix], y[-ix])
sign(beta1 * beta2) * (abs(beta1) + abs(beta2))
```

---

### Mirror Histogram

Let's revisit the negative binomial problem. Here are the $T_{m}$.

.center[
  <img src="figure/mirror_histogram_merged.png"/>
]

---

### Mirror Histogram

A reasonable strategy is to reject for all $T_{m}$ in the far right tail. How should we pick the threshold?

.center[
  <img src="figure/mirror_histogram_merged.png"/>
]

---

### Mirror Histogram

A reasonable strategy is to reject for all $T_{m}$ in the far right tail. How should we pick the threshold?

.center[
  <img src="figure/mirror_histogram.png"/>
]

---

### FDR Estimation

By symmetry, the size of the left tail $\approx$ the number of nulls in the
right tail:

.pull-left[
\begin{align*}
\FDR\left(t\right) &= \frac{\absarg{T_{m} < -t}}{\absarg{T_{m} > t}}
\end{align*}
]

.pull-right[
  <img src="figure/mirror_histogram.png"/>
]

---

### FDR Estimation

Maximize discoveries while controlling false discoveries:

\begin{align*}
\text{maximize } &t \\
\text{subject to } &\FDR\left(t\right) \leq q
\end{align*}

---

### Multiple Splits

Issue:
  * A downside is that sample splitting samples reduces power.
  * Power can be recovered by aggregating over multiple iterations of splitting^[This is a recurring theme in the literature `r Citep(bib, "guo2023")`].

Notation:
  * Create $K$ different pairs of random splits
  * Let $\hat{S}_{k}$ be the selected features when using mirror statistics on the $k^{th}$ random split

---

### Aggregation Procedure

We need a mechanism for aggregating $\hat{S}_{k}$ across iterations.

.center[
<img src="figure/selection.png"/>
]

---

### Aggregation Procedure

Let $\hat{I}_{1}, \dots, \hat{I}_{m}$ store the column averages of this matrix. Larger $\hat{I}_{m}$ means:
  * The feature is selected even when $\absarg{\hat{S}_{k}}$ is small
  * The feature is selected frequently

.center[
  <img src="figure/selection.png"/>
]

---

### Aggregation Procedure

Let $\hat{I}_{1}, \dots, \hat{I}_{m}$ store the column averages of this matrix. Larger $\hat{I}_{m}$ means:
  * The feature is selected even when $\absarg{\hat{S}_{k}}$ is small
  * The feature is selected frequently

.center[
  <img src="figure/I_hat.png"/>
]

---

### Aggregation Procedure

* For the final selection, choose features with the largest $\hat{I}_{m}$ possible, up until the budget exceeds $1 - q$.
* This is guaranteed to (asymptotically) control the FDR at level $q$

.center[
  <img src="figure/I_hat_bar.png"/>
]

---

### Code Example

In our Negative Binomial example, this leads to FDR control and reasonable
power. Here are results over 10 runs.


---

### Variations

* This procedure is a **meta-algorithm** -- we can use intermediate algorithms to construct mirror statistics
  - The only assumption is that the mirrors are symmetric under the null
* This gives flexibility beyond the i.i.d. and maximum likelihood settings assumed by classical hypothesis testing

---

### Conclusion

* Whenever the signal is subtle, hypothesis testing is crucial.
* The key abstractions we encountered were,
  - Data splitting + Symmetry assumptions = Formal Error Control
  - Optimization view of BH + Context = Focused Hypothesis Testing
* These abstractions are useful across the computational genomics workflow
  - E.g., in experimental design `r Citep(bib, c("Fannjiang2022ConformalPU"))` and interpretation of features generated through unsupervised learning `r Citep(bib, c("Song2021PseudotimeDEIO", "Guo2023RanktransformedSI"))`

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 4)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 5, end = 8)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 9, end = 12)
```