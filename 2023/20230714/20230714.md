---
title: "Selective Inference in Computational Genomics"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title
background-image: url("figure/selective-cover.png")
background-size: cover


```{r, echo = FALSE, warnings = FALSE, message = FALSE}
library(RefManageR)
library(knitr)
library(tidyverse)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3, eval = TRUE)
opts_knit$set(eval.after = "fig.cap")

BibOptions(cite.style = "numeric")
bib <- ReadBib("references.bib")
```

.pull-left[
<div id="title">
Selective Inference for Computational Genomics
</div>
<div id="subtitle">
Computational Genomics Summer Institute <br/>
Kris Sankaran <br/>
14 | July | 2023 <br/>
https://github.io/krisrs1128/LSLab
</div>
]

---

### Learning Objectives

You will be able to help collaborators accurately resolve scientific issues by adapting high-power methods from selective inference.

By the end of this talk, you will be able to:

1. Discuss the role of selective inference abstractions in computational genomics settings.
1. Interpret $p$-value histograms and explain how they motivate the Benjamini-Hochberg procedure.
1. Improve power in large scale inference by focusing attention on the most promising contexts.
1. Diagnose miscalibration in large scale inference and apply data splitting to address it.

---

### Large Scale Inference

Notation
  * Hypotheses of interest: $H_{1}, \dots, H_{M}$
  * Associated $p$-values: $p_{1}, \dots, p_{M}$.

Goal: Reject as many non-null hypotheses as possible while controlling the False Discovery Rate, $\mathbb{E}\left[\left\|\frac{\mathrm{False Positives}\right\|}{\left\|\mathrm{Rejections}\right\|}].

---

### Examples

* Cancer: Is elevated immune cell expression of gene $m$ associated with improved survival rates?
* Microbiome: Is taxon $m$ associated with development of Type 1 diabetes?
* Epigenetics: Is SNP $i$ associated with histone marker $j$? Each $i-j$ pairs defines on test $H_{m}$.

--- 

### $p$-value histogram

* Under the null, the $p$-values follow a uniform distribution. 
* The spike near 0 comes from alternative hypotheses
* $\pi_{0}$ denotes the true proportion of nulls

[ Image of p-value histogram ]

---

### $p$-value histogram

* Under the null, the $p$-values follow a uniform distribution. 
* The spike near 0 are likely from the alternative
* $\pi_{0}$ denotes the true proportion of nulls

[ show some with larger or smaller p-value histograms ]

---

### Benjamini-Hochberg

The BH procedure selects hypotheses in a way that controls the FDR at level $q$.

1. Sort the p-values: $p_{(1)} \leq p_{(2}) \leq \dots \leq p_{(m)}$
1. Find the largest $i$ such that $p_{(i)} leq \frac{i q}{m}$
1. Reject hypotheses associated with $p_{(1)} \leq \dots \leq p_{(i)}$ 

---

### Why?

At any threshold $t$, estimate the FDR using the relative areas from the null and alternative density.

<img src="../figure/annotated_histogram.png" width=800>

---

### Why?

At any threshold $t$, estimate the FDR using the relative areas from the null and alternative density.

<img src="../figure/annotated_histogram-2.png" width=800>

---

### Why?

Let $R\left(t\right)$ be the number of rejected hypotheses at threshold $t$. Then,

.pull-left[
\begin{align*}
\hat{\text{FDR}}\left(t\right) &= \frac{\pi_{0}mt}{R\left(t\right)}
\end{align*}
]

.pull-right[
<img src="../figure/annotated_histogram-2.png" width=800>
]

---

### Why?

Maximize the number of rejections while limiting false discoveries.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &R\left(t\right) \\
\text{subject to } &\hat{\text{FDR}} \leq q
\end{align*}
]

.pull-right[
<img src="../figure/annotated_histogram-2.png" width=800>
]


---

### Why?

Larger thresholds let more hypotheses through.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } & t \\
\text{subject to } &\hat{\text{FDR}} \leq q
\end{align*}
]

.pull-right[
<img src="../figure/annotated_histogram-3.png" width=800>
]

---

### Why?

Larger thresholds let more hypotheses through.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } & t \\
\text{subject to } &\hat{\text{FDR}} \leq q
\end{align*}
]

.pull-right[
<img src="../figure/annotated_histogram-4.png" width=800>
]

---

### Why?

Look for the largest $p$-value satisfying this inequality.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &R\left(t\right) \\
\text{subject to } &\hat{\text{FDR}} \leq q
\end{align*}
]

.pull-right[
<img src="../figure/annotated_histogram-5.png" width=800>
]


---

### Why?

Look for the largest $p$-value satisfying this inequality.

.pull-left[
Optimize:
\begin{align*}
\text{maximize } &R\left(t\right) \\
\text{subject to } &\hat{\text{FDR}} \leq q
\end{align*}
]

.pull-right[
If we substitute $t = p_{(i)}$, then

\begin{align*}
\frac{\pi_{0}m p_{(i)}}{i} &\leq q \\
&\implies p_{(i)} \leq \frac{i q}{\pi_{0} m}
\end{align*}
]

---

.middle[
## Power through Context
]

---

### Hypotheses are not identical

* We may know in advance that some hypotheses are more likely to be rejected than others.
  - Prior literature
  - Library size
* Using this information can improve power

.center[
  <img width=700 src="../split-histograms.png"/>
]

---

### Selective Inference

Step (1) is what distinguishes selective inference from ordinary statistical inference.

1. Search for interesting patterns.
1. Test whether they could have just been coincidences.

Looking for interesting contexts $\implies$ we are now doing selective inference.

---

### Example: Testing Pairs

Suppose we are testing associations between,

* SNPs $x_{1}, \dots, x_{I}$
* Histone Markers $y_{1}, \dots, y_{J}$

$H_{m}: \text{Cor}\left(x_{i}, y_{j}\right) = 0$.

Even for moderate $I, J$, this is many tests!

[Picture with links between two rows of circles]

---

### Pairwise Distance

We expect that most true associations are ``local,'' but we don't necessarily want to rule out long-range connections.

[Distance matrix showing expected rho[i, j]]]

---

### Generalization

* Group the distances into bins $1 , \dots, G$.
* We use different thresholds $\mathbf{t} = \left(t_{1}, \dots, t_{G}\right)$ across groups.

.pull-left[
R\left(\mathbf{t}\right) &:= \sum_{g} R_{g}\left(t_{g}\right) \\
\hat{\text{FDR}}\left(\mathbf{t}\right) &:= \frac{\sum_{g = 1}^{G} m_{g}t_{g}}{R\left(\mathbf{t}\right)}
]

.pull-right[
\begin{align*}
\text{maximize } &R\left(\mathbf{t}\right) \\
\text{subject to } &\hat{\text{FDR}\left(\mathbf{t}\right)} \leq q
\end{align*}
]

---

### Reformulation

The number of rejections can be derived from the CDF of the $p$-value histogram.

\begin{align*}
R_{g}\left(t_{g}\right) &= \hat{F}_{g}\left(t_{g}\right)
\end{align*}

---

### Optimization

We can plug this into generic optimization solvers:

.pull-right[
\begin{align*}
\text{maximize } & \sum_{g} m_{g}\hat{F}\left(t_{g}\right)
\text{subject to } &
\sum_{g = 1}^{G} m_{g}\left(t_{g} - m_{g}\hat{F}\left(t_{g}\right)\right) \leq q
\end{align*}
]

---

### Subtleties

* Cross-Weighting: As written, the threshold $t_{g}$ for a hypothesis $m$ in group $g$ may inadvertently depend on $p_{m}$.
* Convex Optimization: We can simplify the optimization by using monotone estimates of $\hat{F}_{g}$
* Regularization: Thresholds can be smoothed to reflect group similarity

---

### Code Demo

We first estimate the CDF of $p$-values within each group.

```{r}
```

---

### Code Demo

We next define the optimization problem.

```{r}
```

---

### Code Demo

We have learned to use a higher threshold for hypotheses with smaller pairwise distances. This focuses power on the most promising candidates.

```{r}
```

---

.middle[
## Calibration through Splitting
]

---

### Miscalibration

Often in computational genomics, traditional hypothesis testing assumptions
don't hold, so our $p$-values are not actually uniformly distributed.

[Show miscalibrated p-value histogram]

---

### Example

Imagine $x_{ij} \vert \text{disease = t} \sim \text{NB}\left(\mu_{jt}, \varphi\right)$ are genes whose expression potentially varies across disease status.

\begin{align*}
H_{0j}: \mu{j0} = \mu{j1} \\
H_{1j}: \mu{j0} \neq \mu_{j1}
\end{align*}

---

### Example

* If we used a Negative Binomial model, there wouldn't be a problem. 
* What if we instead $\log\left(1 + x\right)$ transformed the data and ran two-sample $t$-tests?

.center[
  <img src="../figure/miscalibration_histogram.png" width=800>
]

---

### Key Idea

We can avoid this problem if we can bypass $p$-values altogether.
  - Significance can be deduced from test statistics directly
  - We don't have to use BH to ensure FDR control

This is a highly adaptable idea. We'll discuss a simplification of \citep{dai2022}.

---

### Testing $\to$ Regression

We work with the original data $\*X \in \reals^{N \times M}$ and $\*y \in \reals^{N}$. 
 * E.g.: $y_{n} =$ disease state for patient $n$ and $x_{nm}$ expression level for gene $m$.

Consider the regression $\*y = \*X \*\beta + \epsilon$.
  * $H_{0m}: \hat{\beta}_{m}$ is symmetric around 0

---

### Symmetry and Mirrors

Randomly split rows into $\left(\*X^{(1)}, \*y^{(1)}\right)$ and $\left(\*X^{(2)}, \*y^{(2)}\right)$.

Mirror statistics have the form,
\begin{align*}
T_{m} &= \text{sign}\left(\hat{\beta}^{1}_{m}\hat{\beta}_{m}^{(2)}\right)f\left(\hat{\beta}^{(1)}_{m}, \hat{\beta}^{(2)}_{m}\right)
\end{align*}

where $\hat{\beta}^{(j)}$ are estimated on the separate splits.

---

### Symmetry and Mirrors

Mirror statistics measure the agreement in variable importances between splits.

[Qualitative figure of the estimates between splits]

Example: $\text{sign}\left(\hat{\beta}^{1}_{m}\hat{\beta}_{m}^{(2)}\right)\left[\frac{\hat{\beta}_{m}^{(1)} + \hat{\beta}_{m}^{(2)}}{2}\right]$

---

### Mirror Histogram

Let's revisit the negative binomial problem. This is the histogram of $T_{m}$.

---

### Mirror Histogram

A reasonable strategy is to reject for all $T_{m}$ in the far right tail. How should we pick the threshold?

---

### FDR Estimation

By symmetry, the size of the left tail informs the number of nulls at each threshold $t > 0$:

\begin{align*}
\hat{\text{FDR}}\left(t\right) &= \frac{\absarg{T_{m} < -t}}{\absarg{T_{m} > t}}
\end{align*}

[Include a picture of flipping the null left with the true right tail]

---

### FDR Estimation

Maximize discoveries while controlling false discoveries:

\begin{align*}
\text{minimize} t \\
\text{subject to } \text{FDR}\left(t\right) \leq q
\end{align*}

---

### Code Demo

This is how $\hat{FDR}\left(t\right)$ behaves on the Poisson example.

[Include an interactive]

---

### Multiple Splits

Issue:
  * A downside of this approach is that splitting samples reduces power.
  * It's possible to recover power by aggregating over multiple iterations of random splitting^[This is a recurring theme in the literature [give refs]].

Notation:
  * Create $K$ different pairs of random splits
  * Let $\hat{S}_{k}$ be the selected features when using mirror statistics on the $k^{th}$ random split

---

### Aggregation Procedure

We need a mechanism for aggregating selections across each split.

[Draw the matrix of selections on both ends]

---

### Aggregation Procedure

Let $\hat{I}_{1}, \dots, hat{I}_{m}$ store the column averages of this matrix. Larger $\hat{I}_{m}$ means:
  * The feature is selected even when $\absarg{S}_{k}$ is small
  * The feature is selected frequently

[Draw the matrix of selections on both ends]

---

### Aggregation Procedure

* For the final selection, choose features $m$ with the largest $\hat{I}_{m}$ possible, up until the ``budget'' exceeds $1 - q$.
* This is guaranteed to control the FDR at level $q$

---

### Code Example

In our Poisson example, this leads to FDR control and good power. Here, we've simulated and averaged over 1000 splits.

| Method | FDR | Power |
| --- | --- | --- |
| BH | 0 | 0 |
| Mirrors | 0 | 0 |

---

### Variations

* This procedure is a **meta-algorithm** -- we can use intermediate algorithms to construct mirror statistics
  - The only assumption is that the mirrors are symmetric under the null
* This gives flexibility beyond the i.i.d. and maximum likelihood settings assumed by classical hypothesis testing

---

### Conclusion

* Whenever the signal is subtle, hypothesis testing is crucial.
* The key abstractions we encountered were,
  - Data splitting + Symmetry assumptions = Formal Error Control
  - Optimization view of BH + Context = Focused Hypothesis Testing
* These abstractions are useful across the computational genomics workflow
  - E.g., in experimental design `r Citep(bib, c(""))` and interpretation of features generated through unsupervised learning `r citep(bib, c(""))`

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 4)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 5, end = 8)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 9, end = 12)
```