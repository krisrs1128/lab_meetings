---
title: "Evaluating Causal Microbiome Models"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title
background-image: url("")
background-size: cover

$\def\Gsn{\mathcal{N}}$
$\def\Mult{\text{Mult}}$
$\def\diag{\text{diag}}$
$\def\*#1{\mathbf{#1}}$
$\def\Scal{\mathcal{S}}$
$\def\exp#1{\text{exp}\left(#1\right)}$
$\def\logit#1{\text{logit}\left(#1\right)}$
$\def\absarg#1{\left|#1\right|}$
$\def\E{\mathbb{E}} % Expectation symbol$
$\def\Earg#1{\E\left[{#1}\right]}$
$\def\P{\mathbb{P}} % Expectation symbol$
$\def\Parg#1{\P\left[{#1}\right]}$

```{r, echo = FALSE, warnings = FALSE, message = FALSE}
library(RefManageR)
library(knitr)
library(tidyverse)
library(glue)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)
opts_knit$set(eval.after = "fig.cap")
set.seed(123)

BibOptions(cite.style = "numeric")
bib <- ReadBib("references.bib")
```

.center[
<br/>
# Evaluating Causal Microbiome Models
<br/>
<br/>
<br/>
<br/>
]

#### Holmes Group Meeting

.large[
Joint work with Xinran Miao and Hanying Jiang <br/>
20 | Jan | 2023
<br/>
]

---

### Outline

1. Describe progress on the meditation collaboration.
2. Explore the model selection problem we faced in this project, together with our proposed approach.
3. Review the more general literature on causal inference model evaluation.

---

# Part 1: Meditation Study Update

---

### Mindfulness Interventions

1. There is growing evidence for a relationship between the microbiome and psychiatric conditions, both in mouse models and in observational human studies.
2. Mindfulness interventions (e.g., meditation training) are known to alleviate symptoms associated with depression and anxiety.
3. During one of its mindfulness training programs, the Center for Healthy Minds gathered pre-post 16S data for both program participants and a waitlisted control.

---

### Mediation Analysis

1. We were concerned that the mindfulness intervention might be affect behavioral factors that in turn influences microbiota composition.
2. To address this, we developed a suite of mediation analysis approaches, all based on [Imai and Sohn].

---

### R Package

We have an R package that supports this analysis. It’s possible to specify a variety of different model configurations, and new Stan code is prepared for each combination.

(show an example of fitting)

---

### R package

We can easily summarize and visualize the results, both statically and interactively.

(show static view)

---

### R package

We can easily summarize and visualize the results, both statically and interactively.

(show interactive view)

---

# Part 2: Comparing Models for the Meditation Study

---

### Challenge

1. By far, our team spent more time attempting to choose between possible models than we did with either processing the data or implementing the LNM models in the first place.
2. Prediction performance: Good prediction of future composition doesn’t guarantee accurate inference of mediation effects.
3. Simulations: Developing simulation benchmarks is also challenging. If we knew a good mechanism, we would just use that model.

---

### Benchmark Optimism

1. To illustrate the second issue, consider a simulation study for evaluating a simple (non-mediation) LNM.
2. We will compare estimation quality when we simulate from,
 - The LNM itself
 - A simulator based on a pilot dataset

---

### Synthetic Setup

In the first simulation, we simulate from a version of the LNM,

\begin{align*}
Y &\sim \Mult\left(N_{i}, \varphi^{-1}\left(\xi_{0} + \xi_{T}T\right)\right) \\
\xi_{T} &:= \text{HardThreshold}\left(\tilde{\xi}_{T}, \text{keep 25%}\right) \\
\xi_{0}, \tilde{\xi}_{T} &\sim \Gsn\left(0, I_{K}\right) \\
\end{align*}

.center[
<img src="figures/lnm-spherical.png" width=500/>
]

---
### Semisynthetic Setup

In the second, we use the exact same $\xi_{T}$, but now to exponentially tilt
samples from treatment,
\begin{align*}
Y \sim \Mult\left(N_{i}, \exp{\xi_{T}T}\odot \hat{p}^{\ast}\right)
\end{align*}
Here, $\hat{p}^{\ast}$ is drawn randomly with replacement from compositions
in an observed pilot dataset (the meditation study data, in this case).
---
### Simulation Comparison

The purely synthetic simulation setup leads to overoptimistic power and FSR
estimates, compared to the semisynthetic setup.

.center[
<img src="figures/semisynthetic_comparison.png" width=950/>
]

---

### Zero-Inflated Quantiles (ZINQ)

1. Our main idea is to adapt the semisynthetic setup to the mediation analysis setting using the ZINQ model, following `r Citep(bib, "ling2021powerful")`.
2. This approach estimates a CDF for each species using,
\begin{align*}
 \logit{\Parg{Y > 0 \vert X}} = \gamma_{0} + \gamma^{T}X \\
 Q_{Y}\left(\tau \vert X, Y > 0\right) =\xi_{0}\left(\tau\right) + \xi\left(\tau\right)^{T}X
\end{align*}
    where $Q_{Y}\left(\tau \vert X, Y > 0\right)$ is the conditional $\tau^{th}$ quantile of a nonzero count.
.center[
<img src="figures/zinq.png" width=400/>
]

---

### Semisynthetic Simulation Recipe
  
1. **Estimate $\hat{\gamma}, \hat{\xi}\left(\tau\right)$ from real data**.  This defines $\hat{F}_{y \vert x, t, m}$ from which to simulate community profiles.
2. **Define true positives and negatives**. We rank species according to their estimated effects and set simulation $\xi\left(\tau\right), \gamma$ for all but the top 25% to 0.
3. **Simulate data from alternative configurations**. We vary the sample size and rescale coefficients $\hat{\xi}\left(\tau\right)$ while constraining relative abundances for null taxa to have no counterfactual difference.
4. Estimate models across settings and **compute error rates**.

---

### ZINQ Simulation Fidelity

This model generates fairly realistic data.
.center[
<img src="figures/comparison.png" width=700/>
]

---

### ZINQ Simulation Fidelity

This is the same plot, but restricting to nonnegative counts.
.center[
<img src="figures/comparison_no_zeros.png" width=700/>
]

---

### 

This is an example of how we can manipulate the counterfactual direct effects for a subset of taxa. We can do the same with indirect effects.

---

Give some conclusion from our ZINQ study.

---

---

# Part 3: The Landscape of Causal Model Evaluation

---

### Motivation

1. In future studies, we are likely to encounter similar difficulties in comparing causal inference methods.
2. There is fortunately a growing literature on this topic, and we’ll review a few of the most interesting proposals.

Parikh et al.
Schuler et al.
Schuler et al. (2)
Tran et al.
Athey et al.

---

### Model Complexity

1. The basic difficulty that these papers address is that there is a bias-variance trade-off in causal inference methods as well. 
2. The richest model won’t always give us the best counterfactual estimates (as we discovered firsthand).
3. Unfortunately, we can’t rely on cross-validation to guide us towards the optimal trade-off.

---

### Cross Validation as a Simulator

1. From one perspective, cross validation is a kind of synthetic data generator that comes with ground truth.
2. Critically, it is *dataset specific.* We don’t need to handcraft the simulation mechanism, and it automatically returns datasets that are well-suited to model comparison on our particular problem.
3. The ideal is to have a similar type of dataset-adaptive mechanism for causal method evaluation.

---

### Recurring Theme

Several of the proposals have the following recipe,
1. Specify (or learn) some plausible treatment effect functions.
2. Define a generator that respects these treatment effects while also
resembling the real data.
3. Compare candidate models through their ability to capture the known treatment
effects.

We’ll look at how this is done in two methods, “Credence” and
“Synth-Validation.”

---

### Step 1: Treatment Effect Definition (Credence)

1. In this method, the the functional form of the treatment effect is viewed as
a hyperparameter. It must be specified by the user.
2. They recommend at least trying (a) no treatment effect and (b) simple
polynomials.

---

### Step 1: Treatment Effect Definition (Synth-Validation)

Synth-Validation focuses solely on the average treatment effect. However,
they identify the range of plausible effects using an ensemble of models.

---

### Step 2: Generating Data (Credence)

Credence assumes the following graphical model for the covariates $x_{i}$,
treatment assignments $t_{i}$, and counterfactual outcomes $y^{(0)_{i}$ and
$y^{(1)}_{i}$.

---

### Step 2: Generating Data (Credence)

This model is fit using a series of auto-encoders, but regularized towards the
user-specified treatment effect function.

---

### Step 2: Generating Data (Synth-Validation)

1. In contrast, synth-validation simply samples covariates and treatment
assignments from the observed data.
2. The counterfactual outcomes are fit using separate regressions, but these are
then offset so that the ATE exactly agrees with the value found in Step 1.

---

### Evaluating the Selection Criteria

1. It is tricky to tell whether these techniques actually help in selecting the
best causal inference model.
2. The most common strategy is to define a comprehensive simulation. Then, you
can compare the proposed selection method with an oracle that knows the ground
truth.
3. In some special cases, paired observational and experimental data are
available. The experiments are used for ground truth, and the selection methods
are applied to models trained on the observational counterpart.

---

### Validation Set Approaches

1. So far, I have only considered simulation-based approaches. But there is an interesting alternative that has a very different flavor.
2. The main idea is to compare the model’s predicted treatment effect with some nonparametric estimate on a validation set.

---

### Example: Matching Estimators


---

### Conclusion

1. Even as we write up our approach to the analysis of the meditation study, we
are finding some interesting questions around how to more efficiently compare
between candidate models.
2. Generative models are a valuable tool for microbiome analysis (especially for
counterfactual / hypothetical reasoning), but without reliable approaches for
model comparison, it will be hard to make progress.
