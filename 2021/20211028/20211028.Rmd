---
title: "Understanding ML Algorithms (DSN)"
author: "Kris Sankaran"
date: "October 28, 2021"
output: 
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---

$$\def\*#1{\mathbf{#1}}$$
$$\def\m#1{\boldsymbol{#1}}$$
$$\def\reals{\mathbb{R}} % Real number symbol$$
$$\def\data{\mathcal{D}} % Real number symbol$$
$$\def\P{\mathbb{P}} % Probability symbol$$
$$\def\Parg#1{\P\left({#1}\right)}$$
$$\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}$$
$$\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential$$

```{r, echo = FALSE}
library(knitr)
set.seed(202110)
opts_chunk$set(echo = TRUE, fig.align = "left", cache = FALSE, fig.width = 5, fig.height = 5, warning = FALSE, message = FALSE, dpi = 300)
library(ggplot2)
min_theme <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(min_theme)

# overwrite some default scales in ggplot2
scale_fill_continuous <- function(...) scico::scale_fill_scico(..., palette = "lapaz", direction = -1)
scale_colour_discrete <- function(...) ggplot2::scale_color_brewer(..., palette = "Set2")
scale_x_continuous <- function(...) ggplot2::scale_x_continuous(..., expand = c(0, 0))
scale_y_continuous <- function(...) ggplot2::scale_y_continuous(..., expand = c(0, 0))
```

---

## Understanding Machine Learning Algorithms

```{r, echo = FALSE, out.width = 420, fig.align = "left"}
include_graphics("sketches/logistic_two_d_ys_projection.png")
```

 [Kris Sankaran](https://krisrs1128.github.io/LSLab/) | [UW Madison](https://stat.wisc.edu) | October 28, 2021 | [Code](https://github.com/krisrs1128/talks/blob/master/2021/20211028/20211028.Rmd)
 
.pull-left[
```{r, out.width = 230, fig.show = "hold", echo = FALSE}
include_graphics("sketches/uw-logo-flush-web.png")
```
]
.pull-right[
```{r, out.width = 220, fig.show = "hold", echo = FALSE}
include_graphics("sketches/dsn_logo.png")
```
]

---

### Difficulties

* Machine learning has become important in a wide range of applications
* But studying ML is still a challenge. Why?
  - Uses vocabulary from many different areas
  - Is not centrally-organized around anything physical system
  - Flashy results can make it seem like magic

---

### Learning Techniques

* To learn something well requires returning to the basics
* Recognizing gaps in understanding is critical
  - What is the "muddiest" point?
* Developing hands-on experience is more valuable than trusting experts

---

### Learning Techniques

We will illustrate these techniques over the course of the lecture.

* Translate complicated notation into simpler visuals
 - This supports both memory and brainstorming
* Design simulation experiments that require minimal effort
 - They will nonetheless shed light on the algorithm
* Develop original applications for an idea

---

# Warm-Up: Logistic Regression

---

### Logistic Regression Setup

* $\*x_{n} \in \reals^{D}$: Features describing sample $n$
* $y_{n} \in \{0, 1\}$: The class label of sample $n$
* $N$: The total number of training samples.

The goal is to use the training data to learn a predictor that correctly gives
the class label $y^\ast$ for future examples $\*x^\ast$.
  * Past shopping behavior $\to$ future purchases
  * Medical histories for many patients $\to$ diagnosis for current patient
  * Features in satellite image $\to$ contains a river or not

---

### Textbook Definition

This is from section 5.7 in *Deep Learning* by Goodfellow, Courville, and
Bengio.

> To solve this problem... use the logistic sigmoid function to squash the output
of the linear function into the interval $\left(0, 1\right)$ and interpret that
value as a probability,

\begin{align*}
p\left(y = 1\vert \*x; \*\theta\right) &= \sigma\left(\*\theta^T \*x\right) \\
\sigma\left(z\right) &= \frac{1}{1 + \exp{-z}}
\end{align*}

Simply reading this definition is unlikely to deepen our understanding of
logistic regression.

---

### Simplification: One Dimension

Let's consider the equation when $\*x$ is one-dimensional. Call this version $x$.

\begin{align*}
p\left(y = 1\vert x; \theta\right) &= \sigma\left(\*\theta x\right) \\
\sigma\left(z\right) &= \frac{1}{1 + \exp{-z}}
\end{align*}

Substituting the formula for $\sigma$ into the first equation gives,
\begin{align*}
p\left(y = 1\vert x; \theta\right) &= \frac{1}{1 + \exp{-\theta x}}
\end{align*}
a function of $x$ with parameter $\theta$.

---

### Visuals (1)

We can try visualizing this function for a few different values of $\theta$.

\begin{align*}
p\left(y = 1\vert x; \theta\right) &= \frac{1}{1 + \exp{-\theta x}}
\end{align*}

.pull-left[
We can just plug in a few values of $x$ and interpolate.
- What happens at $x = 0$?
- What happens when $x \to \infty$? When $x \to -\infty$?
]


.pull-right[
$\theta = 1$,
```{r, echo = FALSE, fig.cap = "Label the axes and limits!"}
include_graphics("sketches/logistic_positive.png")
```

]

---

### Visuals (1)

We can try visualizing this function for a few different values of $\theta$.

\begin{align*}
p\left(y = 1\vert x; \theta\right) &= \frac{1}{1 + \exp{-\theta x}}
\end{align*}

.pull-left[
We can just plug in a few values of $x$ and interpolate.
- What happens at $x = 0$?
- What happens when $x \to \infty$? When $x \to -\infty$?
]


.pull-right[
$\theta = -1$,
```{r, echo = FALSE, fig.cap = "Label the axes and limits!"}
include_graphics("sketches/logistic_negative.png")
```
]

---

### Visuals (2)

If the $y = 1$ class tends to be on one side of the $x$-axis, we should be able
to find a $\theta$ so that the probability is different for the two classes.

```{r, echo = FALSE, out.width = 750, fig.cap = "What would be a bad choice of logistic function?"}
include_graphics("sketches/logistic_oned_ys.png")
```

---

### Higher Dimensions

The actual logistic regression formula refers to a vector $\*x \in \reals^{D}$.

\begin{align*}
p\left(y = 1\vert \*x; \*\theta\right) &= \sigma\left(\*\theta^T \*x\right) =  \frac{1}{1 + \exp{-\*\theta^T \*x}}
\end{align*}

.pull-left[
* $\*\theta^T \*x$ is high when the angle between
$\*\theta$ and $\*x$ is small.
* It is constant in the directions at 90 degree angles to $\theta$
* It reduces the $D$-dimensional $\*x$ into one-dimensional $\*\theta^T \*x$
]

.pull-right[
```{r, out.width = 200, echo = FALSE, fig.cap = "Closely aligned vectors have large inner products with one another."}
include_graphics("sketches/inner_product.png")
```
]

---

### Higher Dimensions

The actual logistic regression formula refers to a vector $\*x \in \reals^{D}$.

\begin{align*}
p\left(y = 1\vert \*x; \*\theta\right) &= \sigma\left(\*\theta^T \*x\right) =  \frac{1}{1 + \exp{-\*\theta^T \*x}}
\end{align*}

.pull-left[
* $\*\theta^T \*x$ is high when the angle between
$\*\theta$ and $\*x$ is small.
* It is constant in the directions at 90 degree angles to $\theta$
* It reduces the $D$-dimensional $\*x$ into one-dimensional $\*\theta^T \*x$
]

.pull-right[
```{r, out.width = 150, echo = FALSE, fig.cap = "Perpendicular vectors have inner product equal to 0."}
include_graphics("sketches/inner_product-2.png")
```
]

---

### Higher Dimensions

The actual logistic regression formula refers to a vector $\*x \in \reals^{D}$.

\begin{align*}
p\left(y = 1\vert \*x; \*\theta\right) &= \sigma\left(\*\theta^T \*x\right) =  \frac{1}{1 + \exp{-\*\theta^T \*x}}
\end{align*}

.pull-left[
* $\*\theta^T \*x$ is high when the angle between
$\*\theta$ and $\*x$ is small.
* It is constant in the directions at 90 degree angles to $\theta$
* It reduces the $D$-dimensional $\*x$ into one-dimensional $\*\theta^T \*x$
]

.pull-right[
```{r, out.width = 150, echo = FALSE, fig.cap = "When the vectors are at obtuse angles, their inner product is negative."}
include_graphics("sketches/inner_product_3.png")
```
]

---

### Higher Dimensions

The actual logistic regression formula refers to a vector $\*x \in \reals^{D}$.

\begin{align*}
p\left(y = 1\vert \*x; \*\theta\right) &= \sigma\left(\*\theta^T \*x\right) =  \frac{1}{1 + \exp{-\*\theta^T \*x}}
\end{align*}

.pull-left[
Putting these facts together, we find that logistic regression looks for a
direction $\*\theta$ that defines a line (or plane) to separate the classes.
]

.pull-right[
```{r, echo = FALSE, out.width = 350, fig.cap = "In higher-dimensions, logistic regression finds a direction that separates the classes (in the projection)."}
include_graphics("sketches/logistic_two_d_ys_projection.png")
```
]

---

### Simulation Experiment

.pull-left[
We'll create a simple dataset to get more hands-on experience with logistic
regression. The code here makes a dataset of $\*x_{i} \in \reals^{2}$ from a
$t$-distribution.
]

.pull-right[
```{r, out.width = 240}
N <- 150
library(ggplot2)
x <- matrix(rt(N * 2, 12), N, 2)
simulation <- data.frame(x = x)

ggplot(simulation) +
  geom_point(aes(x.1, x.2))
```
]

---

### Simulation Experiment

.pull-left[
Next, we create two classes by shifting half of the points over to the top
right.
]

.pull-right[
```{r, out.width = 220}
N <- 150
library(ggplot2)
x <- matrix(rt(N * 2, 12), N, 2)
x[1:(N / 2), ] <- x[1:(N / 2), ] + c(2, 2)
simulation <- data.frame(x = x)

ggplot(simulation) +
  geom_point(aes(x.1, x.2))
```
]

---

### Simulation Experiment

.pull-left[
We save this information into a data.frame so that we can plot the two classes
against one another.
]

.pull-right[
```{r, out.width = 220}
N <- 150
library(ggplot2)
x <- matrix(rt(N * 2, 12), N, 2)
x[1:(N / 2), ] <- x[1:(N / 2), ] + c(2, 2)
y <- c(rep("A", N / 2), rep("B", N / 2))
simulation <- data.frame(x = x, y = y)

ggplot(simulation) +
  geom_point(aes(x.1, x.2, col = y)) +
  theme(legend.position = "none")
```
]

---

### Fitting Logistic Regression

```{r, echo = FALSE}
options(width = 500)
```

* The code block below fits a logistic regression model in R.
* Notice that the learned $\*\theta$ is nearly parallel to the offset used to
define the second class.

```{r}
library(caret)
fit <- train(y ~ ., simulation, method = "glm")
coef(fit$finalModel)
```

---

```{r, out.width = 400, fig.cap = "The decision boundary nicely separates the two classes."}
x_star <- seq(-3, 5, length.out = N)
grid <- data.frame(expand.grid(x.1 = x_star, x.2 = x_star))
grid$y_hat <- predict(fit, grid, type = "prob")[, 1]

ggplot(grid, aes(x.1, x.2)) +
  geom_tile(aes(fill = y_hat)) +
  geom_point(data = simulation, aes(col = y))
```

---

# Case Study: Deep Learning

---

### Fully-Connected Multi-Layer Network

Section 6.4 of the same book,

> Most neural networks are organized into groups of units called layers... In this structure, the first layer is given by

\begin{align*}
\*h_{1} &= g_1\left(\*W_{1}^{T}\*x + \*b_{1}\right) \\
\end{align*}
> the second layer by
\begin{align*}
\*h_{2} &= g_2\left(\*W_{2}^{T}\*h^1 + \*b_{2}\right) \\
\end{align*}

* $\*W_k, \*b_k$: Weights and biases for layer $k$
* $g_k, \*h_k$: Nonlinearity and hidden units for layer $k$

---

### One-Dimension

* Let's consider the equation when $\*x$ is one-dimensional. Call this version
$x$. 
* Suppose all nonlinearities $g^k$ are logistic units, $\sigma$.

.pull-left[
Then, 
\begin{align*}
\*h_{1} &= g^1\left(\*W^{T}_1\*x + \*b_{1}\right) \\
\end{align*}
simplifies to
\begin{align*}
\begin{pmatrix} \sigma\left(w^1_1 x + b^1_1\right) \\ \vdots \\ \sigma\left(w^1_{K} x + b^1_{K}\right) \end{pmatrix}
\end{align*}
which is a collection of sigmoids.
]

.pull-right[
```{r, out.width = 220, echo = FALSE}
include_graphics("sketches/hidden_layer_general.png")
```

```{r, out.width = 200, echo = FALSE}
include_graphics("sketches/hidden_layer_general_specific.png")
```
]

---

### Collection of Sigmoids

Each coordinate $w_{k}^1$ defines a logistic function.

.pull-left[
\begin{align*}
\begin{pmatrix} \sigma\left(w^1_1 x + b^1_1\right) \\ \vdots \\ \sigma\left(w^1_{K} x + b^1_{K}\right) \end{pmatrix}
\end{align*}
]

.pull-right[
```{r, echo = FALSE, out.width = 400, fig.cap = "Each coordinate k is a different logistic function, centered around a different x coordinate and with varying degrees of steepness."}
include_graphics("sketches/logistic_collection.png")
```
]

---

### Layer 2

.pull-left[
There are two differences for the second layer,
* The input $\*h_1$ is multidimensional 
* The input is not the original $x$
]

.pull-right[
\begin{align*}
\*h_{2} &= \sigma\left(\*W^{T}_2\*h_1 + \*b_{2}\right) \\
\end{align*}
```{r, echo = FALSE, out.width = 400}
include_graphics("sketches/hidden_layer_general-2.png")
```
]

---

### Layer 2: First Coordinate

What goes in the first coordinate of layer 2?

\begin{align*}
\sigma\left(\sum_{k} w^{2}_{k} h_{k}^{1} + b_{k}^2\right)
\end{align*}

and since the $h_k$'s are themselves sigmoids, the inner summation is like

\begin{align*}
\sum_{k} w^{2}_{k} \sigma\left(w_{k}^{1}x + b_{k}^{1}\right) + b_k^2
\end{align*}
which is a weighted mixture of sigmoids from the previous part.

---

### Visualization

* By mixing simple sigmoids, we can arrive at more complicated functions.
* The same idea holds for other activations (e.g., ReLU)

\begin{align*}
\sum_{k} w^{2}_{k} \sigma\left(w_{k}^{1}x + b_{k}^{1}\right) + b_k^2
\end{align*}

```{r, echo = FALSE, out.width = 800}
include_graphics("sketches/logistic_mixture.png")
```

What do you think is the analogous visualization for $\mathbf{x}\in \reals^{2}$?

---

### Simulation

We generate a dataset where the decision boundary is not linear.

```{r, echo = FALSE}
scale_fill_continuous <- function(...) scico::scale_fill_scico(..., palette = "lapaz", direction = 1)
```


.pull-left[
```{r}
x <- matrix(rt(N * 2, 12), N, 2)
y <- rowSums(x ^ 2) < 1
flip_ix <- sample(N, N / 20) # 5% unavoidable error
y[flip_ix] <- !y[flip_ix]
simulation <- data.frame(x = x, y = y)
```
]

.pull-right[
```{r, out.width = 300}
ggplot(simulation) +
  geom_point(aes(x.1, x.2, col = y))
```
]

---

### Defining a Model

* We can define a simple model with one hidden layer (second line)
* The last line mixes hidden representations to get the class prediction
* Instead of $\sigma$, we use ReLU, but the mixture interpretation still holds

```{r}
library(keras)
library(dplyr)
model <- keras_model_sequential() %>% 
  layer_dense(units = 20, activation = "relu", input_shape = c(2)) %>% 
  layer_dense(units = 20, activation = "relu", input_shape = c(20)) %>% 
  layer_dense(units = 20, activation = "relu", input_shape = c(20)) %>% 
  layer_dense(2, activation = "softmax")
```

---

### Network Training Parameters

.pull-left[
Before we can train the model, we have to define an optimizer and the loss.
]

.pull-right[
```{r}
model %>% 
  compile(
    optimizer = "adam", 
    loss = "categorical_crossentropy"
  )
```
]

---

### Network Training

.pull-left[
* We have to convert the simulation data into the
correct formats 
  - A matrix for the $\*x_i$ and one-hot encodings for $\*y_i$.
* We can then train the model. 
]

.pull-right[
```{r}
model %>% 
  fit(
    x = simulation %>% select(starts_with("x")) %>% as.matrix(),
    y = to_categorical(simulation$y),
    metric = "accuracy",
    epochs = 60,
    verbose = 0
  )
```
]

---

### Visualize Predictions

.pull-left[
* As before, we can visualize predictions along a grid.
* By mixing many linear hyperplanes, we can make a nonlinear decision boundary
* Notice the model overfits to some of the wrong points (which is why the
boundary isn't circular)
]

.pull-right[
```{r, out.width = 300}
x_star <- seq(-4.8, 4.8, length.out = N)
grid <- data.frame(expand.grid(x.1 = x_star, x.2 = x_star))
grid$y_hat <- predict(model, x = as.matrix(grid[, 1:2]))[, 2]

ggplot(grid, aes(x.1, x.2)) +
  geom_tile(aes(fill = y_hat)) +
  geom_point(data = simulation, aes(col = y))
```
]

---

# Case Study: Influence Estimation

---

### Studying Real Papers

* Even though these are textbook examples, similar strategies are useful for
understanding research
  - Drawing guiding pictures
  - Designing simple simulations
* Focus on what the authors claim to know and how they know it

---

### Paper

Let's consider this approach for the paper,

Feldman, V., & Zhang, C. (2020). What neural networks memorize and why:
Discovering the long tail via influence estimation.

---

### Memorization

* How much does an algorithm $\mathcal{A}$ memorize about sample $i$ in dataset $\data$?
* $h$ is the prediction function learned by the algorithm $\mathcal{A}$
* They introduce the definition,

\begin{align*}
\widehat{\text{mem}}(\mathcal{A}, \data, i):= \Psubarg{h \leftarrow \mathcal{A}(\data)}{h\left(\*x_{i}\right)=\*y_{i}}-\Psubarg{h \leftarrow \mathcal{A}\left(\data^{\backslash i}\right)}{h\left(\*x_{i}\right)=\*y_{i}}
\end{align*}

---

### Visualization

\begin{align*}
\widehat{\text{mem}}(\mathcal{A}, \data, i):= \Psubarg{h \leftarrow \mathcal{A}(\data)}{h\left(\*x_{i}\right)=\*y_{i}}-\Psubarg{h \leftarrow \mathcal{A}\left(\data^{\backslash i}\right)}{h\left(\*x_{i}\right)=\*y_{i}}
\end{align*}

```{r, echo = FALSE, fig.cap = "The memorization statistic captures how much the predicted probabilities change when a point is removed.", out.width = 650}
include_graphics("sketches/memorization_picture.png")
```

---

### How to Estimate Memorization

* Sample $S$ subsets $\data_{s}$ of size $m$ from $\data$
* For each subset $s$, train the predictor $h_{s} = \mathcal{A}\left(\data_{s}\right)$
* For each example $i$, estimate the change in probability when that example is
held out,

\begin{align*}
\widehat{\text{mem}}\left(\mathcal{A}, \data, i\right) &= \Parg{h_{s}\left(\*x_{i}\right) = y_{i} \vert i \in \data_{s}} - \Parg{h_{s}\left(\*x_{i}\right) = y_{i} \vert i \notin \data_{s}}
\end{align*}

where the probabilities are averages over algorithms run with / without that
sample.

---

### Simulation Setup

* What do the memorization estimates look like for the simulation data from the
previous example?
* We will subsample it 100 times, train 100 models, and compute the
$\widehat{\text{mem}}\left(\mathcal{A}, \data, i\right)$ statistic

```{r}
library(purrr)
B <- 100
indices <- rerun(B, sample(1:N, N / 2))
indices[1:2] # indices of first two subsampled datasets
```

---

This is some helper code. It wraps the model building and fitting from before
into functions, but otherwise does nothing new.

```{r}
init_model <- function() {
  keras_model_sequential() %>% 
    layer_dense(units = 20, activation = "relu", input_shape = c(2)) %>% 
    layer_dense(units = 20, activation = "relu", input_shape = c(20)) %>% 
    layer_dense(units = 20, activation = "relu", input_shape = c(20)) %>% 
    layer_dense(2, activation = "softmax") %>%
    compile(
      optimizer = "adam", 
      loss = "categorical_crossentropy"
    )
}

fit_model <- function(model, df) {
  model %>%
    fit(
      x = df %>% select(starts_with("x")) %>% as.matrix(),
      y = to_categorical(df$y),
      epochs = 60,
      verbose = 0
    )
}
```

---

### Fitting Models

* In the block below, we fit 100 models to each of the different subsamples.
* The difference in predicted probabilities for these models will be used to
estimate memorization

```{r}
models <- rerun(B, init_model()) # initialize models
map(indices, ~ simulation[., ]) %>% # get subsamples
  map2(models, ~ fit_model(.y, .x)) # fit model to subsample
```

---

### Predicted Probabilities

* We now get predicted probabilities from each of these models, across the
entire dataset
* Each row in `probs` is a different sample
* Each column is a different model
* Samples with very different predicted probabilities across models are
potentially being "memorized"

```{r}
probs <- map_dfc(models, ~ predict(., as.matrix(simulation[, 1:2]))[, 2]) %>%
  as.matrix()

probs[1:4, 1:8]
```

---

### Indicator Matrix

.pull-left[
* For computing the memorization statistic, we want to know which models used which samples.
* This matrix has a 1 in row $i$ and column $j$ if model $j$ used sample $i$
]

.pull-right[
```{r}
indicator <- matrix(FALSE, nrow(probs), ncol(probs))
for (i in seq_along(indices)) {
  indicator[indices[[i]], i] <- TRUE
}

indicator[1:10, 1:5]
```
]

---

### Memorization Function

The function below computes the formula for $\widehat{\text{mem}}(\mathcal{A}, \data, i)$ given the variables discussed above. The line within the for-loop is
the key one.

```{r}
memorization <- function(probs, indicator, y) {
  result <- vector(length = length(y))
  for (i in seq_along(y)) {
      result[i] <- mean(probs[i, indicator[i, ]]) - mean(probs[i, !indicator[i, ]])
  }

  result[!y] <- -result[!y]
  result
}
```

---

### Visualization

.pull-left[
The memorized samples tend to be,
* Nearer to the decision boundary
* Points that would be otherwise misclassified

This is consistent with the findings of the paper, even though the dataset we've
worked with is much simpler.
]

.pull-right[
```{r, out.width = 350}
simulation$mem <- memorization(probs, indicator, y)
ggplot(simulation, aes(x.1, x.2)) +
  geom_point(aes(col = y, size = mem)) +
  scale_size_binned(breaks = c(0, 0.02, 0.05), range = c(0, 2))
```
]

---

### Conclusion

.pull-left[
We've worked through three case studies that emphasize,

* To deeply understand an idea, it helps to view it from several perspectives
* Try illustrating abstract ideas and constructing hands-on examples
* The goal is to make something that appears complex as simple as possible

Everyone has their own way of learning, but I think that the more creative (and
less routine) the process, the better.
]

.pull-right[
Feel free to connect on linkedin or email me at
[ksankaran@wisc.edu](mailto:ksankaran@wisc.edu).
]

Enjoy the journey!