---
title: "Multiscale Analysis of Count Data"
author: "Kris Sankaran"
date: "November 5, 2021"
output: xaringan::moon_reader
---

```{r}
library(knitr)
opts_chunk$set(echo = FALSE, fig.align = "center")
```

## Multiscale Analysis of Count Data through Topic alignment

.pull-left[
   Kris Sankaran
   UW Madison
   November 5, 2021
 ]

---

### Come up with a better introduction
### Some comparison with hierarchical clustering

---

Having these properties would support,
* Navigation: Multiscale methods help the analyst navigate
 richness-interpretability trade-offs between views
* Parsimony: Support for partial membership can reduce the number of prototypes
 needed
* Critical Evaluation: What are the assumptions used in data analysis? It helps
 to have an explicit, generative model.

--

### Main Idea

* We will fit an ensemble of generative models of varying levels of complexity (Parsimony, Evaluation)
* We will build a compact representation of the result (Navigation)

.pull-left[
<img src="figure/transport-true-lda.png"/>
]
.pull-right[
<img src="figure/transport-true-lda-betas.png"/>
]

---

# Methodology

---

### Topic Models

Latent Dirichlet Allocation (LDA) is a popular topic model that supposes samples
$x_i \in \mathbb{R}^{D}$ are drawn independently from,

\begin{align*}
x_i \vert \gamma_i &\sim \text{Mult}\left(n_{i}, B\gamma_{i}\right) \\
\gamma_{i} &\sim \text{Dir}\left(\lambda_{\gamma} 1_{K}\right)
\end{align*}
where the columns $\beta_{k}$ of $B \in \Delta^{D}$ lie in the $D$
dimensional simplex and are themselves drawn independently from,
\begin{align*}
\beta_{k} \sim \text{Dir}\left(\lambda_{\beta} 1_{D}\right).
\end{align*}

We vertically stack the $N$ $\gamma_i$'s into an $N \times K$ matrix $\Gamma$.

---

### Interpretation

Topic models are well-suited to count data dimensionality reduction. The
estimated parameters can be interpreted as,
* $\Gamma \in \Delta_{K}^{N}$: Per-document memberships across $K$ topics.
* $B \in \Delta_{V}^{K}$: Per topic distributions over $V$ words.

<img src="figure/lda_overview.png"/>

---

(Need to introduce some sort of transitional idea at this point)

---

### Alignment as a Graph

We view an alignment as a graph across the ensemble. Index models by $m$ and
topics by $k$. Then,
* Nodes $V$ corresponds to topics, parameterized by $\{\beta^m_{k},
\gamma^m_{ik}\}$.
* Edges $E$ are placed between topics from neighboring models ($K$ vs.
$K + 1$ topics)
* Weights $W$ encode the similarity between topics.

---

### Notation

This graph-based view provides a convenient notation,

* $m\left(v\right)$ is the model for node $v$
* $k\left(v\right)$ is the topic for node $v$
* $\Gamma\left(v\right) := \left(\gamma_{i
v\left(k\right)}^m\left(k\right)\right) \in \reals^n_{+}$ is the vector of
mixed memberships for topic $v$
* $\beta\left(v\right) := \beta_{k}^m \in \Delta^{D}$ is the
corresponding topic distribution
* $e = \left(v, v'\right)$ is an edge linking topics $v$ and $v'$.

---

### Estimating Weights: Product approach

To compute weights, we can use,
\begin{align*}
w\left(e\right) = \Gamma\left(v\right)^T\Gamma\left(v'\right)
\end{align*}

```{r out.width = 375}
include_graphics("figure/product_alignment.png")
```

---

### Estimating Weights: Transport approach

Let $V_p$ and $V_q$ be two subsets of topics within the graph.

* Let the total "mass" of $V_p$ be $p = \left\{\Gamma\left(v\right)^T 1 : v \in V_{p}\right\}$. Define $q$ similarly.
* Define the transport cost $C\left(v, v^\prime\right) := JSD\left(\beta\left(v\right), \beta\left(v^\prime\right)\right)$, the Jensen-Shannon divergence between the pair of topic distributions.

```{r out.width = 400}
include_graphics("figure/transport_alignment.png")
```

---

### Estimating Weights: Transport approach


The weights $W$ can than be estimated by solving the optimal transport problem,
\begin{align*}
&\min_{W \in \mathcal{U}\left(p, q\right)} \left<C,W\right> \\
\mathcal{U}\left(p, q\right) := &\{W\in \mathbb{R}^{\left|p\right| \times \left|q\right|}_{+} : W 1_{\left|q\right|} = p \text{ and } W^{T} 1_{\left|p\right|^\prime} = q\}.
\end{align*}

```{r, out.width = 400}
include_graphics("figure/transport_alignment.png")
```

---

### Summaries

* Representing the models by an alignment suggests a few summary measures
* These summaries help measure topic quality
* They can also be used to diagnose model mis-specification

---

### Paths

For each $v$, identify the incoming edge with the highest normalized weight,
\begin{align*}
  e^\ast\left(v\right) = \arg \max_{e : \text{target}\left(e\right) = v} \tilde{w}_{\text{out}}\left(e\right) + \tilde{w}_{\text{in}}\left(e\right).
\end{align*}

* Iterate this process from large to small $l$ to construct a set of distinct paths along the alignment
* The number of unique paths is a useful property of an alignment

```{r}
include_graphics("figure/branch_construction-1.png")
```

---

### Paths

For each $v$, identify the incoming edge with the highest normalized weight,
\begin{align*}
  e^\ast\left(v\right) = \arg \max_{e : \text{target}\left(e\right) = v} \tilde{w}_{\text{out}}\left(e\right) + \tilde{w}_{\text{in}}\left(e\right).
\end{align*}

* Iterate this process from large to small $l$ to construct a set of distinct paths along the alignment
* The number of unique paths is a useful property of an alignment

```{r}
include_graphics("figure/branch_construction-3.png")
```

---

### Paths

For each $v$, identify the incoming edge with the highest normalized weight,
\begin{align*}
  e^\ast\left(v\right) = \arg \max_{e : \text{target}\left(e\right) = v} \tilde{w}_{\text{out}}\left(e\right) + \tilde{w}_{\text{in}}\left(e\right).
\end{align*}

* Iterate this process from large to small $l$ to construct a set of distinct paths along the alignment
* The number of unique paths is a useful property of an alignment

```{r}
include_graphics("figure/branch_construction-2.png")
```

---

### Refinement

Suppose there are actually $K_0$ topics in the data. If we fit a model with
$K$ topics, then

* $K < K_0$: True topics are merged together into "compromise" topics
* $K > K_0$: True topics are arbitrarily split

---

### Refinement

A consequence is that parent specificity differs between these regimes,

* $K < K_0$: Each topic receives most mass from a unique parent, corresponding to a true or "compromise" topic
* $K > K_0$: Each topic receives substantial mass from several parents, each
 corresponding to an arbitrary split of a true topic

.pull-left[
```{r}
include_graphics("figure/refinement-branches-2.png")
```
]

.pull-right[
```{r}
include_graphics("figure/refinement-branches.png")
```
]

---

### Coherence

The coherence of a topic is defined as its average connectedness to other topics
along the same branch,

---
