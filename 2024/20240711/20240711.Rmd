---
title: "Interactivity for Interpretable Machine Learning"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(RefManageR)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)
BibOptions(
  check.entries = TRUE, 
  bib.style = "numeric", 
  cite.style = "numeric", 
  style = "markdown",
  hyperlink = FALSE, 
  dashed = FALSE,
  max.names = 1
)
bib <- ReadBib("references.bib")
```

<div id="title">
Interactivity for Interpretable Machine Learning<br/>
</div>
<div id="under_title">
Computational Genomics Summer Institute 2024<br/>
</div>

<div id="subtitle">
Kris Sankaran <br/>
UW - Madison <br/>
11 | July | 2024 <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
<a href="https://go.wisc.edu/pl9a65">Slides</a><br/>
<!-- <img src="figures/cropped-CGSI_logo_final-2.jpg" width=100/><br/> -->
<img src="figures/slides-link.png" width=100/>
</div>

---

### Learning Objectives

By the end of this tutorial, you will be able to:

1. Compare and contrast outputs from different interpretability techniques.

1. Apply interactive computing ideas to the research code that you develop.

---

### Motivation: Outpatient Care for Pneumonia

.center[
<img src="figures/asthma.png" width=1000/>

<span style="font-size: 18px;">
Example from `r Citep(bib, "Caruana2015IntelligibleMF")`.
</span>
]


---

### Motivation: The Google Bard Demo

.center[
<img src="figures/bard-hallucination.webp" width=830/>

See the discussion in `r Citep(bib, "kundaliya2023")`.
]

---

### Motivation

1. Computers let us solve problems that would be impossible to manage any other
way, but we need some way of **checking our work**, especially when the stakes
are high.

1. We can often **improve our models** by looking more closely at what they learn
and intervening as necessary.

1. In the long-run, we'll be able to **get more out of our data and models** if we
look more critically at them.

---

class: middle

.center[
## Interpretability
]

---

### What is Interpretability?

Models with these properties tend to be more interpretable `r Citep(bib, c("Lipton2018-dt", "Murdoch2019-aw", "Doshi-Velez2017-qo", "Sankaran2024-ny"))`:

<img src="figures/simplicity.png" width=50/> **Parsimony**: The model has relatively few components. <br/><br/>

<img src="figures/crystal-ball.png" width=40/> **Simulatability**: Users can predict model behavior on new samples. <br/><br/>

<img src="figures/Lego_Brick_4.svg" width=50/> **Modularity**: The model can be broken into simpler components.

<!-- First Q&A: Would you say that a linear regression model is interpretable? -->

---

### Distinctions

1. **Interpretable Model**: A model that, by virtue of its design, is easy to
accurately describe and edit.
1. **Explainability Technique**: A method that summarizes some aspect of a black
box system.

.center[
  <img src="figures/black_box_flashlight.png" width=720/>
]

---

### Distinctions

1. **Local Explanation**: An artifact for reasoning about individual predictions.
1. **Global Explanation**: An artifact for reasoning about an entire model.

.center[
<img src="figures/explanation_types.png" width=800/>
]

---

### Illustrative Example

Imagine sampling microbiome profiles over time and seeing that some of the hosts
develop a disease. Can we discover risk factors from these data?

.center[
  <img src="figures/simulated-data.svg" width=830/>
]
<span style="font-size: 18px;">
This simulation is motivated by microbiome studies of HIV risk `r Citep(bib, 'Gosmann2017-aw')`.
</span>

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is preferable to
expert design.
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformers-analogy-2.png"/>
]

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is preferable to
expert design.
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformer_analogy.png"/>
]

---

### Representation Learning

How can we try to understand the representations $z_{i}$?

.center[
<img src="figures/representation_learning.png" width=500/>
]

---

### Embeddings

In text data, we can identify context-dependent meaning by looking for clusters
in the PCA of embeddings `r Citep(bib, "Coenen2019VisualizingAM")`.
.center[
<img src="figures/bert_context.png" width=670/>
]

---

### Embeddings

We can build the analogous visualization for our microbiome problem. Samples
that are nearby in the embedding space are similar w.r.t. predictive features.

.center[
<img src="figures/pca_comparison.svg" width=1400/>
]

---

### Interpolations

Another common technique is to analyze linear interpolations in this space 
`r Citep(bib, "Liu2019LatentSC")`.  This figure traces out the microbiome
profiles between two samples.

.center[
<img src="figures/species_21_interpolation.svg" width=940/>
]

---

### Perturbation

For local explanations, we can perturb the sample of interest and see how model
predictions change.
<br/>
<br/>
.center[
<img src="figures/perturbation.png" width=500/>
]
Examples: LIME `r Citep(bib, "10.1145/2939672.2939778")`, integrated gradients
`r Citep(bib, "10.5555/3305890.3306024")`, GradCAM `r Citep(bib, "8237336")`,
local variable importance `r Citep(bib, "Agarwal2023-np")`.

<!-- Q&A: How can we operationalize this intuition? -->

---

### Integrated Gradients

Integrated gradients are better than ordinary gradients because they are less
sensitive to saturation in the usual logistic loss.

\begin{align*}
\left(x_{i} - x_{i}'\right) \int_{\alpha \in \left[0, 1\right]} \frac{\partial f\left(x_{i}' + \alpha\left(x_{i} - x_{i}'\right)\right)}{\partial x_{i}} d\alpha
\end{align*}

.center[
  <img src="figures/integrated_gradients_animation.gif" width=750/><br/>
<span style="font-size: 18px;">
Animation from `r Citep(bib, "Sturmfels2020")`.
</span>
]

---

### Integrated Gradients

In our microbiome example, this highlights the species and timepoints that are
most responsible for the disease vs. healthy classification for each sample.

.center[
<img src="figures/microbiome_integrated_gradients.svg"/>
]

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts `r Citep(bib, "50351")`.  This is part
of a larger body of work that attempts to establish shared
language/representations for interacting with models 
`r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/koh_concept.png" width=750/> <br/>
Figure from `r Citep(bib, "50351")`.
</span>
]

---

### Concept Bottlenecks

In the microbiome example, we could define concepts like blooms or trends. These
would have to be manually annotated in the original training data.

.center[
<img src="figures/trends.png" width=1000/>
]

---

### Concept Bottlenecks

We reconfigure our transformer model to first predict the concept label before
making a final classification.

.center[
<img src="figures/concept_architecture-2.png" width=600/>
]

---

.center[
## Interactivity
]

---

### What is Interactivity?

Interactivity allows us to specify what computation we want done without having
to write code.

.pull-left[
<img src="figures/autocomplete.gif" width=300/><br/>
<span style="font-size: 18px;">
Text entry can be considered a type of interaction.
</span>
]

.pull-right[
<img src="figures/delegate-calc-1.gif"/>
<span style="font-size: 18px;">
An interactive delegate calculator created by the NYT `r Citep(bib,
"vis4DefenseInteractive")`.
</span>
]

---


### <span style="col: #D93611;">Focus-plus-Context</span>

We can let readers zoom into patterns of interest without losing relevant
context. This has a flavor of "analyzing the residuals."
<br/>
<br/>

.pull-left[
> Overview first, zoom and filter, then details on demand.

-- Schneiderman's "Visual Information Seeking Mantra" `r Citep(bib, "Shneiderman2002-ju")`.
]

.pull-right[
<img src="figures/structuration.png" width=250/>
<span style="font-size: 18px;"><br/>
Tukey's iterative structuration, as imagined by `r Citep(bib, "Holmes1993-fl")`.
</span>
]

---

### Example: Tree Navigation

Large trees can be difficult to explore. Focus-plus-context gives a natural way
of navigating them, recomputing the view according to user interactions  `r Citep(bib, c("Heer2004DOITreesRS", "sankaran_interactive_2018"))`.

.center[
<iframe src="https://krisrs1128.github.io/treelapse/pages/antibiotic.html#htmlwidget-dd8d9e7ec77f2a8cc333" width=900 height=380></iframe>
]

---

### Example: Dimensionality Reduction

We can use focus-plus-context to compare topics across models with different
complexity `r Citep(bib, c("Symul2023-ug", "Fukuyama2023-ph"))`. Low $K$ gives
an overview, large $K$ gives details.

.center[
<img src="figures/vaginal_microbiome_alto.jpg" width=900/>
]

---

### Linked Views

.pull-left[
1. We can navigate higher dimensions by linking low-dimensional views `r Citep(bib, c("Becker1987-rc", "Buja1996-sq"))`.

1. Notice that we're defining queries graphically, not just through selection menus.
]

.pull-right[
<iframe src="https://connect.doit.wisc.edu/content/6df2063a-1c7d-4f01-b98c-3aebed82d190/" allowfullscreen="" data-external="1" height=500 width=600></iframe>
]

<!-- Q&A: What are some interesting questions you'd like to answer on this flight delay data? -->

---

### Example: Multiple Testing

We can use linked views to navigate a collection of hypothesis tests. Each
letter corresponds to an experimental factor (details in `r Citep(bib,
"Chevrette2022-om")`).

.center[
<iframe src="https://connect.doit.wisc.edu/content/7d109162-8690-4c84-8563-4bdee8f15ca0" width=990 height=400></iframe>
]

---

### Example: Model Evaluation

This visualization uses both linked views and the focus-plus-context principle
to help evaluate the quality of a single-cell simulator. 

---

### Software

.pull-left[
**Interpretability**

1. Captum (Python)
1. DALEX (R)
1. imodels (Python)
1. interpretml (Python)

Review Paper<br/> - [Code Repository](https://go.wisc.edu/3k1ewe)

]


.pull-right[
**Interactivity**

1. Shiny (R/Python)
1. Vega/Vega-lite (JS, Python, R)
1. D3 (JS)
1. p5 (JS)
1. Jupyter Widgets (Python)

Visualization Course<br/> - Notes [I](https://krisrs1128.github.io/stat679_notes/), [II](https://krisrs1128.github.io/stat436_s24/)<br/>- Recordings [I](https://www.youtube.com/watch?v=cc__b5R6OzA&list=PLhax_7Mawcfk1GEl_vOg7cE_vtRTsqMWw&pp=gAQBiAQB), [II](https://mediaspace.wisc.edu/channel/STAT+479%3A+Statistical+Data+Visualization/197911113)
]


---

class: middle

.center[
## Augmentation and AI
]

---

### AI and IA - Design Space

Computers are good at accurately reaching well-defined goals, but people are
good at criticism and planning. How can we get the best of both worlds?

**Artificial Intelligence (AI)**: Solve problems directly.<br/>
**Intelligence Augmentation (IA)**: Enhance problem solving ability.
<br/>
.center[
<img src="figures/dm_vs_ai.png" width=800/>
]

---

### Guide-Decide Loop

1. The AI make suggestions that we can choose to reject, revise, or accept 
`r Citep(bib, c("predictive-interaction", "Heer2019-ie"))`.

1. This depends on their being a shared representation that links the frontend
(for human interpretation) with the backend (for model prediction). 

.center[
<img src="figures/guide-decide.png" width=700/>
]

---

### Interactive Translation

A well-designed interface helps professional translators achieve better results
than simply editing the output of an automated translation system
`r Citep(bib, c("10.1145/2642918.2647408", "green-etal-2014-human", "10.1145/2470654.2470718"))`.

.center[
<img src="figures/phrasal.png" width=800/>
]

Predictions can be updated in response to interactions. The shared
representation here is natural language.

---

### Conclusion

.pull-left[
1. It takes effort to specify and validate models. The literatures on
interpretability and interactivity can help us to think systematically about
this work.

1. More than just help answer questions, these tools can help shape good
questions during day-to-day analysis.
]

.pull-right[
<img src="figures/question_evidence.png" width=420/><br/>
<span style="font-size: 18px;">
Figure from `r Citep(bib, "simplystatisticsSimplyStatistics")`'s reflection on `r Citep(bib, "Tukey1962-vv")`.
</span>
]

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 3)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 4, end = 8)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 9, end = 13)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 14, end = 17)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 18, end = 22)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 23, end = 25)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 26, end = 29)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 30, end = 35)
```


---

### Figure Attribution

Simplicity by M. Oki Orlando from <a href="https://thenounproject.com/browse/icons/term/simplicity/" target="_blank" title="Simplicity Icons">Noun Project</a> (CC BY 3.0)

Crystal Ball by Kiki Rizky from <a href="https://thenounproject.com/browse/icons/term/crystal-ball/" target="_blank" title="Crystal Ball Icons">Noun Project</a> (CC BY 3.0)

---