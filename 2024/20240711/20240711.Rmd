---
title: "Interactivity for Interpretable Machine Learning"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(RefManageR)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)
BibOptions(
  check.entries = TRUE, 
  bib.style = "numeric", 
  cite.style = "numeric", 
  style = "markdown",
  hyperlink = FALSE, 
  dashed = FALSE,
  max.names = 8
)
bib <- ReadBib("references.bib")
```

<div id="title">
Interactivity for Interpretable Machine Learning<br/>
</div>
<div id="under_title">
Computational Genomics Summer Institute 2024<br/>
</div>

<div id="subtitle">
Kris Sankaran <br/>
UW - Madison <br/>
11 | July | 2024 <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
<a href="https://go.wisc.edu/pl9a65">Slides</a><br/>
<!-- <img src="figures/cropped-CGSI_logo_final-2.jpg" width=100/><br/> -->
<img src="figures/slides-link.png" width=100/>
</div>

---

### Learning Objectives

By the end of this tutorial, you will be able to:

1. Compare and contrast outputs from different interpretability techniques.

1. Apply interactive computing ideas to the research code that you develop.

---

### Motivation: Outpatient Care for Pneumonia

.center[
<img src="figures/asthma.png" width=1000/>

<span style="font-size: 18px;">
Example from `r Citep(bib, "Caruana2015IntelligibleMF")`.
</span>
]

<!-- Q&A This is pretty counterintuitive. What do you think is going on? -->

---

### Motivation: The Google Bard Demo

.center[
<img src="figures/bard-hallucination.webp" width=830/>

See the discussion in `r Citep(bib, "kundaliya2023")`.
]

---

### Motivation

1. Computers let us solve problems that would be impossible to manage any other
way, but we need some way of **checking our work**, especially when the stakes
are high.

1. We can often **improve our models** by looking more closely at what they learn
and intervening as necessary.

1. In the long-run, we'll be able to **get more out of our data and models** if we
look more critically at them.

---

class: middle

.center[
## Interpretability
]

---

### What is Interpretability?

Models with these properties tend to be more interpretable `r Citep(bib, c("Lipton2018-dt", "Murdoch2019-aw", "Doshi-Velez2017-qo", "Sankaran2024-ny"))`:

<img src="figures/simplicity.png" width=50/> **Parsimony**: The model has relatively few components. <br/><br/>

<img src="figures/crystal-ball.png" width=40/> **Simulatability**: Users can predict model behavior on new samples. <br/><br/>

<img src="figures/Lego_Brick_4.svg" width=50/> **Modularity**: The model can be broken into simpler components.

<!-- First Q&A: Would you say that a linear regression model is interpretable? -->

---

### Distinctions

1. **Interpretable Model**: A model that, by virtue of its design, is easy to
accurately describe and edit.
1. **Explainability Technique**: A method that summarizes some aspect of a black
box system.

.center[
  <img src="figures/black_box_flashlight.png" width=720/>
]

---

### Distinctions

1. **Local Explanation**: An artifact for reasoning about individual predictions.
1. **Global Explanation**: An artifact for reasoning about an entire model.

.center[
<img src="figures/explanation_types.png" width=800/>
]

---

### Illustrative Example

Imagine sampling microbiome profiles over time and seeing that some of the hosts
develop a disease. Can we discover risk factors from these data?

.center[
  <img src="figures/simulated-data.svg" width=830/>
]
<span style="font-size: 18px;">
This simulation is motivated by microbiome studies of HIV risk over time `r Citep(bib, 'Gosmann2017-aw')`.
</span>

---

### Transformers

.pull-left[
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
1. The hope is that this model automatically discovers temporal and taxonomic
features that distinguish the classes.
]

.pull-right[
<img src="figures/transformers-analogy-2.png"/>
]

---

### Transformers

.pull-left[
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
1. The hope is that this model automatically discovers temporal and taxonomic
features that distinguish the classes.

Transformer: 83.2%<br/>
Lasso: 78%<br/>
Lasso (True Features): 87.8%
]

.pull-right[
<img src="figures/transformer_analogy.png"/>
]

---

### Representation Learning

How can we try to understand the transformer embeddings $\tilde{\mathbf{z}}_{i}$?
<br/>
<br/>
.center[
<img src="figures/transformer_layers.png" width=900/>
]

---

### Representation Learning

We can use PCA to explore the embedding space `r Citep(bib, "Coenen2019VisualizingAM")`.
<br/>
<br/>
.center[
<img src="figures/transformer_layers_pca.png" width=650/>
]

---

### Embeddings

We can interpolate between samples to understand the resulting embedding space.

.center[
<img src="figures/species_21_interpolation.svg" width=940/>
]


---

### Perturbation

To understand predictions at individual samples, we can use *local*
explanations. These apply a perturbation and quantify the results.
<br/>
<br/>

.center[
<img src="figures/perturbation.png" width=500/>
]
Examples: LIME `r Citep(bib, "10.1145/2939672.2939778")`, integrated gradients
`r Citep(bib, "10.5555/3305890.3306024")`, GradCAM `r Citep(bib, "8237336")`,
local variable importance `r Citep(bib, "Agarwal2023-np")`.

<!-- Q&A: How can we operationalize this intuition? -->

---

### Integrated Gradients

Integrated gradients are better than ordinary gradients because they are less
sensitive to saturation in the usual logistic loss.

\begin{align*}
\left(x_{i} - x_{i}'\right) \int_{\alpha \in \left[0, 1\right]} \frac{\partial f\left(x_{i}' + \alpha\left(x_{i} - x_{i}'\right)\right)}{\partial x_{i}} d\alpha
\end{align*}

.center[
  <img src="figures/integrated_gradients_animation.gif" width=750/><br/>
<span style="font-size: 18px;">
Interactive visualization from `r Citep(bib, "Sturmfels2020")`.
</span>
]

---

### Integrated Gradients

In our microbiome example, this highlights the species and timepoints that are
most responsible for the disease vs. healthy classification for each sample.

.center[
<img src="figures/microbiome_integrated_gradients.svg"/>
]

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts `r Citep(bib, "50351")`.  This is part
of a larger body of work that attempts to establish shared
language/representations for interacting with models 
`r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/koh_concept.png" width=750/> <br/>
Figure from `r Citep(bib, "50351")`.
</span>
]

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts `r Citep(bib, "50351")`.  This is part
of a larger body of work that attempts to establish shared
language/representations for interacting with models 
`r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/concept_architecture-2.png" width=600/>
</span>
]

---

### Concept Bottlenecks

In the microbiome example, we could define concepts like blooms or trends. These
would have to be manually annotated in the original training data. The test set
performance is comparable to before (84%).

.center[
<img src="figures/trends.png" width=1000/>
]

---

.center[
## Interactivity
]

---

### What is Interactivity?

Interactivity allows us to specify what computation we want done without having
to write code.

.pull-left[
<img src="figures/autocomplete.gif" width=300/><br/>
<span style="font-size: 18px;">
Text entry is a type of interaction.
</span>
]

.pull-right[
<img src="figures/delegate-calc-1.gif"/>
<span style="font-size: 18px;">
An interactive delegate calculator created by the NYT `r Citep(bib,
"vis4DefenseInteractive")`.
</span>
]

---


### <span style="col: #D93611;">Focus-plus-Context</span>

We can let readers zoom into patterns of interest without losing relevant
context. This has a flavor of "analyzing the residuals."
<br/>
<br/>

.pull-left[
> Overview first, zoom and filter, then details on demand.

-- Schneiderman's "Visual Information Seeking Mantra" `r Citep(bib, "Shneiderman2002-ju")`.
]

.pull-right[
.center[<img src="figures/structuration.png" width=250/>]
<span style="font-size: 18px;">
Tukey's iterative structuration, as interpreted by `r Citep(bib, "Holmes1993-fl")`.
</span>
]

---

### Example: Tree Navigation

Large trees can be difficult to explore. Focus-plus-context gives a natural way
of navigating them, updating the view according to user interactions  `r Citep(bib, c("Heer2004DOITreesRS", "sankaran_interactive_2018"))`.

.center[
<iframe src="https://krisrs1128.github.io/treelapse/pages/antibiotic.html#htmlwidget-dd8d9e7ec77f2a8cc333" width=900 height=380></iframe>
]

---

### Example: Dimensionality Reduction

We can use focus-plus-context to compare topics across models with different
complexity `r Citep(bib, c("Symul2023-ug", "Fukuyama2023-ph"))`. Low $K$ gives
context, large $K$ gives focus.

.center[
<img src="figures/vaginal_microbiome_alto.jpg" width=900/>
]

---

### Linked Views

.pull-left[
1. We can navigate higher dimensions by linking low-dimensional views `r Citep(bib, c("Becker1987-rc", "Buja1996-sq"))`.

1. Notice that we're defining queries graphically, not just through selection menus.
]

.pull-right[
<iframe src="https://connect.doit.wisc.edu/content/6df2063a-1c7d-4f01-b98c-3aebed82d190/" allowfullscreen="" data-external="1" height=500 width=600></iframe>
]

<!-- Q&A: What are some interesting questions you'd like to answer on this flight delay data? -->

---

### Example: Model Evaluation

We can use these ideas to evaluate the quality of a single-cell simulator. 

<iframe src="http://localhost:5000/" width=950 height=450/>

---

### Software

.pull-left[
**Interpretability**

1. Captum (Python)
1. DALEX (R)
1. imodels (Python)
1. interpretml (Python)

Review Paper<br/> - [Code Repository](https://go.wisc.edu/3k1ewe)

]


.pull-right[
**Interactivity**

1. Shiny (R/Python)
1. Vega/Vega-lite (JS, Python, R)
1. D3 (JS)
1. p5 (JS)
1. Jupyter Widgets (Python)

Visualization Course<br/> - Notes [I](https://krisrs1128.github.io/stat679_notes/), [II](https://krisrs1128.github.io/stat436_s24/)<br/>- Recordings [I](https://www.youtube.com/watch?v=cc__b5R6OzA&list=PLhax_7Mawcfk1GEl_vOg7cE_vtRTsqMWw&pp=gAQBiAQB), [II](https://mediaspace.wisc.edu/channel/STAT+479%3A+Statistical+Data+Visualization/197911113)
]


---

class: middle

.center[
## Augmentation and AI
]

---

### AI and IA - Design Space

Computers are good at accurately reaching well-defined goals, but people are
good at criticism and planning. How can we get the best of both worlds?

**Artificial Intelligence (AI)**: Solve problems without human intervention.<br/>
**Intelligence Augmentation (IA)**: Enhance human problem solving ability.
<br/>
<br/>
.center[
<img src="figures/dm_vs_ai.png" width=800/>
]

---

### Guide-Decide Loop

1. In an interactive setting, AI can make suggestions for the next step in an
analysis `r Citep(bib, c("predictive-interaction", "Heer2019-ie"))`.

1. This depends on their being a shared representation that links the frontend
(for human interpretation) with the backend (for model prediction). 

.center[
<img src="figures/guide-decide.png" width=700/>
]

---

### Interactive Translation

A well-designed interface helps professional translators achieve better results
than simply editing the output of an automated translation system
`r Citep(bib, c("10.1145/2642918.2647408", "green-etal-2014-human", "10.1145/2470654.2470718"))`.

.center[
<img src="figures/phrasal.png" width=800/>
]

Predictions can be updated in response to interactions. The shared
representation here is natural language.

---

### Conclusion

1. Interpretability and interactivity can simplify the process of specifying and
validating models.

1. Good research questions arise from carefully looking at your data and models,
and these tools can help.

---

### Thank you!

* Lab Members: Margaret Thairu, Hanying Jiang, Shuchen Yan, Yuliang Peng, Kaiyan Ma, Kai Cui, and Kobe Uko.
* Funding: NIGMS R01GM152744.



---

class: reference

<h3 style="font-size: 22px;">References</h3>
```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 14)
```

---

class: reference

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 15, end = 29)
```

---

class: reference

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 30, end = 33)
```

---

### Figure Attribution

Simplicity by M. Oki Orlando from <a href="https://thenounproject.com/browse/icons/term/simplicity/" target="_blank" title="Simplicity Icons">Noun Project</a> (CC BY 3.0)

Crystal Ball by Kiki Rizky from <a href="https://thenounproject.com/browse/icons/term/crystal-ball/" target="_blank" title="Crystal Ball Icons">Noun Project</a> (CC BY 3.0)

---

### Example: Multiple Testing

Linked views can help make sense of a collection of hypothesis tests. Each
letter corresponds to an experimental factor (details in `r Citep(bib, "Chevrette2022-om")`).

.center[
<iframe src="https://connect.doit.wisc.edu/content/7d109162-8690-4c84-8563-4bdee8f15ca0" width=1400 height=600></iframe>
]

---
