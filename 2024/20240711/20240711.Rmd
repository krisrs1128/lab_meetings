---
title: "Interactivity for Interpretable Machine Learning"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(RefManageR)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)
BibOptions(
  check.entries = TRUE, 
  bib.style = "numeric", 
  cite.style = "numeric", 
  style = "markdown",
  hyperlink = FALSE, 
  dashed = FALSE,
  max.names = 1
)
bib <- ReadBib("references.bib")
```

<div id="title">
Interactivity for Interpretable Machine Learning<br/>
</div>
<div id="under_title">
Computational Genomics Summer Institute 2024<br/>
</div>

<div id="subtitle">
Kris Sankaran <br/>
UW - Madison <br/>
11 | July | 2024 <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
[Slides](https://go.wisc.edu/8k8r2q)<br/>
<!-- <img src="figures/cropped-CGSI_logo_final-2.jpg" width=100/><br/> -->
<img src="figures/slides-link.png" width=100/>
</div>

---

### Learning Objectives

By the end of this tutorial, you will be able to:

1. Compare and contrast outputs from different interpretability techniques.

1. Apply interactive computing ideas to the research code that you develop.

---

### Motivation: Outpatient Care for Pneumonia

.center[
<img src="figures/asthma.png" width=1000/>

<span style="font-size: 18px;">
Example from `r Citep(bib, "Caruana2015IntelligibleMF")`.
</span>
]


---

### Motivation: The Google Bard Demo

.center[
<img src="figures/bard-hallucination.webp" width=830/>

See the discussion in `r Citep(bib, "kundaliya2023")`.
]

---

### Know Your Data

1. Computers let us solve problems that would be impossible to manage any other
way, but we need some way of **checking our work**, especially when there are
real-world consequences.

1. We can often **improve our models** by looking more closely at what they learn
and intervening as necessary.

1. In the long-run, we'll be able to **get more out of our data and models** if we
look more critically at them.

---

class: middle

.center[
## Interpretability
]

---

### What is Interpretability?

Models with these properties tend to be more interpretable `r Citep(bib, c("Lipton2018-dt", "Murdoch2019-aw", "Doshi-Velez2017-qo", "Sankaran2024-ny"))`:

<img src="figures/simplicity.png" width=50/> **Parsimony**: The model has relatively few components. <br/><br/>

<img src="figures/crystal-ball.png" width=40/> **Simulatability**: Users can predict model behavior on new samples. <br/><br/>

<img src="figures/Lego_Brick_4.svg" width=50/> **Modularity**: The model can be broken into simpler components.

<!-- First Q&A: Would you say that a linear regression model is interpretable? -->

---

### Distinctions

1. **Interpretable Model**: A model that, by virtue of its design, is easy to
accurately describe and edit.
1. **Explainability Technique**: A method that summarizes some aspect of a black
box system.

.center[
  <img src="figures/black_box_flashlight.png" width=720/>
]

---

### Distinctions

1. **Local Explanation**: An artifact for reasoning about individual predictions.
1. **Global Explanation**: An artifact for reasoning about an entire model.

.center[
<img src="figures/explanation_types.png" width=800/>
]

---

### Illustrative Example

Imagine sampling microbiome profiles over time and seeing that some of the hosts
develop a disease. Can we discover risk factors from these data?

.center[
  <img src="figures/simulated-data.svg" width=830/>
]
<span style="font-size: 18px;">
This simulation is motivated by microbiome studies of HIV risk `r Citep(bib, 'Gosmann2017-aw')`.
</span>

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is preferable to
expert design.
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformers-analogy-2.png"/>
]

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is preferable to
expert design.
1. We apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformer_analogy.png"/>
]

---

### Representation Learning

How can we try to understand the representations $z_{i}$?

.center[
<img src="figures/representation_learning.png" width=700/>
]

---

### Embeddings

In text data, we can identify context-dependent meaning by looking for clusters
in the PCA of embeddings `r Citep(bib, "Coenen2019VisualizingAM")`.
.center[
<img src="figures/bert_context.png" width=670/>
]

---

### Embeddings

We can build the analogous visualization for our microbiome problem. Samples
that are nearby in the embedding space are similar w.r.t. predictive features.

.center[
<img src="figures/pca_comparison.svg" width=1400/>
]

---

### Interpolations

Another common technique is to analyze linear interpolations in this space 
`r Citep(bib, "Liu2019LatentSC")`.  This figure traces out the microbiome
profiles between two samples.

.center[
<img src="figures/species_21_interpolation.svg" width=940/>
]

---

### Perturbation

For local explanations, we can perturb the sample of interest and see how model
predictions change.
<br/>
<br/>
.center[
<img src="figures/perturbation.png" width=500/>
]
Examples: LIME `r Citep(bib, "10.1145/2939672.2939778")`, integrated gradients
`r Citep(bib, "10.5555/3305890.3306024")`, GradCAM `r Citep(bib, "8237336")`,
local variable importance `r Citep(bib, "Agarwal2023-np")`.

<!-- Q&A: How can we operationalize this intuition? -->

---

### Integrated Gradients

Integrated gradients are better than ordinary gradients because they are less
sensitive to saturation in the usual logistic loss.

\begin{align*}
\left(x_{i} - x_{i}'\right) \int_{\alpha \in \left[0, 1\right]} \frac{\partial f\left(x_{i}' + \alpha\left(x_{i} - x_{i}'\right)\right)}{\partial x_{i}} d\alpha
\end{align*}

.center[
  <img src="figures/integrated_gradients_animation.gif" width=750/><br/>
<span style="font-size: 18px;">
Animation from `r Citep(bib, "Sturmfels2020")`.
</span>
]

---

### Integrated Gradients

In our microbiome example, this highlights the species and timepoints that are
most responsible for the disease vs. healthy classification for each sample.

.center[
<img src="figures/microbiome_integrated_gradients.svg"/>
]

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts `r Citep(bib, "50351")`.  This is part
of a larger body of work that attempts to establish shared
language/representations for interacting with models 
`r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/koh_concept.png" width=750/> <br/>
Figure from `r Citep(bib, "50351")`.
</span>
]

---

### Concept Bottlenecks

In the microbiome example, we could define concepts like blooms or trends. These
would have to be manually annotated in the original training data.

.center[
<img src="figures/trends.png" width=1000/>
]

---

### Concept Bottlenecks

We reconfigure our transformer model to first predict the concept label before
making a final classification.

.center[
<img src="figures/concept_architecture-2.png" width=600/>
]

---

.center[
## Interactivity
]

---

### What is Interactivity?

Interactivity allows us to specify what computation we want done without writing
code.

.pull-left[
<img src="figures/autocomplete.gif" width=350/><br/>
<span style="font-size: 18px;">
Text entry can be considered a type of interaction.
</span>
]

.pull-right[
<img src="figures/delegate-calc-1.gif"/>
<span style="font-size: 18px;">
An interactive delegate calculator created by the NYT `r Citep(bib,
"vis4DefenseInteractive")`.
</span>
]

---


### <span style="col: #D93611;">Focus-plus-Context</span>

We can let readers zoom into patterns of interest without losing relevant
context. This has a flavor of "analyzing the residuals."
<br/>
<br/>

.pull-left[
> Overview first, zoom and filter, then details on demand.

-- Schneiderman's "Visual Information Seeking Mantra" `r Citep(bib, "Shneiderman2002-ju")`.
]

.pull-right[
<img src="figures/structuration.png"/>
<span style="font-size: 18px;">
Tukey's iterative structuration, as imagined by `r Citep(bib, "Holmes1993")`.
</span>
]

---

### Example: Tree Navigation

Large trees can be difficult to explore. Focus-plus-context gives a natural way
of navigating them, recomputing the view according to user interactions.  `r Citep(bib, c("Heer2004DOITreesRS", "sankaran_interactive_2018"))`.

.center[
<iframe src="https://krisrs1128.github.io/treelapse/pages/antibiotic.html#htmlwidget-dd8d9e7ec77f2a8cc333" width=900 height=380></iframe>
]

---

### Example: Dimensionality Reduction

We can use focus-plus-context to compare topics across models with different
complexity `r Citep(bib, c("Symul2023-ug", "Fukuyama2023-ph"))`. Low $K$ gives
an overview, large $K$ gives details.

.center[
<img src="figures/vaginal_microbiome_alto.jpg" width=900/>
]

---

### Linked Views

.pull-left[
1. We can navigate higher dimensions by linking low-dimensional views `r Citep(bib, c("Becker1987-rc", "Buja1996-sq"))`.

1. Notice that we're defining queries graphically, not just through selection menus.
]

.pull-right[
<iframe src="https://connect.doit.wisc.edu/content/6df2063a-1c7d-4f01-b98c-3aebed82d190/" allowfullscreen="" data-external="1" height=500 width=600></iframe>
]

<!-- Q&A: What are some interesting questions you'd like to answer on this flight delay data? -->

---

### Example: Multiple Testing

We can use linked views to navigate a collection of hypothesis tests. Each
letter corresponds to an experimental factor.

.center[
<iframe src="https://connect.doit.wisc.edu/content/7d109162-8690-4c84-8563-4bdee8f15ca0" width=990 height=400></iframe>
]

---

### Example: Model Evaluation

This visualization uses both linked views and the focus-plus-context principle
to help evaluate the quality of a single-cell simulator. 

---

### Software

.pull-left[
**Interpretability**

1. Captum (python)
1. DALEX (R)
1. imodels (python)
1. interpretml (python)

Review Paper<br/> - [Code Repository](https://go.wisc.edu/3k1ewe)

]


.pull-right[
**Interactivity**

1. Shiny (R/Python)
1. D3 (Javascript)
1. p5 (Javascript)
1. Jupyter Widgets (python)

Visualization Course<br/> - Notes [I](https://krisrs1128.github.io/stat679_notes/), [II](https://krisrs1128.github.io/stat436_s24/)<br/>- Recordings [I](https://www.youtube.com/watch?v=cc__b5R6OzA&list=PLhax_7Mawcfk1GEl_vOg7cE_vtRTsqMWw&pp=gAQBiAQB), [II](https://mediaspace.wisc.edu/channel/STAT+479%3A+Statistical+Data+Visualization/197911113)
]


---

class: middle

.center[
## Augmentation and AI
]

---

### AI and IA - Design Space

Computers are good at accurately reaching well-defined goals, but people are
good at criticism and planning. How can we get the best of both worlds?

**Artificial Intelligence (AI)**: Solve problems directly.<br/>
**Intelligence Augmentation (IA)**: Enhance problem solving ability.
<br/>
.center[
<img src="figures/dm_vs_ai.png" width=800/>
]

---

### Guide-Decide Loop

1. The AI make suggestions that we can choose to reject, revise, or accept 
`r Citep(bib, c("predictive-interaction", "Heer2019-ie"))`.

1. This depends on their being a shared representation that links the frontend
(for human interpretation) with the backend (for model prediction). 

.center[
<img src="figures/guide-decide.png" width=700/>
]

---

### Interactive Translation

A well-designed interface helps professional translators achieve better results
than simply editing the output of an automated translation system
`r Citep(bib, c("10.1145/2642918.2647408", "green-etal-2014-human", "10.1145/2470654.2470718"))`.

.center[
<img src="figures/phrasal.png" width=800/>
]

Predictions can be updated in response to interactions. The shared
representation here is natural language.

---

### Conclusion

We'll see throughout this retreat ways in which models can help generate useful
catalogs, improve engineering processes, and uncover the truth.

.center[
  Include a figure that summarizes a few of these talks
  phylogenetic tree
  single cell map
  EHR or drug development picture
]

These problems aren't solved by models in isolation -- there is human labor
involved in specification and oversight. Interpretability and interactivity can
make this work more accessible and reliable.

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 3)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 4, end = 7)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 8, end = 10)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 11, end = 15)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 16, end = 20)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 21, end = 25)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 26, end = 30)
```

---

### Figure Attribution

Simplicity by M. Oki Orlando from <a href="https://thenounproject.com/browse/icons/term/simplicity/" target="_blank" title="Simplicity Icons">Noun Project</a> (CC BY 3.0)

Crystal Ball by Kiki Rizky from <a href="https://thenounproject.com/browse/icons/term/crystal-ball/" target="_blank" title="Crystal Ball Icons">Noun Project</a> (CC BY 3.0)

---