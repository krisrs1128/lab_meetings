---
title: "Interactivity for Interpretable Machine Learning"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
---

class: title

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(RefManageR)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)
BibOptions(
  check.entries = TRUE, 
  bib.style = "numeric", 
  cite.style = "numeric", 
  style = "markdown",
  hyperlink = FALSE, 
  dashed = FALSE,
  max.names = 1
)
bib <- ReadBib("references.bib")
```

<div id="title">
Interactivity for Interpretable Machine Learning<br/>
<br/>
</div>

<div id="subtitle">
Kris Sankaran <br/>
UW - Madison <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
11 | July | 2024 <br/>
Computational Genomics Summer Institute <br/>
Slides: <a href="https://go.wisc.edu/">go.wisc.edu/</a> 
</div>

---

### Motivation: Knowing your Data

We might identify new structure (or issues) by going behind high-level metrics
or summaries.

---

class: middle

.center[
## Interpretability
]

---

### What is Interpretability?

---

### Distinctions

1. **Interpretable Model**: A model that, by virtue of its design, is easy to
accurately describe and edit.
1. **Explainability Technique**: A method that summarizes some aspect of a black
box system.

.center[
  <img src="figures/black_box_flashlight.png" width=720/>
]

---

### Distinctions

1. **Local Explanation**: An artifact for reasoning about individual predictions.
1. **Global Explanation**: An artifact for reasoning about an entire model.

.center[
<img src="figures/explanation_types.png" width=800/>
]

---

### Illustrative Example

Problem: Imagine sampling longitudinal microbiome profiles from 500 study
participants, some of whom eventually developed a disease. Can we discovery any
microbiome-related risk factors?  This simulation is motivated by microbiome studies of HIV risk
`r Citep(bib, 'Gosmann2017LactobacillusDeficientCB')`.

.center[
  <img src="figures/simulated-data.svg" width=830/>
]

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is more general
than expert design.
1. We can apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformers-analogy-2.png"/>
]

---

### Transformers

.pull-left[
1. A principle of deep learning is that end-to-end optimization is more general
than expert design.
1. We can apply the GPT2 architecture to our problem, viewing a sequence of
microbiome profiles like a sequence of words.
]

.pull-right[
<img src="figures/transformer_analogy.png"/>
]

---

### Embeddings

In text data, we can understand context-dependent meaning by looking for
clusters in the PCA of embeddings `r Citep(bib, "Coenen2019VisualizingAM")`.
These represent a type of interaction.
.center[
<img src="figures/bert_context.png" width=670/>
]

---

### Embeddings

We can build the analogous visualization for our microbiome problem. Samples
that are nearby in the embedding space are similar w.r.t. predictive features.

.center[
<img src="figures/pca_comparison.svg" width=1400/>
]

---

### Interpolations

Another common technique is to analyze linear interpolations in this space 
`r Citep(bib, "Liu2019LatentSC")`.  This figure traces out the microbiome
profiles between two samples.

.center[
<img src="figures/species_21_interpolation.svg" width=940/>
]

---

### Perturbation

To explain a generic modelâ€™s decision on an instance, we can perturb it and see
how the prediction changes.

<img src="figures/perturbation_types.png"/>

---

### Integrated Gradients

For example, we can compute the gradient of each class as we perturb a reference
towards a sample of interest.

\begin{align*}
\left(x_{i} - x_{i}'\right) \int_{\alpha \in \left[0, 1\right]} \frac{\partial f\left(x_{i}' + \alpha\left(x_{i} - x_{i}'\right)\right)}{\partial x_{i}} d\alpha
\end{align*}

.center[
  <img src="figures/integrated_gradients_animation.gif" width=1200/>
]

---

### Integrated Gradients

In our microbiome example, this can highlight the species and timepoints that
are most responsible for the disease vs. healthy classification of each example.

.center[
<img src="figures/microbiome_integrated_gradients.svg"/>
]

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts `r Citep(bib, "Koh2020ConceptBM")`.
This is part of a larger body of work that attempts to establish shared
language/representations for interacting with models.

.center[
<img src="figures/koh_concept.png" width=750 style="position: absolute; top: 340px; left: 300px"/> 
]

---

### Concept Bottlenecks

In the microbiome example, we could define interpretable "concepts" by looking
at the taxa trends for commonly co-varying groups of species.

.center[
<img src="figures/concept_1.svg"/>
]

---

### Concept Bottlenecks

We reconfigure our transformer model to first predict the concept label before
making a final classification.

.center[
<img src="figures/concept_architecture.png"/>
]

---

### Concept Bottlenecks

---

.center[
## Interactivity
]

---

### What is Interactivity?

An interface is interactive if it can respond to user inputs. 

* 

---


### <span style="col: #D93611;">Focus-plus-Context</span>

The focus-plus-context principle is to allow readers to zoom into patterns of
interest without losing relevant context `r Citep(bib, "Heer2004DOITreesRS")`.

> "Overview first, details-on-demand."

---

### Example: Degree-of-Interest

<!-- You may have to install this again -->
<iframe src="krisrs1128.github.io/treelapse/"

---

### Example: Dimensionality Reduction

We can use focus-plus-context to compare topics across a range of $K$ from a
mixed-membership model `r Citep(bib, "Symul2022")`. Low $K$ gives an overview,
large $K$ gives details.

```{r, out.width = 750}
include_graphics("figures/alto_sketches_annotated alignment.png")
```

---

### Linked Views

1. By linking many low-dimensional views, we can begin understanding
higher-dimensional relationships.

1. User queries can be defined visually, not just through selection menus. This
increases information density.

---

### Example: Multiple Testing

We can use linked views to navigate a collection of hypothesis tests. Each
letter corresponds to an experimental factor.

<iframe src="https://connect.doit.wisc.edu/content/7d109162-8690-4c84-8563-4bdee8f15ca0" width=990 height=400>
</iframe>

---

### Example: Model Evaluation

This visualization uses both linked views and the focus-plus-context principle
to help evaluate the quality of a single-cell simulator. 

---

class: middle

.center[
## Augmentation and AI
]

---

### AI and IA - Design Space

Computers are good at scaling repetitive operations, but people are good at
criticism and planning. How can we get the best of both worlds?

.pull-left[
* Artificial Intelligence (AI): Solves problems directly.

* Intelligence Augmentation (IA): Enhances problem solving ability.
]

.pull-right[
  Figure showing the trade-off. I think this was in Design Study Methodology.
  Accurate vs. inaccurate system.
  Well-defined vs. open-ended tasks.
]

---

### Guide-Decide Loop

1. We can have the AI make suggestions and then design interactions that allow
users to reject, revise, or accept changes.

1. This depends on their being a good shared representation that links the
frontend (human interaction) and backend (computation). 

---

### Interactive Cleaning

The wrangler system recommends transformations based on what the interactions
that the user has made so far.

A domain-specific language for data cleaning serves as the shared
representation.

---

### Interactive Translation

A well-designed interface could help professional translators achieve better
results than simply editing text that was generated by an automated translation
system.

The natural language itself serves as the shared representation between front
and backends.

---

### Conclusion

We'll see throughout this retreat ways in which models can help generate useful
catalogs, streamline design processes, and uncover the truth.

These problems are not solved by models in isolation. There is usually some
human labor involved either for specification or oversight. Interpretability and
interactivity help with these issues.

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 3)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 4, end = 7)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 8, end = 11)
```

---

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 12, end = 16)
```

---

### Figure Attribution