{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import PIL\n",
    "import captum as cp\n",
    "import glob\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.utils.data as dt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filename):\n",
    "    img = PIL.Image.open(filename).convert(\"RGB\")\n",
    "    return transform(img)\n",
    "\n",
    "def transform(img):\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )(img)\n",
    "\n",
    "def load_tensors(class_name, root_path='data/tcav/image/imagenet/', transform=True):\n",
    "    path = pathlib.Path(root_path) / class_name\n",
    "    filenames = glob.glob(path / '*.jpg')\n",
    "\n",
    "    tensors = []\n",
    "    for filename in filenames:\n",
    "        img = PIL.Image.open(filename).convert('RGB')\n",
    "        tensors.append(transform(img) if transform else img)\n",
    "    return tensors\n",
    "\n",
    "def assemble_concept(name, id, concept_path):\n",
    "    dataset = dt.CustomIterableDataset(load_tensor, concept_path / name)\n",
    "    concept_iter = dt.dataset_to_dataloader(dataset)\n",
    "    return cp.concept.Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_path = pathlib.Path(\"data/tcav/image/concepts/\")\n",
    "stripes_concept = assemble_concept(\"striped\", 0, concepts_path=concepts_path)\n",
    "dotted_concept = assemble_concept(\"dotted\", 1, concepts_path=concepts_path)\n",
    "random_concept = assemble_concept(\"random\", 2, concepts_path=concepts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=['inception4c', 'inception4d', 'inception4e']\n",
    "\n",
    "mytcav = cp.concept.TCAV(\n",
    "    model=model, \n",
    "    layers=layers,\n",
    "    layer_attr_method=cp.LayerIntegratedGradients(model, None, multiply_by_inputs=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zebra_images = load_tensors('zebra', transform=False)\n",
    "zebra_tensors = torch.stack([transform(img) for img in zebra_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = [[stripes_concept, random_concept]]\n",
    "\n",
    "ix = 0\n",
    "tcav_scores_w_random = mytcav.interpret(\n",
    "    inputs=zebra_tensors, \n",
    "    experimental_sets=classification_data,\n",
    "    target=ix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = [[stripes_concept, dotted_concept]]\n",
    "tcav_scores_w_random = mytcav.interpret(\n",
    "    inputs=zebra_tensors, \n",
    "    experimental_sets=classification_data,\n",
    "    target=ix\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
