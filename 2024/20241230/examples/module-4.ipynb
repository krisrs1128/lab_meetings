{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Explaining Models (Dictionary Learning)\n",
    "\n",
    "In this demo, we'll use dictionary learning to analyze how the final hidden\n",
    "layer of a GPTNeo model organizes articles from the fineweb-edu dataset. This\n",
    "hidden layer is 768-dimensional, but analyzing individual neurons is not an\n",
    "efficient way to work. We will find that looking at the learned dictionary atoms\n",
    "associated with this layer's activations are much more interesting.\n",
    "\n",
    "The libraries below link to data and models in huggingface. They are already\n",
    "included in the iisa312 environment, defined in this [yaml file](https://github.com/krisrs1128/talks/blob/master/2024/20241230/examples/environment-iisa312.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(20241230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below defines a data loader for the\n",
    "[fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "dataset. This is a 7.5TB dataset, so we'll only try working with a streaming\n",
    "version, which allows us to read a few articles at a time (we'll be looking at a\n",
    "tiny fraction of the original data, but it will be enough to see some\n",
    "interesting structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"CC-MAIN-2024-10\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save 2500 articles on which to extract activations. You can see the first\n",
    "200 characters of the raw text from a few articles below. They are all somewhat\n",
    "academic in style, but they range quite dramatically in the topics they discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['– Computer viruses are parasitic programs which are able to replicate themselves, attach themselves to other executables in the computer, and perform some unwanted and often malicious actions. A virus...',\n",
       " 'For those unfamiliar with Cornish, it is classed as a p-Celtic member of the family of Celtic languages, which was once spoken across much of Europe, and is now restricted to the insular world and Bri...',\n",
       " 'Our cultural identity: Experience the culture and heritage of Cyprus Course Description Culture has the power to transform entire societies, strengthen local communities and forge a sense of identity ...',\n",
       " '“The more you empower kids, the more they can do,” said one Providence actor after working with Rhode Island public school students in the Arts/ Literacy Project, based at Brown University’s education...',\n",
       " \"Mixed Progress Against Cancers in Teens, Young Adults\\nWEDNESDAY, July 28, 2021 (HealthDay News) -- There's some encouraging news for U.S. teens and young adults with cancer.\\nSurvival rates have improv...\",\n",
       " 'Rhetorical analysis is not for the faint of heart. It’s for teachers and instructors who don’t mind students feeling uncomfortable enough to take a risk. Rhetorical analysis has changed everything for...',\n",
       " 'Sport plays an important role in the educational process since the TPS when the child’s need for movement is answered by daily activities within the school.\\nTPS and PS practice sport with their teache...',\n",
       " \"World's first 3D keyhole surgery performed at University of Surrey\\n28 Jan 2011\\nDoctors performed the world's first remote 3D keyhole surgery\\nduring a symposium at the University of Surrey last Decembe...\",\n",
       " 'The Lodge Pole Pine Christmas tree is a native to the Rocky Mountains in the western in the western United States.\\nHowever, the Lodgepole found greater popularity in the UK as a Christmas tree.\\nLodgep...',\n",
       " 'After the famous earthquake of 1755 that destroyed much of the city and could be felt all across the Mediterranean and much of continental Europe, the people of Lisbon had to rebuild the city. Portuga...']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_stream = 2500\n",
    "texts = []\n",
    "for x in fw:\n",
    "    texts.append(x[\"text\"])\n",
    "    if len(texts) > n_stream: break\n",
    "\n",
    "[f\"{s[:200]}...\" for s in texts[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below extracts embeddings from the final hidden state\n",
    "(`.hidden_states[-1]`) in a GPTNeo model. Notice that we're averaging the hidden dimension across all tokens in the text. In theory, we could analyze activations within smaller stretches of text, but we are aiming more for simplicity than completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[-1].mean(axis=(0, 1))\n",
    "\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = GPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below applies `extract_embeddings` to all the articles we downloaded\n",
    "above. This is relatively fast on a machine with a GPU, but since we're running\n",
    "all the demos on our laptops, it would be quite slow, so I've commented it out.\n",
    "Instead, we'll just download embeddings that I extracted in advance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embeddings = []\n",
    "for x in fw:\n",
    "    embeddings.append(extract_embeddings(x[\"text\"], model, tokenizer))\n",
    "    if len(embeddings) > n_stream: break\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "     pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below define the dictionary learning algorithm. We will alternate\n",
    "`alpha_update` with `dictionary_update` to learn the $\\alpha$ and $\\Phi$\n",
    "parameters, respectively. For `alpha_update`, we are using a standard\n",
    "implementation of the fast iterative soft thresholding algorithm (this is where\n",
    "the seemingly arbitrary formulas like $1 + \\sqrt{1 + 4t^{2}}$ are coming from).\n",
    "Notice that we are constraining $\\alpha \\succeq 0$ using the `clamp` argument of\n",
    "`threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(x, lambd, clamp=True):\n",
    "    x_ = np.sign(x) * np.maximum(np.abs(x) - lambd, 0)\n",
    "    if clamp:\n",
    "        x_[x_ < 0] = 0\n",
    "    return x_\n",
    "\n",
    "def alpha_update(X, D, alpha, lambd, max_iter=20):\n",
    "    L = np.linalg.norm(D, ord=2) ** 2\n",
    "    a = np.zeros_like(alpha)\n",
    "    a_ = np.copy(a)\n",
    "    \n",
    "    t = 1\n",
    "    for _ in range(max_iter):\n",
    "        a_prev = np.copy(a)\n",
    "        a = threshold(a_ + (1 / L) * D.T @ (X - D @ a_), lambd / L)\n",
    "        t_new = (1 + np.sqrt(1 + 4 * t ** 2)) / 2\n",
    "        a_ = a + ((t - 1) / t_new) * (a - a_prev)\n",
    "        t = t_new\n",
    "    \n",
    "    return a\n",
    "\n",
    "def dictionary_update(X, a):\n",
    "    return X @ np.linalg.pinv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run 25 iterations of dictionary learning with $K = 250$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:45<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "K = 250\n",
    "X = torch.stack(embeddings).numpy().T\n",
    "D, N = X.shape\n",
    "Phi = np.random.randn(D, K)  # Initial dictionary\n",
    "alpha = np.random.rand(K, N)  # Initial sparse codes\n",
    "l = 0.5  # Regularization parameter\n",
    "\n",
    "for i in tqdm(range(25)):\n",
    "    alpha = alpha_update(X, Phi, alpha, l)\n",
    "    Phi = dictionary_update(X, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at articles that have especially high activations\n",
    "$\\alpha_{i}$ on subsets of articles. For example, it seems the first dictionary\n",
    "atom $\\phi_{1}$ is mainly related to languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learn How to say Thank you and Sorry in Croatian\\nIn any language it’s important to learn how to say thank you and sorry in Croatian. Find some common phrases in Best Languages to Learn below. Thank yo...',\n",
       " 'Irish is a living, modern language, spoken by many people and communities in Ireland and studied by people all around the globe. I would be doing Irish a great disservice if I were to present it as an...',\n",
       " 'The German Genitive\\nDer Genitiv im Deutschen – Erklärungen und Beispiele\\nThe German Genitive – Summary\\nA noun is in the genitive case when it is a attribute of another noun or when it indicates posses...',\n",
       " 'Learn how to say the names of many different countries, nationalities and languages in English. Learn how to say the names of many different countries, nationalities and languages in English. Menu. Co...',\n",
       " 'Welcome to the Quranic Arabic Corpus, an annotated linguistic resource which shows the Arabic grammar, syntax and morphology for each word in the Holy Quran. The corpus provides three levels of analys...',\n",
       " 'In a World Characterized by Increasing Globalization And Cultural Diversity, The Concept Of National Identity Takes On a New Dimension. This Article Delves Into The Intriguing Journey Of ‘Do Nahi Aik ...',\n",
       " 'Thursday, 22 February 2024 | 02:42 AM\\nGenerosity and gratitude are two characteristics that go hand in hand. In the materialist world today, where people are running after having the most material or ...',\n",
       " 'Presentation on theme: \"C ITING Q UOTATIONS MLA Requirements and Examples.\"— Presentation transcript:\\nC ITING Q UOTATIONS MLA Requirements and Examples\\nW HY WE CITE SOURCES High schools and colleges t...',\n",
       " 'The next step after learning letters is to learn the correct sequence. Once kids have mastered the letter sequence, they should try writing the letter which comes before. Practicing a missing letter i...',\n",
       " 'Part of speech: noun\\nOrigin: French, mid-18th century\\nA summary or abstract of a text or speech.\\nAn outline of main points or facts.\\nExamples of Précis in a sentence\\n\"I read the précis of the required...']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ix = np.argsort(alpha[0, :])[-10:][::-1]\n",
    "[f\"{texts[i][:200]}...\" for i in top_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisa312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
