{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 3: Explaining Models (Concepts)\n",
    "\n",
    "Let's see how concept activation vectors can be used to explain the Google LeNet\n",
    "image classification model. Specifically, we'll follow the `captum` package\n",
    "tutorial to see whether the model uses the concept of \"stripes\" to classifying\n",
    "images into the \"zebra\" class. To define the concepts, we download reference\n",
    "images from the Broden catalog. Both the concepts and the images to classify are\n",
    "stored in [this zip file](https://drive.google.com/file/d/18dDYwSH-OiovV8vDfmu8eMKIVFuweOy9/view?usp=sharing),\n",
    "which can be unzipped using\n",
    "\n",
    "```\n",
    "tar -zxvf data.tar.gz\n",
    "```\n",
    "\n",
    "Make sure the results are stored in a directory `data/concepts/` relative to\n",
    "this notebook (or be prepared to change the paths below). The block below loads\n",
    "libraries which are already installed in the `iisa312` conda environment, which\n",
    "can be setup by downloading this [environment.yaml file](https://github.com/krisrs1128/talks/blob/master/2024/20241230/examples/environment-iisa312.yaml) and running\n",
    "\n",
    "```\n",
    "conda env create -yf environment-iisa312.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "import PIL\n",
    "import captum as cp\n",
    "import captum.concept._utils.data_iterator as di\n",
    "import glob\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the helper functions below. These are doing some routine image loading and transformation steps and are not specific to concept models in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(filename):\n",
    "    \"\"\"Load a single image file as a tensor\"\"\"\n",
    "    img = PIL.Image.open(filename).convert(\"RGB\")\n",
    "    return transform(img)\n",
    "\n",
    "def load_tensors(class_name, root_path=\"data/concepts/\", transform=True):\n",
    "    \"\"\"Load all the images belonging to a class as a tensor. This assumes that\n",
    "    each class gets a subdirectory of root_path.\"\"\"\n",
    "    path = Path(root_path) / class_name\n",
    "    filenames = glob.glob(str(path / '*.jpg'))\n",
    "\n",
    "    tensors = []\n",
    "    for filename in filenames:\n",
    "        img = PIL.Image.open(filename).convert('RGB')\n",
    "        tensors.append(transform(img) if transform else img)\n",
    "    return tensors\n",
    "\n",
    "def transform(img):\n",
    "    \"\"\"Transform an image into a form that can be used for classification\"\"\"\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ]\n",
    "    )(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`captum` defines a `Concept` class. This is basically a glorified data loader,\n",
    "only really different from ordinary data loaders because it is associated with a\n",
    "human interpretable label, `name`. The concept objects created by the factory\n",
    "below will loop over the images within the `concept_path/name` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_concept(name, id, concept_path):\n",
    "    dataset = di.CustomIterableDataset(load_tensor, f\"{str(concept_path / name)}/\")\n",
    "    concept_iter = di.dataset_to_dataloader(dataset)\n",
    "    return cp.concept.Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define 5 different instances of this Concept object. We are mainly interested in the `striped` concept, because this seems like it should be related to the zebra classification. All the other concepts are a form of control. `dotted` is a control where there is some systematic structure in the concept, but we don't think it should be related to zebra classification at all. The other three are random subsets of imagenet which don't share any systematic structure. These are a somewhat more distant control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_path = Path(\"data/concepts/\")\n",
    "stripes_concept = assemble_concept(\"striped\", 0, concept_path=concept_path)\n",
    "dotted_concept = assemble_concept(\"dotted\", 1, concept_path=concept_path)\n",
    "random0_concept = assemble_concept(\"random500_0\", 2, concept_path=concept_path)\n",
    "random1_concept = assemble_concept(\"random500_1\", 3, concept_path=concept_path)\n",
    "random2_concept = assemble_concept(\"random500_2\", 4, concept_path=concept_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain the Google LeNet model loaded in the block below. We need to\n",
    "access activations, but we won't be doing any training, so we call\n",
    "`model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.googlenet(pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TCAV implementation in captum has an object-oriented design. We first define a generic TCAV explainer associated with a model and set of layers of interest (in this cass, a few of the `inception4` layers). We can then apply this object to arbitrary concept objects and response classes. Since the object is already associated with a model/layers, we no longer need to specify the model or layers in each call -- we can just ask whether a concept is related to a class, and the TCAV explainer object will know to look it up in the correct model. Notice that we are using integrated gradients to define the class' sensitivity to perturbations to the model activations. This is different from the original TCAV paper, which just ordinary gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=['inception4c', 'inception4d', 'inception4e']\n",
    "\n",
    "tcav = cp.concept.TCAV(\n",
    "    model=model, \n",
    "    layers=layers,\n",
    "    layer_attr_method=cp.attr.LayerIntegratedGradients(model, None, multiply_by_inputs=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply this object to see whether stripes are related to zebra\n",
    "classification.  Our first version defines the concept direction by classifying\n",
    "stripes images vs. random imagenet images. Here, 340 is the label for the zebra\n",
    "class in imagenet.  `n_steps` refers to the number of steps in the integrated\n",
    "gradients approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = [[stripes_concept, random0_concept]]\n",
    "zebra_images = load_tensors('zebra', transform=False)\n",
    "zebra_tensors = torch.stack([transform(img) for img in zebra_images])\n",
    "\n",
    "response_id = 340\n",
    "tcav_scores = tcav.interpret(\n",
    "    inputs=zebra_tensors, \n",
    "    experimental_sets=classification_data,\n",
    "    n_steps=5,\n",
    "    target=response_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sign_count` output below is the test statistic $T_{k}$ introduced in our notes. Values close to one mean that the class gradients $\\nabla y_{k}\\left(h\\left(\\mathbf{x}\\right)\\right)$ are almost always in the same half space as the concept activation direction $\\mathbf{v}$. We see that the striped class is close to 1 while the random class is always less than 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function captum.concept._core.tcav.TCAV.interpret.<locals>.<lambda>()>,\n",
       "            {'0-2': defaultdict(None,\n",
       "                         {'inception4c': {'sign_count': tensor([0.9231, 0.0769]),\n",
       "                           'magnitude': tensor([ 0.6405, -0.6405])},\n",
       "                          'inception4d': {'sign_count': tensor([1., 0.]),\n",
       "                           'magnitude': tensor([ 1.2210, -1.2210])},\n",
       "                          'inception4e': {'sign_count': tensor([0.8846, 0.1154]),\n",
       "                           'magnitude': tensor([ 0.3268, -0.3268])}})})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcav_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blocks below repeat this analysis with the dotted rather than random control. Again, we see that the striped class defines as concept activation direction $v$ that is much more relevant to zebra class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = [[stripes_concept, dotted_concept]]\n",
    "\n",
    "tcav_scores = tcav.interpret(\n",
    "    inputs=zebra_tensors, \n",
    "    experimental_sets=classification_data,\n",
    "    n_steps=5,\n",
    "    target=response_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function captum.concept._core.tcav.TCAV.interpret.<locals>.<lambda>()>,\n",
       "            {'0-1': defaultdict(None,\n",
       "                         {'inception4c': {'sign_count': tensor([1., 0.]),\n",
       "                           'magnitude': tensor([ 1.4850, -1.4850])},\n",
       "                          'inception4d': {'sign_count': tensor([1., 0.]),\n",
       "                           'magnitude': tensor([ 1.3086, -1.3086])},\n",
       "                          'inception4e': {'sign_count': tensor([1., 0.]),\n",
       "                           'magnitude': tensor([ 0.6868, -0.6868])}})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcav_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisa312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
