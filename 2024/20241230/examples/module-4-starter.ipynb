{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Explaining Models (Dictionary Learning)\n",
    "\n",
    "In this demo, we'll use dictionary learning to analyze how the final hidden\n",
    "layer of a GPTNeo model organizes articles from the fineweb-edu dataset. This\n",
    "hidden layer is 768-dimensional, but analyzing individual neurons is not an\n",
    "efficient way to work. We will find that looking at the learned dictionary atoms\n",
    "associated with this layer's activations are much more interesting.\n",
    "\n",
    "The libraries below link to data and models in huggingface. They are already\n",
    "included in the iisa312 environment, defined in this [yaml file](https://github.com/krisrs1128/talks/blob/master/2024/20241230/examples/environment-iisa312.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPTNeoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(20241230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below defines a data loader for the\n",
    "[fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n",
    "dataset. This is a 7.5TB dataset, so we'll only try working with a streaming\n",
    "version, which allows us to read a few articles at a time (we'll be looking at a\n",
    "tiny fraction of the original data, but it will be enough to see some\n",
    "interesting structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"CC-MAIN-2024-10\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save 2500 articles on which to extract activations. You can see the first\n",
    "200 characters of the raw text from a few articles below. They are all somewhat\n",
    "academic in style, but they range quite dramatically in the topics they discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stream = 2500\n",
    "texts = []\n",
    "for x in fw:\n",
    "    texts.append(x[\"text\"])\n",
    "    if len(texts) > n_stream: break\n",
    "\n",
    "[f\"{s[:200]}...\" for s in texts[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below extracts embeddings from the final hidden state\n",
    "(`.hidden_states[-1]`) in a GPTNeo model. Notice that we're averaging the hidden dimension across all tokens in the text. In theory, we could analyze activations within smaller stretches of text, but we are aiming more for simplicity than completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=False)\n",
    "    # fill in this function\n",
    "    pass\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = GPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block below applies `extract_embeddings` to all the articles we downloaded\n",
    "above. This is relatively fast on a machine with a GPU, but since we're running\n",
    "all the demos on our laptops, it would be quite slow, so I've commented it out.\n",
    "Instead, we'll just download embeddings that I extracted in advance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embeddings = []\n",
    "for x in fw:\n",
    "    embeddings.append(extract_embeddings(x[\"text\"], model, tokenizer))\n",
    "    if len(embeddings) > n_stream: break\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "     pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below define the dictionary learning algorithm. We will alternate\n",
    "`alpha_update` with `dictionary_update` to learn the $\\alpha$ and $\\Phi$\n",
    "parameters, respectively. For `alpha_update`, we are using a standard\n",
    "implementation of the fast iterative soft thresholding algorithm (this is where\n",
    "the seemingly arbitrary formulas like $1 + \\sqrt{1 + 4t^{2}}$ are coming from).\n",
    "Notice that we are constraining $\\alpha \\succeq 0$ using the `clamp` argument of\n",
    "`threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(x, lambd, clamp=True):\n",
    "    x_ = np.sign(x) * np.maximum(np.abs(x) - lambd, 0)\n",
    "    if clamp:\n",
    "        x_[x_ < 0] = 0\n",
    "    return x_\n",
    "\n",
    "def alpha_update(X, D, alpha, lambd, max_iter=20):\n",
    "    L = np.linalg.norm(D, ord=2) ** 2\n",
    "    a = np.zeros_like(alpha)\n",
    "    a_ = np.copy(a)\n",
    "    \n",
    "    t = 1\n",
    "    for _ in range(max_iter):\n",
    "        a_prev = np.copy(a)\n",
    "        a = threshold(a_ + (1 / L) * D.T @ (X - D @ a_), lambd / L)\n",
    "        t_new = (1 + np.sqrt(1 + 4 * t ** 2)) / 2\n",
    "        a_ = a + ((t - 1) / t_new) * (a - a_prev)\n",
    "        t = t_new\n",
    "    \n",
    "    return a\n",
    "\n",
    "def dictionary_update(X, a):\n",
    "    return X @ np.linalg.pinv(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run 25 iterations of dictionary learning with $K = 250$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "K = 250\n",
    "X = torch.stack(embeddings).numpy().T\n",
    "D, N = X.shape\n",
    "Phi = np.random.randn(D, K)  # Initial dictionary\n",
    "alpha = np.random.rand(K, N)  # Initial sparse codes\n",
    "l = 0.5  # Regularization parameter\n",
    "\n",
    "### define the dictionary learning optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at articles that have especially high activations\n",
    "$\\alpha_{i}$ on subsets of articles. For example, it seems the first dictionary\n",
    "atom $\\phi_{1}$ is mainly related to languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ix = np.argsort(alpha[0, :])[-10:][::-1]\n",
    "[f\"{texts[i][:200]}...\" for i in top_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisa312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
