---
title: ""
author: "Kris Sankaran"
#output:
  # xaringan::moon_reader:
  #   css: ["default", "css/xaringan-themer.css"]
  #   lib_dir: libs
  #   self_contained: false
  #   nature:
  #     highlightStyle: github
  #     highlightLines: true
  #     ratio: "16:9"
  #   seal: false
output: html_document
---

class: title

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      myred: ["{\\color{myred}{#1}}", 1],
      mygreen: ["{\\color{mygreen}{#1}}", 1],
      reals: "{\\mathbb{R}}",
      indic: ["{\\mathbf{1}\\left\\{{#1}\\right\\}}", 1],
      Esubarg: ["{\\mathbf{E}_{#1}\\left[{#2}\\right]}", 2],
      absarg: "{\\left|{#1}\\right|}",
      "\*": ["{\\mathbf{#1}}", 1],
      diag: ["{\\text{diag}\\left({#1}\\right)}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>


<style>
.myred {color: #B4575C;}
.mygreen {color: #5A8A80;}
</style>

```{r flair_color, echo=FALSE, warning = FALSE, message = FALSE}
library(xaringancolor)
setup_colors(
  myred = "#B4575C",
  mygreen = "#5A8A80"
)

library(flair)
myred <- "#B4575C"
mygreen <- "#5A8A80"
```


```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(MASS)
library(knitr)
library(RefManageR)
library(tidyverse)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)

BibOptions(
  check.entries = FALSE,
  bib.style = "numeric",
  cite.style = "numeric",
  style = "markdown",
  hyperlink = FALSE,
  dashed = FALSE,
  max.names = 1
)
bib <- ReadBib("references.bib")
```

## Concepts and Probes

<div id="subtitle">
Kris Sankaran <br/>
30 | December | 2024 <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
IISA Interpretability Short Course <br/>
Slides: <a href="https://go.wisc.edu/">go.wisc.edu/</a><br/>
</div>
<!-- 30 minute talk -->

---

### Activations Analysis

1. Deep learning models are composed of many simpler layers. When an input is
passed through the network, it is transformed into a series of neuron
``activations''' at each layer.

1. Neurons activate on inputs with specific properties. For example, many
neurons in the first layer of a network detect edges with a given orientation.

(maybe show the image with Cat neurons)

---

### Activations Analysis

1. Deep learning models are composed of many simpler layers. When an input is
passed through the network, it is transformed into a series of neuron
``activations''' at each layer.

1. Neurons activate on inputs with specific properties. For example, many
neurons in the first layer of a network detect edges with a given orientation.

(show the image of Yoshua from his Trends article)

---

### Activation Analysis

There is a long line.... in real world evidence


In some cases, the deep representations reflect features we can easily
understand, like whether a string of text lies within quotes.

---

### Directed Questions

We often want to see whether a model has learned specific features.

* Validation: Has the model learned a property that we expect to generalize across many contexts?
* Fairness: Can we ensure that the model isn't using a protected attribute, like race?

In this case, we need a more directed search.

---

### Linear Probes

1. A probe is a linear model that predicts a feature of interest from intermediate model activations.

1. If the probe has high accuracy, then we might say that this feature has been
``learned'' by the model in that layer.

(graphical representation of probe within the network)

---

### Application

(give a more meaningful application of linear probes -- I think the original
paper was for debugging) 

---

### Concept Bottlenecks

A related idea is to reduce arbitrary model activations to a set of
human-interpretable concepts `r Citep(bib, "50351")`.  This is part of a larger
body of work that attempts to establish shared language/representations for
interacting with models `r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/koh_concept.png" width=750/> <br/>
Figure from `r Citep(bib, "50351")`.
</span>
]

---

### Concept Bottlenecks

A related idea is to reduce arbitrary model activations to a set of
human-interpretable concepts `r Citep(bib, "50351")`.  This is part of a larger
body of work that attempts to establish shared language/representations for
interacting with models `r Citep(bib, "yuksekgonul2023posthoc")`.

.center[
<span style="font-size: 18px;">
<img src="figures/concept_architecture-2.png" width=600/>
</span>
]

---

### Concepts and Hypothesis Testing

Even without any special architecture, we can test whether a model has learned a
concept. 

* Test Statistic: Compute classifier performance between samples that represent a concept and random sampled negative examples.

* Null Distribution: Compute the test statistic after permuting the positive and negative labels.

---

### Application

Concepts have been applied to medical imaging tasks. In this example...

---

### Takeaways

Probes and concepts provide a bridge between difficult-to-interpret deep
learning model activations and vocabulary people already use within an
application domain. 

Deep learning has made manual feature engineering unecessary for many problems,
but simple features are still useful for explanation.

---

class: reference

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 13)
```

---

class: reference

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 14, end = 29)
```
