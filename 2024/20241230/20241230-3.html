<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Interpretability Short Course: Probes and Concepts</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kris Sankaran" />
    <script src="libs/header-attrs-2.28/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title

&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      myred: ["{\\color{myred}{#1}}", 1],
      mygreen: ["{\\color{mygreen}{#1}}", 1],
      reals: "{\\mathbb{R}}",
      indic: ["{\\mathbf{1}\\left\\{#1\\right\\}}", 1],
      Esubarg: ["{\\mathbf{E}_{#1}\\left[{#2}\\right]}", 2],
      absarg: ["{\\left|{#1}\\right|}", 1],
      "\*": ["{\\mathbf{#1}}", 1],
      diag: ["{\\text{diag}\\left({#1}\\right)}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;


&lt;style&gt;
.myred {color: #B4575C;}
.mygreen {color: #5A8A80;}
&lt;/style&gt;





## Explaining Models: Concepts and Probes

&lt;div id="subtitle"&gt;
Kris Sankaran &lt;br/&gt;
30 | December | 2024 &lt;br/&gt;
Lab: &lt;a href="https://go.wisc.edu/pgb8nl"&gt;go.wisc.edu/pgb8nl&lt;/a&gt; &lt;br/&gt;
&lt;/div&gt;

&lt;div id="subtitle_right"&gt;
IISA Interpretability Short Course &lt;br/&gt;
Schedule: &lt;a href="https://go.wisc.edu/zk3gim"&gt;go.wisc.edu/zk3gim&lt;/a&gt;&lt;br/&gt;
&lt;/div&gt;
&lt;!-- 30 minute talk --&gt;

---

### Activation Analysis

.pull-left[
1. Deep learning models are built from simple layers. When an input passes
through the network, each layer transforms the activations of the layer before.

1. Within a layer, neurons activate on inputs with specific properties. For
example, neurons in an image classifier detect edge with specific
orientations.
]

.pull-right[
&lt;img width="340" src="figures/fig2_zeiler.png"/&gt; &lt;br/&gt;
&lt;span style="font-size: 18px;"&gt;
Figure from [4].
&lt;/span&gt;
]

---

### Activation Analysis

.pull-left[
1. Deep learning models are built from simple layers. When an input passes
through the network, each layer transforms the activations of the layer before.

1. Within a layer, neurons activate on inputs with specific properties. For
example, neurons in an image classifier detect edge with specific
orientations.
]

.pull-right[
&lt;img width="240" src="figures/fig1_bengio.png"/&gt; &lt;br/&gt;
&lt;span style="font-size: 18px;"&gt;
Figure from [5].
&lt;/span&gt;
]

---

### Activation Analysis

There is a line of research suggesting that the later layers in deep models
learn meaningful abstractions.

.center[
&lt;img width="900" src="figures/fig2_karpathy.png"/&gt; &lt;br/&gt;
&lt;span style="font-size: 18px;"&gt;
Figure from [6].
&lt;/span&gt;
]

In some cases, the deep representations reflect features we can easily
understand, like whether a string of text lies within quotes.

---

### Directed Questions

We often want to see whether a model has learned specific features.

* **Validation**: Has the model learned a property that we expect to generalize across many contexts?

* **Fairness**: Can we ensure that the model isn't using a protected attribute, like race?

* **Development**: Are claims about why models do well accurate? (e.g., does transfer learning really reuse features?)

In this case, we need a more focused analysis.

---

### Linear Probes

1. A probe is a linear model that predicts a feature of interest from
intermediate model activations.

1. If the probe has high accuracy, then we might say that this feature has been
learned by the model in that layer.

.center[
&lt;img width="600" src="figures/alain_fig2.png"/&gt; &lt;br/&gt;
&lt;span style="font-size: 18px;"&gt;
In this example, probes have been added to every layer of a network. Figure from
[7].
&lt;/span&gt;
]

---

### Application

This can be a useful debugging strategy for model with more complex
architectures.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="500" src="figures/pathological_training.gif"/&gt; &lt;br/&gt;
In this example from [7], a skip connection across layers 1 - 64  has prevented learning across those layers.
&lt;/span&gt;
]

---

### Concept Bottlenecks

A related idea is to reduce model activations to a set of interpretable concepts
[8]. This is part of a literature that tries to relate
complex models to human-defined "concepts" [9; 10; 11].

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/koh_concept.png" width=750/&gt; &lt;br/&gt;
Figure from [8].
&lt;/span&gt;
]

---

exclude: true

### Concept Bottlenecks

A related idea is to reduce model activations to a set of interpretable concepts
[8]. This is part of a literature that tries to relate
complex models to human-defined "concepts" [9; 10; 11].

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concept_architecture-2.png" width=600/&gt;
&lt;/span&gt;
]

---

### Concepts and Hypothesis Testing

TCAV [12] formally tests whether a concept is being used to predict
a particular class. It applies to any architecture.

**Step 1**. Define a collection of images `\(\*x_{n}\)` that represents the concept. Also construct a control pool of random images `\(\*x_{n}^\prime\)`.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concepts_step1.png" width=600/&gt;&lt;br/&gt;
Here, we are interested in the concept "stripes." Figure from [12].
&lt;/span&gt;
]

---

**Step 2**. Use an intermediate layer's activations `\(h\left(\*x\right)\)` as
predictors for a linear classifier to distinguish the groups. This is like
probes, except the task is derived from Step 1.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concepts_step2.png" width=530/&gt;&lt;br/&gt;
Let `\(\*v\)` denote the normal to the decision boundary.
&lt;/span&gt;
]

---

**Step 3**. Compare the direction `\(v\)` to the directions in the embedding spac
that increase each sample's class `\(k\)` probability.

`\begin{align*}
S_{k}\left(\*x\right) = \nabla y_{k}\left(h\left(\*x\right)\right)^\top \*v
\end{align*}`

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concepts_step3.png" width=700/&gt;&lt;br/&gt;
`\(\nabla y_{k}\left(h\left(\*x\right)\right)\)` is the direction in the activation
space with the steepest increase in class `\(k\)`'s logit.
&lt;/span&gt;
]


---

**Step 4**. Let `\(N_{k}\)` be the number of images in class `\(k\)`, and define the test
statistic:

`\begin{align*}
T_{k} = \frac{1}{N_{k}}\absarg{\{\*x \in \text{Class } k : S_{k}\left(\*x\right) &gt; 0\}}.
\end{align*}`

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concepts_step4.png" width=580/&gt;&lt;br/&gt;
We count the number of class `\(k\)`'s gradients that are in the same halfspace as `\(\*v\)`.&lt;br/&gt;
&lt;/span&gt;
]

---

**Step 5** Test the significance of `\(T_{k}\)` by learning reference `\(\*v\)` trained
to distinguish randomly chosen example images.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/concepts_step5.png" width=700/&gt;&lt;br/&gt;
Random classification directions serve as a negative control.
&lt;/span&gt;
]

---

### Real-World Example

Concepts have been applied to medical imaging tasks. In this example, concepts
give additional context about why a model might classify the lesion as a 
melanoma.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/lucieri_fig5.png" width=800/&gt; &lt;br/&gt;
Figure from [13]. In this case, the model is incorrect despite concepts suggesting the correct diagnosis.
&lt;/span&gt;
]

---

### Takeaways

Probes and concepts provide a bridge between difficult-to-interpret deep
learning model activations and vocabulary people already use within an
application domain. 

Deep learning has made manual feature engineering unecessary for many problems,
but simple features are still useful for explanation.

---

### Discussion (go.wisc.edu/aonpy0)

[**Compare and Contrast**] Think back to the problem you discussed in
[Interpretability Goals] How might you have applied one of the interpretability
techniques that we have discussed today? In what ways are the explanations
well-suited to the problem? In what ways do they come short?

---

class: reference

### References

[1] R. Caruana et al. "Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission". In: _Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining_ (2015).

[2] T. Gu et al. "BadNets: Evaluating Backdooring Attacks on Deep Neural Networks". In: _IEEE Access_ 7 (2019), p. 47230–47244. ISSN: 2169-3536. DOI:
[10.1109/access.2019.2909068](https://doi.org/10.1109%2Faccess.2019.2909068). URL: [http://dx.doi.org/10.1109/ACCESS.2019.2909068](http://dx.doi.org/10.1109/ACCESS.2019.2909068).

[3] K. Sankaran. "Data Science Principles for Interpretable and Explainable AI". In: _arXiv_ (2024).

[4] M. D. Zeiler et al. "Visualizing and Understanding Convolutional Networks". In: _Computer Vision – ECCV 2014_. Springer International Publishing, 2014, p. 818–833. ISBN:
9783319105901. DOI: [10.1007/978-3-319-10590-1_53](https://doi.org/10.1007%2F978-3-319-10590-1_53). URL:
[http://dx.doi.org/10.1007/978-3-319-10590-1_53](http://dx.doi.org/10.1007/978-3-319-10590-1_53).

[5] Y. Bengio. "Learning Deep Architectures for AI". In: _Foundations and Trends® in Machine Learning_ 2.1 (2009), p. 1–127. ISSN: 1935-8245. DOI:
[10.1561/2200000006](https://doi.org/10.1561%2F2200000006). URL: [http://dx.doi.org/10.1561/2200000006](http://dx.doi.org/10.1561/2200000006).

[6] A. Karpathy et al. _Visualizing and Understanding Recurrent Networks_. 2015. DOI: [10.48550/ARXIV.1506.02078](https://doi.org/10.48550%2FARXIV.1506.02078). URL:
[https://arxiv.org/abs/1506.02078](https://arxiv.org/abs/1506.02078).

[7] G. Alain et al. _Understanding intermediate layers using linear classifier probes_. 2017. URL:
[https://openreview.net/forum?id=ryF7rTqgl](https://openreview.net/forum?id=ryF7rTqgl).

[8] P. W. Koh et al. "Concept Bottleneck Models". In: 2020.

[9] M. Yuksekgonul et al. "Post-hoc Concept Bottleneck Models". In: _The Eleventh International Conference on Learning Representations _. 2023. URL:
[https://openreview.net/forum?id=nA5AZ8CEyow](https://openreview.net/forum?id=nA5AZ8CEyow).

[10] B. Kim. _Beyond Interpretability: Developing a Language to Shape Our Relationships with AI_. En. 2022. URL:
[https://cacmb4.acm.org/opinion/articles/260816-beyond-interpretability-developing-a-language-to-shape-our-relationships-with-ai/fulltext](https://cacmb4.acm.org/opinion/articles/260816-beyond-interpretability-developing-a-language-to-shape-our-relationships-with-ai/fulltext)
(visited on 05/16/2024).

[11] V. V. Ramaswamy et al. _Overlooked factors in concept-based explanations: Dataset choice, concept learnability, and human capability_. 2022. DOI:
[10.48550/ARXIV.2207.09615](https://doi.org/10.48550%2FARXIV.2207.09615). URL: [https://arxiv.org/abs/2207.09615](https://arxiv.org/abs/2207.09615).

[12] B. Kim et al. "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)". In: _Proceedings of the 35th International Conference
on Machine Learning_. Ed. by J. Dy and A. Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR, 2018, pp. 2668-2677. URL:
[https://proceedings.mlr.press/v80/kim18d.html](https://proceedings.mlr.press/v80/kim18d.html).

[13] A. Lucieri et al. "ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions". In: _Computer Methods and Programs in Biomedicine_ 215 (Mar. 2022),
p. 106620. ISSN: 0169-2607. DOI: [10.1016/j.cmpb.2022.106620](https://doi.org/10.1016%2Fj.cmpb.2022.106620). URL:
[http://dx.doi.org/10.1016/j.cmpb.2022.106620](http://dx.doi.org/10.1016/j.cmpb.2022.106620).

---

class: reference

### References


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
