<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Interpretability Short Course: Dictionary Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kris Sankaran" />
    <script src="libs/header-attrs-2.28/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title

&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      myred: ["{\\color{myred}{#1}}", 1],
      mygreen: ["{\\color{mygreen}{#1}}", 1],
      reals: "{\\mathbb{R}}",
      indic: ["{\\mathbf{1}\\left\\{#1\\right\\}}", 1],
      Esubarg: ["{\\mathbf{E}_{#1}\\left[{#2}\\right]}", 2],
      absarg: "{\\left|{#1}\\right|}",
      "\*": ["{\\mathbf{#1}}", 1],
      diag: ["{\\text{diag}\\left({#1}\\right)}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;


&lt;style&gt;
.myred {color: #B4575C;}
.mygreen {color: #5A8A80;}
&lt;/style&gt;






## Interpretability for Model Developers: Dictionary Learning

&lt;div id="subtitle"&gt;
Kris Sankaran &lt;br/&gt;
30 | December | 2024 &lt;br/&gt;
Lab: &lt;a href="https://go.wisc.edu/pgb8nl"&gt;go.wisc.edu/pgb8nl&lt;/a&gt; &lt;br/&gt;
&lt;/div&gt;

&lt;div id="subtitle_right"&gt;
IISA Interpretability Short Course &lt;br/&gt;
Schedule: &lt;a href="https://go.wisc.edu/zk3gim"&gt;go.wisc.edu/zk3gim&lt;/a&gt;&lt;br/&gt;
&lt;/div&gt;
&lt;!-- 30 minute talk --&gt;

---

### Dictionary Learning

1. We need to have specific concepts in mind before we can use concept methods.
How can we find out what the model "knows" from scratch?

1. We might try to analyze individual neurons, but individual neurons often play
multiple roles. This "distributed representation" property makes the neurons
more difficult to study (they are also the reason the model is powerful).

1. One idea is to apply dictionary learning to the space of model embeddings.
This method can disentangle the many roles for each neuron into a discrete set
of interpretable features.

---

### Formulation

Following [19], suppose `\(\*x_{n}\)` are the concatenated
activations across all layers in the network. Then solve:

.pull-left[
`\begin{align*}
\arg\min_{\Phi, \left(\alpha_{n}\right)} \sum_{n = 1}^{N} \|\*x_n - \*\Phi\*\alpha_{n}\|_{2} + \lambda\|\alpha_n\|_{1} \\
\text{subject to } \alpha_{n} \succeq 0 \text{ for } n = 1, \dots, N
\end{align*}`
]

.pull-right[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="500" src="figures/dictionary_rectangles.png"/&gt;
&lt;/span&gt;
]

---

### Formulation

The columns `\(\varphi_{k}\)` of `\(\Phi\)` are called atoms. Since the basis is
overcomplete, it can reconstruct relatively complex patterns.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="600" src="figures/atoms_interpretation.png"/&gt;
&lt;/span&gt;
]


---

### Example Factor

.pull-left[
1. To interpret `\(\varphi_{k}\)`, we can look for examples `\(i\)` with large `\(\alpha_{ik}\)`.

1. For example, this feature has the largest `\(\alpha_{ik}\)` on examples related
to the Golden Gate Bridge.
]

.pull-right[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="500" src="figures/golden_gate_feature_discovery.png"/&gt;
&lt;/span&gt;
]

---

### Atlases

.pull-left[
We can also build maps of many related features. This figure is a dimensionality
reduction of the `\(\varphi_{k}\)`, organized so that those that have high inner
product are placed close to one another.
]

.pull-right[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="500" src="figures/golden_gate_umap.png"/&gt;
These annotations were generated automatically, which helps make this analysis
more automatic.
&lt;/span&gt;
]

---

## Applications

---

### Abstraction

1. One of the motivations for deep learning is that deeper layers of a network
can learn more abstract representations.

1. Using dictionary learning, we can test this by comparing features with more
weight on high vs. lower parts of the network [19].

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="500" src="figures/dictionary_rectangles_compare.png"/&gt;
&lt;/span&gt;
]


---

### Example Features

The associated features are consistent with the belief that deeper layers are
more abstract. First, they find that factors whose coefficients `\(\alpha_{n}^{l}\)`
is much larger in either the shallower or deeper parts of the network.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/fig2_yun.png"&gt;
&lt;/span&gt;
]

---

### Example Features

The associated features are consistent with the belief that deeper layers are
more abstract. First, they find that factors whose coefficients `\(\alpha_{n}^{l}\)`
is much larger in either the shallower or deeper parts of the network.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/yun_depth_comparison.png"&gt;
&lt;/span&gt;
]

---

### Local Explanation

We can use LIME to identify individual words that contribute the most to a single feature.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/fig7_yun.png"&gt;
&lt;/span&gt;
]

---

### Memorization

The New York Times sued OpenAI after finding that ChatGPT can reproduce articles
verbatim [20; 21].  AI companies
need to address this "memorization" issue both to protect privacy and to
respect copyright.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/nyt_memorization.png"&gt;
&lt;/span&gt;
]


---

### Defining Memorization

The reference [22] made the idea of memorization more
precise. When prompted with 50 tokens, how closely do the next 50 generated
tokens match any training example?

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="600" src="figures/fig2_stoehr.png"/&gt;&lt;br/&gt;
The red points are generated texts that exactly match those in the training
data. Notice that many paragraphs occur thousands of times.
&lt;/span&gt;
]


---

### Activation Patterns

The authors summarized layers using the maximum activation for each "component"
of transformer self-attention heads.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="600" src="figures/fig4_stoehr.png"/&gt;&lt;br/&gt;
There are qualitative differences in activation patterns between memorized and
non-memorized paragraphs.
&lt;/span&gt;
]

---

### Activation Patterns

One of the heads is very active on memorized paragraphs, and they found that it
attends to rare tokens. The theory is that it might be "looking up" the
memorized text whenever it encounters one of these rare tokens.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="1000" src="figures/fig7_stoehr.png"/&gt;
&lt;/span&gt;
]

---

## Control

---

### Steering Generations

If we want generated output to reflect more (or less) of feature `\(\varphi_{k}\)`,
we can manually increase (or decrease) the activation of the associated neurons.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="1000" src="figures/golden_gate_self.png"/&gt;
&lt;/span&gt;
]

---

### Steering Generations

If we want generated output to reflect more (or less) of feature `\(\varphi_{k}\)`,
we can manually increase (or decrease) the activation of the associated neurons.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="1000" src="figures/transit_self.png"/&gt;
&lt;/span&gt;
]

---

### Steering and Safety

This is one plausible direction for improving model safety. However, there are so
many ways in which an output can be harmful that it's not yet clear how this can
be practically done.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img width="600" src="figures/scam_emails.png"/&gt;
&lt;/span&gt;
]

---

### Takeaways

1. While concepts and probes work well when we have specific concepts in mind a
priori, dictionary learning is helpful when want to explore models in a more
open-ended way.

1. Interpretability is interesting not only for understand but also for control.
We may be able to steer LLMs towards more desirable properties by clearly
understanding their features.

---

### Discussion (go.wisc.edu/aonpy0)

[**Interesting/Confusing**] For any of the topics in this short course, please
share:

- 3 points that you learned for the first time.
- 2 points that you found interesting.
- 1 point that was confusing.

I’ll write replies to the confusing points by the end of the day.

---

class: reference

### References

[1] R. Caruana et al. "Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission". In: _Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining_ (2015).

[2] K. Sankaran. "Data Science Principles for Interpretable and Explainable AI". In: _arXiv_ (2024).

[3] T. Gu et al. "BadNets: Evaluating Backdooring Attacks on Deep Neural Networks". In: _IEEE Access_ 7 (2019), p. 47230–47244. ISSN: 2169-3536. DOI:
[10.1109/access.2019.2909068](https://doi.org/10.1109%2Faccess.2019.2909068). URL: [http://dx.doi.org/10.1109/ACCESS.2019.2909068](http://dx.doi.org/10.1109/ACCESS.2019.2909068).

[4] P. Atanasova et al. "A Diagnostic Study of Explainability Techniques for Text Classification". In: _Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP)_. Ed. by B. Webber, T. Cohn, Y. He and Y. Liu. Online: Association for Computational Linguistics, Nov. 2020, pp. 3256-3274. DOI:
[10.18653/v1/2020.emnlp-main.263](https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.263). URL:
[https://aclanthology.org/2020.emnlp-main.263](https://aclanthology.org/2020.emnlp-main.263).

[5] E. Hvitfeldt et al. _lime: Local Interpretable Model-Agnostic Explanations_. https://lime.data-imaginist.com, https://github.com/thomasp85/lime. 2022.

[6] M. T. Ribeiro et al. "Why should I trust you?" In: _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. San Francisco California
USA: ACM, Aug. 2016.

[7] D. W. Apley et al. "Visualizing the effects of predictor variables in black box supervised learning models". En. In: _J. R. Stat. Soc. Series B Stat. Methodol._ 82.4 (Sep. 2020),
pp. 1059-1086.

[8] A. Agarwal et al. "MDI+: A flexible random forest-based feature importance framework".  (2023). eprint: 2307.01932.

[9] M. D. Zeiler et al. "Visualizing and Understanding Convolutional Networks". In: _Computer Vision – ECCV 2014_. Springer International Publishing, 2014, p. 818–833. ISBN:
9783319105901. DOI: [10.1007/978-3-319-10590-1_53](https://doi.org/10.1007%2F978-3-319-10590-1_53). URL:
[http://dx.doi.org/10.1007/978-3-319-10590-1_53](http://dx.doi.org/10.1007/978-3-319-10590-1_53).

[10] Y. Bengio. "Learning Deep Architectures for AI". In: _Foundations and Trends® in Machine Learning_ 2.1 (2009), p. 1–127. ISSN: 1935-8245. DOI:
[10.1561/2200000006](https://doi.org/10.1561%2F2200000006). URL: [http://dx.doi.org/10.1561/2200000006](http://dx.doi.org/10.1561/2200000006).

[11] A. Karpathy et al. _Visualizing and Understanding Recurrent Networks_. 2015. DOI: [10.48550/ARXIV.1506.02078](https://doi.org/10.48550%2FARXIV.1506.02078). URL:
[https://arxiv.org/abs/1506.02078](https://arxiv.org/abs/1506.02078).

[12] G. Alain et al. _Understanding intermediate layers using linear classifier probes_. 2017. URL:
[https://openreview.net/forum?id=ryF7rTqgl](https://openreview.net/forum?id=ryF7rTqgl).

[13] A. Lucieri et al. "ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions". In: _Computer Methods and Programs in Biomedicine_ 215 (Mar. 2022),
p. 106620. ISSN: 0169-2607. DOI: [10.1016/j.cmpb.2022.106620](https://doi.org/10.1016%2Fj.cmpb.2022.106620). URL:
[http://dx.doi.org/10.1016/j.cmpb.2022.106620](http://dx.doi.org/10.1016/j.cmpb.2022.106620).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
