<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Interpretability Short Course: Global Measures</title>
    <meta charset="utf-8" />
    <meta name="author" content="Kris Sankaran" />
    <script src="libs/header-attrs-2.28/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title

&lt;script type="text/x-mathjax-config"&gt;
MathJax.Hub.Config({
  TeX: {
    Macros: {
      myred: ["{\\color{myred}{#1}}", 1],
      mygreen: ["{\\color{mygreen}{#1}}", 1],
      reals: "{\\mathbb{R}}",
      pdp: ["{\\text{PDP}_{#1}\\left(x_{#1}\\right)}", 1],
      indic: ["{\\mathbf{1}\\left\\{#1\\right\\}}", 1],
      Esubarg: ["{\\mathbf{E}_{#1}\\left[{#2}\\right]}", 2],
      absarg: "{\\left|{#1}\\right|}",
      "\*": ["{\\mathbf{#1}}", 1],
      diag: ["{\\text{diag}\\left({#1}\\right)}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
&lt;/script&gt;

&lt;style&gt;
.myred {color: #B4575C;}
.mygreen {color: #5A8A80;}
&lt;/style&gt;






## Explaining Models: Profiles and Importance

&lt;div id="subtitle"&gt;
Kris Sankaran &lt;br/&gt;
30 | December | 2024 &lt;br/&gt;
Lab: &lt;a href="https://go.wisc.edu/pgb8nl"&gt;go.wisc.edu/pgb8nl&lt;/a&gt; &lt;br/&gt;
&lt;/div&gt;

&lt;div id="subtitle_right"&gt;
IISA Interpretability Short Course &lt;br/&gt;
Schedule: &lt;a href="https://go.wisc.edu/zk3gim"&gt;go.wisc.edu/zk3gim/&lt;/a&gt;&lt;br/&gt;
&lt;/div&gt;

&lt;!-- 30 minute talk --&gt;

---

### Global Summaries

1. LIME and Shapley are used when we want to understand the important variables
for predictions at a specific instance `\(\*x\)`.

1. What if we want global variable summaries across the entire model? We could
average Shapley values across samples, but this doesn't tell us *how* a variable
influences the response.

---

## Partial Dependence

---

### Definition

Partial Dependence Profiles   are based on a
"ceteris paribus" idea. To understand the effect of variable `\(x_{d}\)`, evaluate
the function `\(f\)` at different values of `\(x_{d}\)`, holding other variables at
values observed in the training data.

`\begin{align*}
\pdp{d} &amp;= \Esubarg{\*x_{-d}}{f\left(x_d, \*x_{-d}\right)} \\
&amp;\approx \frac{1}{N}\sum_{n = 1}^{N} f\left(x_{d}, \*x_{n, -d}\right)
\end{align*}`

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_1.png"/&gt;
Suppose we want to a PDP curve for the first dimension evaluated at `\(x_{d}^{\ast}\)`.
&lt;/span&gt;
]

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_2.png"/&gt;
We set the first coordinate of all observed data points to `\(x_{d}^{\ast}\)`.
&lt;/span&gt;
]

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_3.png"/&gt;
We evaluate `\(f\)` at these points and take the average.
&lt;/span&gt;
]

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_4.png"/&gt;
We can do this at another point `\(x_{d}^{\ast\ast}\)`.
&lt;/span&gt;
]

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_5.png"/&gt;
&lt;/span&gt;
We are now beginning to see a PDP curve along this direction.
]

---

### Intuition

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_6.png"/&gt;
We continue until we trace out a full curve along that axis.
&lt;/span&gt;
]

---

### Pathologies: Extrapolation

While partial dependence is a simple idea, it suffers from a few issues.

First, we might query the model at coordinates `\(\left(x_{d}, \*x_{-d}\right)\)` that
are never encountered in the data.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/pdp_pathology.png"/&gt;
&lt;/span&gt;
]

---

### Pathologies: Multicollinearity

We could try to fix this by evaluating the sum only over `\(\*x_{-d}\)` that are close to the observed data.

There is a subtle problem: If `\(x_{d}\)` and `\(x_{d^\prime}\)` are highly correlated, we
might accidentally attribute effects from one onto the other.

---

## Accumulated Local Effects

---

### Geometric Intuition

The main idea of [18] is to resolve these issues using
the derivatives:

`\begin{align*}
\frac{\partial f\left(x_d, \*x_{-d}\right)}{\partial x_d}.
\end{align*}`

This removes any linear effects from variables other than `\(x_{d}\)`.

---

### Geometric Intuition

We can evaluate these derivatives locally across the observed data range. Then,
we can stitch ("accumulate") them into a single, coherent summary.

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/fig9_apley.png"/&gt;
The ALE curve summarizes many local effects.
&lt;/span&gt;
]


---

### ALE Estimand

This is formalized by the following formula,

`\begin{align*}
\text{ALE}_{d}\left(x_{d}\right) := \int \Esubarg{\*x_{-d}\vert x_{d}}{\left.\frac{\partial f\left(x_d, \*x_{-d}\right)}{\partial x_d} \right| x_{d} = z} dz
\end{align*}`

---

### ALE Estimator

This is estimated using partial differences along a fine partition

`\begin{align*}
\sum_{k = 1}^{K} \frac{1}{N_{d}\left(k\right)} \sum_{x_{n} \in N_{d}\left(k\right)} \left[f\left(z_{dk}, \*x_{-d}\right) - f\left(z_{d,k-1}, \*x_{-d}\right)\right]
\end{align*}`

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/fig3_apley.png"/&gt;
The derivatives are approximated by using empirical differences along a fine
grid.
&lt;/span&gt;
]

---

### Computation

1. Each neighborhood `\(N_{d}\)` is usually relatively small. This means that ALE
plots needs fewer function evaluations `\(f\)` compared to Partial Dependence Plots.

1. It's also possible to bootstrap the ALE estimator, so we can easily get
pointwise confidence bands.

---

## MDI+

---

### Model-Specific Variable Importance

Both PDP and ALE plots are model agnostic. If we allow ourselves to focus on
specific classes of models, then we can come up with more precise notions of
importance (give some references,e.g., DeepShap).

To illustrate this idea, we will consider the MDI+ (Mean Decrease in Impurity)
measure proposed in `Citep(bib, "Agarwal2023-sx")` for decision trees and random
forests.

---

### Classic MDI

Let `\(t\)` be a node within a decision tree. Let `\(t_{L}\)` and `\(t_{R}\)` be the left
and right descendent subtrees. We define the decrease in impurity at node `\(s\)`
as:

`\begin{align*}
\Delta\left(s\right) &amp;= \frac{1}{N\left(t\right)}\left[
\sum_{\*x_{n} \in t} \left(y_i - \overline{y}_{t}\right)^2 -
\sum_{\*x_{n} \in t_{L}} \left(y_i - \overline{y}_{t_{L}}\right) ^ 2 - 
\sum_{\*x_{n} \in t_{R}} \left(y_i - \overline{y}_{t_{R}}\right) ^ 2 \right]
\end{align*}`

---

### Classic MDI

The MDI for feature `\(d\)` is then a weighted mean of `\(\Delta\)` across all splits that
involving feature `\(d\)`.

`\begin{align*}
\text{MDI}_{k}\left(s\right) &amp;= \frac{1}{N}\sum_{s : \text{splits } k} N\left(s\right) \Delta\left(s\right)
\end{align*}`

Here, `\(N\left(s\right)\)` is the number of samples that descend from split `\(s\)`.

---

### Issues

1. Like with PDP, the main issue with MDI is that it behaves poorly when features are correlated.

1. If a variable is highly correlated with any others, then its estimated MDI will be low. 

---

### Decision Trees `\(\to\)` Regression

Decision trees can be represented as linear regressions in an basis derived from the tree.

`\begin{align*}
\psi_{s}\left(\*x\right) &amp;= \frac{1}{\sqrt{N\left(t_{L}\right)N\left(t_{R}\right)}} \left[N\left(t_{R}\right)\indic{\*x \in t_{L}} - N\left(t_{L}\right)\indic{\*x \in t_{R}}\right]
\end{align*}`

---

### Decision Trees `\(\to\)` Regression

The `\(\hat{y}\)` from this regression are exactly the fitted values from a decision tree.

`\begin{align*}
\hat{y} = \left[\psi_{1}\left(\*x\right) \vert \dots \vert \psi_{S}\left(\*x\right)\right]\hat{\beta}
\end{align*}`

.center[
&lt;span style="font-size: 18px;"&gt;
&lt;img src="figures/tree_regression_correspondence.png"/&gt;
&lt;/span&gt;
]


---

### MDI `\(\to\)` `\(R^2\)`

Replace all basis elements not involving feature `\(k\)` by their averages:

`\begin{align*}
\hat{y}^{(k)} = \left[\overline{\psi}_{1}\left(\*x\right) \vert \dots \vert \psi_{s}\left(\*x\right) \vert \dots \vert \overline{\psi}_{S}\left(\*x\right)\right]\hat{\beta}
\end{align*}`

The `\(R^2\)` of this model is exactly the MDI of feature `\(k\)`.

---

### MDI `\(\to\)` MDI$+$

1. The main idea of MDI+ is that we don't have to use linear regression -- we
can use the entire high-dimensional statistics toolkit.

1. E.g., `Citep(bib, "Agarwal2023-sx")` argue that using `\(R^2\)` from a ridge
regression model leads to more stable rankings than ordinary MDI.

---

### Software

1. `imodels`: 


---

### Takeaways

1. For global explanations, partial dependence profiles give intuitive summaries
for arbitrary models. Accumulated local effects plots are an improved version of
partial dependence plots.

1. When restricting attention to tree-based measures, it's possible to improve
on classical purity-based variable importance measures by making the connection
to regression.

---

### Discussion (go.wisc.edu/aonpy0)

[**Jargon-free Summary**] Imagine you are working with a client who is familiar
with the basics of machine learning (e.g., supervised vs. unsupervised learning)
but not the interpretability literature. Give a jargon-free but precise
description of *one* of the following techniques. What does it output and what
is the correct interpretation?  

- LIME, Shapley values, PDP, ALE, MDI+

---

class: reference

### References

[1] D. W. Apley et al. "Visualizing the effects of predictor variables in black box supervised learning models". En. In: _J. R. Stat. Soc. Series B Stat. Methodol._ 82.4 (Sep. 2020),
pp. 1059-1086.

[2] P. Atanasova et al. "A Diagnostic Study of Explainability Techniques for Text Classification". In: _Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP)_. Ed. by B. Webber, T. Cohn, Y. He and Y. Liu. Online: Association for Computational Linguistics, Nov. 2020, pp. 3256-3274. DOI:
[10.18653/v1/2020.emnlp-main.263](https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.263). URL:
[https://aclanthology.org/2020.emnlp-main.263](https://aclanthology.org/2020.emnlp-main.263).

[3] E. Hvitfeldt et al. _lime: Local Interpretable Model-Agnostic Explanations_. https://lime.data-imaginist.com, https://github.com/thomasp85/lime. 2022.

[4] M. T. Ribeiro et al. "Why should I trust you?" In: _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. San Francisco California
USA: ACM, Aug. 2016.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
