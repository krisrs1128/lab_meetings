---
title: "Interpretability Short Course: Dictionary Learning"
author: "Kris Sankaran"
#output:
  # xaringan::moon_reader:
  #   css: ["default", "css/xaringan-themer.css"]
  #   lib_dir: libs
  #   self_contained: false
  #   nature:
  #     highlightStyle: github
  #     highlightLines: true
  #     ratio: "16:9"
  #   seal: false
output: html_document
---

class: title

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      myred: ["{\\color{myred}{#1}}", 1],
      mygreen: ["{\\color{mygreen}{#1}}", 1],
      reals: "{\\mathbb{R}}",
      indic: ["{\\mathbf{1}\\left\\{{#1}\\right\\}}", 1],
      Esubarg: ["{\\mathbf{E}_{#1}\\left[{#2}\\right]}", 2],
      absarg: "{\\left|{#1}\\right|}",
      "\*": ["{\\mathbf{#1}}", 1],
      diag: ["{\\text{diag}\\left({#1}\\right)}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>


<style>
.myred {color: #B4575C;}
.mygreen {color: #5A8A80;}
</style>

```{r flair_color, echo=FALSE, warning = FALSE, message = FALSE}
library(xaringancolor)
setup_colors(
  myred = "#B4575C",
  mygreen = "#5A8A80"
)

library(flair)
myred <- "#B4575C"
mygreen <- "#5A8A80"
```


```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(MASS)
library(knitr)
library(RefManageR)
library(tidyverse)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.align = "center", fig.width = 6, fig.height = 3)

BibOptions(
  check.entries = FALSE,
  bib.style = "numeric",
  cite.style = "numeric",
  style = "markdown",
  hyperlink = FALSE,
  dashed = FALSE,
  max.names = 1
)
bib <- ReadBib("references.bib")
```

## Interpretability for Model Developers: Dictionary Learning

<div id="subtitle">
Kris Sankaran <br/>
30 | December | 2024 <br/>
Lab: <a href="https://go.wisc.edu/pgb8nl">go.wisc.edu/pgb8nl</a> <br/>
</div>

<div id="subtitle_right">
IISA Interpretability Short Course <br/>
Slides: <a href="https://go.wisc.edu/">go.wisc.edu/</a><br/>
</div>
<!-- 30 minute talk -->

---

### Motivation

1. Understanding: Explanations can give more insight into model mechanics than
benchmark metrics alone. We can check whether the model is right for the right
reasons.

1. Improvement: By clarifying model mechanics, we might be able to address
current challenges, like memorization.

---


### Dictionary Learning

Instead of analyzing individual neurons, we can identify latent factors using
dictionary learning. Since DL uses an overcomplete basis ($K > D$), the features
are sparser and more disentangled relative to standard dimensionality reduction.

---

### Formulation

Following `r Citep(bib, "Yun2021-jc")`, suppose $\*x_{n}$ are the concatenated
activations across all layers in the network. Then solve:

\begin{align*}
\arg\min_{\Phi, \left(\alpha_{n}\right)} \sum_{n = 1}^{N} \|\*x_n - \*\Phi\*\alpha_{n}\|_{2} + \lambda\|\alpha_n\|_{1} \\
\text{subject to } \alpha_{n} \succeq 0 \text{ for } n = 1, \dots, N
\end{align*}

.center[
<span style="font-size: 18px;">
<img src="figures/dictionary_rectangles.png"/>
</span>
]

---

### Formulation

The columns $\varphi_{k}$ of $\Phi$ are called atoms. Since the basis is
overcomplete, it can reconstruct relatively complex patterns.

.center[
<span style="font-size: 18px;">
<img src="figures/atoms_interpretation.png"/>
</span>
]


---

### Example Factor

1. To interpret $\varphi_{k}$, we can look for examples $i$ with large $\alpha_{ik}$.

1. For example, for this feature has the largest $\alpha_{ik}$ in examples that
have something to do with the Golden Gate Bridge.

---

### Atlases

We can also build maps of many related features. This figure is a UMAP of ...
(phi? alpha?)

.center[
<span style="font-size: 18px;">
<img src="figures/golden_gate_umap.png"/>
These annotations were generated automatically, which helps make this analysis
more automatic.
</span>
]

---

## Applications

---

### Abstraction

1. One of the motivations for deep learning is that deeper layers of a network
can learn more abstract representations.

1. Using dictionary learning, we can test this by comparing features with more
weight on high vs. lower parts of the network.

.center[
<span style="font-size: 18px;">
<img src="figures/dictionary_rectangles_compare.png"/>
</span>
]


---

### Example Features

The associated features are consistent with the belief that deeper layers are
more abstract. First, they find that factors whose coefficients $\alpha_{n}^{l}$
is much larger in either the shallower or deeper parts of the network.

.center[
<span style="font-size: 18px;">
<img src="figures/fig2_yun.png">
</span>
]

---

### Example Features

The associated features are consistent with the belief that deeper layers are
more abstract. First, they find that factors whose coefficients $\alpha_{n}^{l}$
is much larger in either the shallower or deeper parts of the network.

.center[
<span style="font-size: 18px;">
<img src="figures/yun_depth_comparison.png">
</span>
]

---

### Local Explanation

We can use LIME to identify individual words that contribute the most to a single feature.

.center[
<span style="font-size: 18px;">
<img src="figures/fig7_yun.png">
</span>
]

---

### Memorization

1. The New York Times sued OpenAI partly on the basis that ChatGPT sometimes
reproduces articles verbatim `r Citep(bib, c("freeman2024exploring", "nyt_case"))`.

1. AI companies need to understand this "memorization" issue both to protect
privacy and to respect copyright.

.center[
<span style="font-size: 18px;">
<img src="figures/nyt_memorization.png">
</span>
]


---

### Defining Memorization

`r Citep(bib, "stoehr2024")` made the idea of memorization more precise. When
prompted with 50 tokens, how closely do the next 50 generated tokens match any
training example?

.center[
<span style="font-size: 18px;">
<img src="figures/fig2_stoehr.png"/>
The red points are generated texts that exactly match those in the training
data. Notice that many paragraphs occur thousands of times.
</span>
]


---

### Activation Patterns

It would be interesting to see what happens when using dictionary learning.
Instead, the authors summarized layers using the maximum activation for each
"component" of transformer self-attention heads.

.center[
<span style="font-size: 18px;">
<img src="figures/fig4_stoehr.png"/>
There are qualitative differences in activation patterns between memorized and
non-memorized paragraphs.
</span>
]

---

### Activation Patterns

One of the heads is very active on memorized paragraphs, and they found that it
attends to rare tokens. The theory is that it might be ``looking up'' the
memorized text whenever it encounters one of these rare tokens.

.center[
<span style="font-size: 18px;">
<img src="figures/fig7_stoehr.png"/>
</span>
]

---

## Control

---

### Steering Generations

If we want generated output to reflect more (or less) of feature $\varphi_{k}$,
we can manually increase (or decrease) the activation of the associated neurons.

.center[
<span style="font-size: 18px;">
<img src="figures/golden_gate_self.png"/>
</span>
]

---

### Steering Generations

If we want generated output to reflect more (or less) of feature $\varphi_{k}$,
we can manually increase (or decrease) the activation of the associated neurons.

.center[
<span style="font-size: 18px;">
<img src="figures/transit_self.png"/>
</span>
]

---

### Steering and Safety

This is one plausible direction for improving model safety. Though, there are so
many ways in which an output can be harmful that it's not yet clear how this can
be practically done.

.center[
<span style="font-size: 18px;">
<img src="figures/scam_emails.png"/>
</span>
]


---

### Software

1.

1.

---


### Takeaways

1. While concepts and probes work well when we have specific concepts in mind a
priori, dictionary learning is helpful when want to explore in a more open-ended
way.

1. Interpretability is interesting not only for understand but also for control.
We may be able to steer LLMs towards more desirable properties by clearly
understanding their features.


---

class: reference

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 13)
```

---

### Formulation II

The formulation in `r Citep(bib, "templeton2024scaling")`. $\*x_{i}$ are
activations from a single residual connection in the Claude Sonnet model.

\begin{align*}
\arg \min &\|x - \hat{x}\|_{2}^{2} + \lambda \sum_{k} f_{k}\left(x\right) \|\varphi_{K}^{(2)}\|_{2} \\
\hat{x} &= b_{k} + \Phi^{(2)}f\left(x\right) + b \\
f\left(x\right)_k &= \left(\varphi_{k}^{1,\top}x + b_{k}\right)_{+}
\end{align*}