---
title: Interpretability
author: Kris Sankaran
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    self_contained: false
    fig_caption: true
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
    seal: false
css: custom.css
---

```{r, echo = FALSE}
library(RefManageR)
bib <- ReadBib("references.bib")
```


<div id="links">
Slides: <a href="https://go.wisc.edu/4y9176">go.wisc.edu/4y9176</a>
</div>
<div id="title">
Interpretable Machine Learning <br> 

<span style="font-size: 30px">
What's Possible? What's Next?
</span>
</div>

<br/>
<div id="subtitle">
Kris Sankaran <br/>
<a href="https://go.wisc.edu/">go.wisc.edu/</a> <br/>
12 | February | 2023 <br/>
Workshop on Trustworthy Machine Learning<br/>
</div>

---

### The Context

Models are woven into the fabric of modern life,

.pull-left[
Decisions: They can be automate or assist with judgments that previously would
have been done entirely by people.
]

.pull-right[
  Figures for advertisements (google)
  and for diagnosis (X-ray)
]

---

### The Context

Models are woven into the fabric of modern life,

.pull-left[
  Discovery: They can orient us within large data catalogs and can guide us
  towards promising hypotheses.
]

.pull-right[
 Figures of satellite imagery with greenhouse gas monitoring
 Figure of next generation sequencing data, maybe visium
]

---

### The Context

Models are woven into the fabric of modern life,

.pull-left[
  Creativity: They can make it easier for those without technical training to
  explore ideas and express themselves.
]

.pull-right[
  AI assisted artwork
  AI assisted font design
  Gmail autoreplies...
]

---

### The Challenge

Each class of problems comes with subtle objectives. We should...

* [Decision-Making] Respect the right-to-explanation.
* [Decision-Making] Promote socially-agreed-upon ideas of fairness.
* [Decision-Making] Avoid introducing new types of mistakes [cite Lipton]
* [Discovery] Ensure that analysis are reproducible.
* [Discovery] Highlight parsimonious and testable explanations.
* [Creativity] Allow user interaction and control.
* [Creativity] Avoid the homogeneization of perspectives.

---

### The Challenge

Yet, for so much of model development, practitioners still focus on
out-of-sample prediction error.

We need ways to encode the less obvious objectives as well.

---

# What Makes a Model Interpretable?
<br/>
.center[
<img src="figures/computer.png" width=350 style="position: absolute; left: 500px"/>
]

---

# What Makes a Model Interpretable?
<br/>
.center[
<img src="figures/computer.png" width=350 style="position: absolute; left: 500px"/>
]

<p style="font-size: 30px; position: absolute; left: 20px; top: 200px; width: 450px">
This is a difficult questions....
let's start with an easier one.
</p>

---

# What Makes a Visualization Good?
<br/>
.center[
<img src="figures/visualization.png" width=350 style="position: absolute; left: 450px"/>
]

---

### At-the-Surface

A good visualization is:

1. Legible: It knows to omit extraneous, distracting elements.
1. Annotated: All axes and graphical encodings should be described.
1. Information-Dense: A large amount of variation should be communicated
efficiently.

---

### Below-the-Surface

More subtlely, it should pay attention to:

1. Data Provenance: If we don't know the data sources, we should be skeptical or
anything that's shown, no matter how compelling.
1. Audience: The effectiveness of a visualization is dependent on the visual
vocabulary of its audience.
1. Prioritization: Every design emphasizes some comparisons over others. Are the
comparisons honest, and are the "important" patterns visible?
1. Interactivity: Does it engage the reader's problem solving capacity or
spoonfeed them trivialities?

---

### Analogy<sup>1</sup>.footnote[<sup>1</sup><p style="font-size: 20px">Of course, this analogy is only an approximation -- e.g., you can opt-out of a
newspaper visualization, not a AI-based loan decision.</p>]

The nuance with which we can think about visualization can be translated to
interpretability.

*  Legibility $\to$ Parsimony, Modularity
*  Annotation $\to$ Explanations, Simulatability
*  Information Density $\to$ Variance Explained, Prediction Performance

Whether or not a model is interpretable is as subtle as whether a visualization
is effective.

---

### Historical Context

1. **Initial Wave**: Early ML systems required expert-crafted features. Deep learning
removed this requirement, creating a new need for post-hoc expalnations.

1. **Critical Self-Reflection**: Experiments highlight issues in common assumptions
and commentaries attempt to establish shared vocabulary (Lipton 2018, Adebayo
2018, Murdoch 2019, Rudin 2019).

1. **Systematic Evaluation**: Systematic progress depends on shared tasks, objective
evaluation, and substantive theory -- these are beginning to emerge.

.center[
  <img src="timeline"/>
]

---

### Vocabulary

1. **Interpretable Model**: A model that, by virtue of its design, is easy for
its stakeholders to accurately describe and alter.
1. **Explainability Technique**: A method that shapes our mental models about
black box systems.

.center[
  <img src="figure/interpretable_explainable.png"/>
]

---

### Vocabulary

1. **Local Explanation**: An artifact for reasoning about individual predictions.

1. **Global Explanation**: An artifact for reasoning about an entire model.

<img src="figures/local_vs_global.png"/>

---

## Methods

---

### Running Example

I have simulated a toy dataset on which to illustrate some common approaches.

Problem: Our collaborators sampled longitudinal microbiome profiles from 1000
study participants. Some of these participants eventually developed a disease.
What microbiome properties are related to disease development?

.pull[
toy figure of FRESH cohort

This setup is motivated by studies of HIV development in the FRESH cohort.
]

---

### Dataset

$N = 1000$ participants, $P = 144$ species, $T = 50$ timepoints

.center[
  <img src="figure/simulated-data.png"/>
]

---

### Directly Interpretable Models

Let's consider sparse logistic regression and decision trees applied to the
widened data.

.pull-left[
1. Parsimony: Predictions can be traced to a few input features, low-order
interactions, or latent factors.
2. Simulatability: Given a new input and a description of the model, a model
user can make a prediction with relatively little effort.
]

.pull-left[
  <img src="figures/widened_data.png"/>
]

---

### Sparse Logistic Regression

.center[
  Figure of cross-validation performance against number of features.
]

Using too few features leads to poor cross-validation performance.

---

### Decision Trees

We can achieve slightly better performance with decision trees. They are
somewhat deep, which complicates interpretability.

.center[
  example decision tree prediction. 
]

---

### Instability

More troublingly, the outputs from both approaches are unstable. This makes
sense -- the longitudinal structure makes features very correlated.

.center[
  unstable, different decision tree prediction.
]

---

### Feature Engineering

To address this, we decide to reduce dimensionality by handcrafting some
features: overall slope and curvature for each taxon.

.center[
 same regression picture, but now showing the reduced feature space
]

---

### Feature Engineering

This improves prediction performance and somewhat addresses the instability
problem.

Lesson: In the original feature space, there was an interpretability-accuracy
trade-off (a consequence of the bias variance trade-off). However, we have the
flexibility to choose *more interpretable and more predictive representatives.*

---

### Transformers

1. One driving principle of deep learning is that, with sufficiently many examples,
end-to-end optimization should be preferred to expert design.
  - Easier to implement, more accurate.
1. We can apply the GPT2 architecture to our problem, viewing neighboring
microbiome profiles as neighboring words in a sentence.
  - It should automatically learn features like what we defined.

---

### Aside: Transformers Mechanics

---

### Aside: Transformers Mechanics

---

### Embeddings

We can obtain a global explanation by analyzing intermediate representations
("embeddings") within the deep learning architecture. Let's analyze the features
just before the classification step in our transformer architecture.

.center[
  figure of achitecture with the embeddings being extracted before the end
]

---

### Interactions via Embeddings

In text data, we can understand context-dependent meaning by looking for
clusters after applying PCA to the embeddings. These represent a type of
interaction between features.

.center[
<img src="figures/bert_context.png"/>
]

---

### Interaction via Embeddings

We can build the analogous visualization for our microbiome problem.

* Word $\to$ Microbe
* Clusters of Meaning $\to$ Subcommunities containing that microbe

.center[
  Show the cluster visualization for the microbiome
]

---

### Interpolations

Another common technique is to analyze linear interpolations in this space (ref.
Latent Space Cartography).  Here, I have traced out the microbiome profiles
lying between two groups.

---

### Distillation

1. Distillation is another form of global summarization. It uses a transparent
model to approximate a black box.

1. For example, Tan et al. (2018) audited proprietary policing and loan models
using iterative generalized additive models.

.center[
  <img src="figures/distillation-details.png"/>
]

---

### Distillation

If we fit a GAM to the predicted probabilities from our transformer, we obtain
slightly better performance than if we had fit the GAM originally (if not true,
move to the assess the distillation discussion).

.pull-left[
  <img src="figures/distillation_example.png"/>
]

.pull-right[
  ```{r}
    # distillation code
  ```
]

---

### Iterative Structuration

.pull-left[
1. One perspective is that we are treating the black box itself as the object of
data analysis.

1. Like in any data analysis, it is important to assess the quality of the
approximation.
]

.pull-right[
<img src="figures/iterative_structuration.png"/>
]

---

### Perturbation

If you want an explanation for a generic model’s decision on an instance, one
approach is to perturb that input and see how the prediction could have changed.

<img src="figures/perturbation_types.png"/>

---

### Integrated Gradients

For example, we can compute the gradient of each class as we perturb a reference
towards a sample of interest (Sundararajan, Sturmfels).

\begin{align*}
\left(x_{i} - x_{i}'\right) \int_{\alpha \in \left[0, 1\right]} \frac{\partial f\left(x_{i}' + \alpha\left(x_{i} - x_{i}'\right)\right)}{\partial x_{i}} d\alpha
\end{align*}

---

### Integrated Gradients

For example, we can compute the gradient of each class as we perturb a reference
towards a sample of interest (Sundararajan, Sturmfels).

.center[
  <img src="figures/integrated_gradients_animation.gif" width=1200/>
]

---

### Integrated Gradients

In our microbiome example, this can highlight the species and timepoints that
are most responsible for the disease vs. healthy classification of each example.

.center[
Show one example where it highlights the bumps for some taxa.
<img src="figure/"/>
]

---

### Sanity Checks

Evaluating local explanations is notoriously subjective. Some researchers have
proposed automatic "sanity checks" (Adebayo et al. 2018).

.center[
  <img src="figures/sanity_checks.png" width=680/>
]

There have also been theoretical results that identify situations where feature
attribution is unidentified (Bilodeau et al. 2023).

---

### Concept Bottlenecks

Alternatively, we can explain a decision by reducing the arbitrary feature space
to a set of human-interpretable concepts (Koh, 2020). This requires dense,
concept-level annotations for every sample.

.center[
<img src="figures/koh_concept.png" width=750 style="position: absolute; top: 340px; left: 300px"/> 
]

---

### Concept Bottlenecks

For example, in the microbiome example, we could define interpretable "concepts"
by looking at the taxa trends for commonly co-varying groups of species.

.center[
  Visualization of concepts
]

---

### Concept Bottlenecks

We reconfigure our transformer model to first predict the concept label before
making a final classification.

.center[
  Modified architecture
]

---

### Concept Bottlenecks

Performance is comparable to before, and we obtain concept labels to help us
explain each instance's prediction.

---

### Simulation Mechanism

Here is the simulation mechanism that we used.

.center[
  Come up with a visual representation of this mechanism, starting with
  increasing/decreasing, continuing to nearest neighbors in the concept space
]

---

## Discussion

---

### Generalist Models

.pull-left[
1. **Modern deep learning models are being designed to solve many problems
simultaneously.**

1. Since deep learning architectures are composable, they can handle multimodal
settings easily.

1. We are also seeing increasingly rich ways to communicate with them.
]

.pull-right[
  <img src="figures/chatgpt.png"/>
]

---

### Generalist Models

.pull-left[
1. Modern deep learning models are being designed to solve many problems
simultaneously.

1. **Since deep learning architectures are composable, they can handle multimodal
settings easily.**

1. We are also seeing increasingly rich ways to communicate with them.
]

.pull-right[
  <img src="figures/open-vocab-segmentation.gif"/>
]
---

### Generalist Models

.pull-left[

1. Modern deep learning models are being designed to solve many problems
simultaneously.

1. Since deep learning architectures are composable, they can handle multimodal
settings easily.

1. **We are also seeing increasingly rich ways to communicate with them.**

]

.pull-right[
  <img src="figures/scribbles.gif"/>
]

---

### Shared Language

However, this communication has been essentially one-way. Models don't always
behave in the ways we expect, and they do not offer artifacts to guide
interpretation.

---

### Visualization Metaphor

We can look to data visualization for inspiration. 

1. People from many backgrounds are comfortable reading and creating data
visualizations.

1. Visualization tools provide shared representations between code and thought,
and they encourage critical evaluation of data.

---

### Visualization Metaphor

Model interpretability still seems quite far from this potential.

1. How will interpretable ML appear in future news and scientific reports?

1. What interpretable ML tools will be taught to undergraduates across
disciplines?

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 1, end = 3)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 4, end = 6)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 7, end = 10)
```

---

### References

```{r, results='asis', echo = FALSE}
PrintBibliography(bib, start = 11, end = 14)
```


---

### Attributions

explainable reinforcement learning by iconpro86 from <a href="https://thenounproject.com/browse/icons/term/explainable-reinforcement-learning/" target="_blank" title="explainable reinforcement learning Icons">Noun Project</a> (CC BY 3.0)

data visualization by Iconiqu from <a href="https://thenounproject.com/browse/icons/term/data-visualization/" target="_blank" title="data visualization Icons">Noun Project</a> (CC BY 3.0)